[{"path":"index.html","id":"module-preliminaries","chapter":"1 Module Preliminaries","heading":"1 Module Preliminaries","text":"book contains lecture notes module ST303 Linear Models offered Mathematics Statistics Department Maynooth University.continually revised teach course, content may change year year. base link https://bookdown.org/domijank/ST463/\nNote module code changed since.Feel free adopt book parts purposes, please acknowledge use work. cite book, please use:Katarina Domijan, Catherine Hurley (2018). Notes ST463/ST683 Linear Models 1. Department Mathematics Statistics, Maynooth University, Maynooth Ireland. Online textbook (version 1.0, November 12, 2018). Retrieved \nhttps://bookdown.org/domijank/ST463/, DOI:10.5281/zenodo.17945213","code":""},{"path":"index.html","id":"recommended-texts","chapter":"1 Module Preliminaries","heading":"1.1 Recommended texts","text":"many good resources. lecture notes based :Weisberg (2005),\nFox (2005),\nFox (2016),\nRamsey Schafer (2002),\nDraper Smith (1966).","code":""},{"path":"index.html","id":"software","chapter":"1 Module Preliminaries","heading":"1.2 Software","text":"use R.lecture R beginners bring people speed week 2. code needed complete assignments similar examples notes.create document, using bookdown package (Xie 2018) built top R Markdown (Allaire et al. 2025), (Xie, Allaire, Grolemund 2018), (Xie, Dervieux, Riederer 2020) knitr (Xie 2015).use:tidyverse (Wickham et al. 2019), particulardplyr (Wickham et al. 2022) andggplot2 (Wickham 2016).useful packages used :broom(Robinson, Hayes, Couch 2022),MASS (Venables Ripley 2002),car (Fox Weisberg 2011),Sleuth3 (F. L. Ramsey et al. 2016).","code":""},{"path":"intro.html","id":"intro","chapter":"2 Introduction","heading":"2 Introduction","text":"","code":""},{"path":"intro.html","id":"data-and-models","chapter":"2 Introduction","heading":"2.1 Data and Models","text":"Modeling: development mathematical expressions describe behavior random variable interest.variable interest called response (dependent) variable denoted \\(Y\\).variable interest called response (dependent) variable denoted \\(Y\\).Predictor explanatory variables (also called independent variables) thought provide information behavior \\(Y\\) incorporated model.Predictor explanatory variables (also called independent variables) thought provide information behavior \\(Y\\) incorporated model.Predictor variables denoted \\(X\\).Predictor variables denoted \\(X\\).Data consist information taken \\(n\\) units. Subscripts \\(= 1,..., n\\) identify particular unit observations taken.Data consist information taken \\(n\\) units. Subscripts \\(= 1,..., n\\) identify particular unit observations taken.Additional subscripts can used identify different predictors.Additional subscripts can used identify different predictors.models involve unknown constants, called\nparameters, control behavior model. parameters denoted Greek letters (e.g. \\(\\beta\\)) estimated data.models involve unknown constants, called\nparameters, control behavior model. parameters denoted Greek letters (e.g. \\(\\beta\\)) estimated data.denote estimates using hat notation, e.g. \\(\\hat{\\beta}\\).denote estimates using hat notation, e.g. \\(\\hat{\\beta}\\).module study linear models. parameters enter model simple coefficients \\(X\\)s functions \\(X\\)s.module study linear models. parameters enter model simple coefficients \\(X\\)s functions \\(X\\)s.","code":""},{"path":"intro.html","id":"introductory-examples","chapter":"2 Introduction","heading":"2.2 Introductory Examples","text":"first look \\(Y\\) changes \\(X\\) varied seen scatterplot.","code":""},{"path":"intro.html","id":"mother-and-daughter-heights","chapter":"2 Introduction","heading":"2.2.1 Mother and daughter heights","text":"Data Pearson Lee (1903).\\(\\bar{y}=\\) 63.75, sd(\\(y\\)) = 2.6Taller mothers taller daughters.Since points fall line \\(y=x\\) daughters taller.-data follow linear pattern? can use linear regression line summarise data.-can use regression line predict daughters height based mother’s height.:\\(\\hat{y}=\\) 29.92 +0.54 \\(x\\)","code":"\nheights <- read.table(here(\"data\", \"heights.txt\"), header = TRUE)\nhead(heights)##   Mheight Dheight\n## 1    59.7    55.1\n## 2    58.2    56.5\n## 3    60.6    56.0\n## 4    60.7    56.8\n## 5    61.8    56.0\n## 6    55.5    57.9\nheights |>\nggplot(aes(x = Dheight)) +\ngeom_histogram()\nheights |>\nggplot(aes(x = Mheight, y = Dheight)) +\ngeom_point() +\ngeom_abline() +\nxlab(\"mother height (inches)\") +\nylab(\"daughter height (inches)\")\nheights |>\nggplot(aes(x = Mheight, y = Dheight)) +\ngeom_point() +\ngeom_abline() +\ngeom_smooth(method = \"lm\", col = \"red\") +\nxlab(\"mother height (inches)\") +\nylab(\"daughter height (inches)\")"},{"path":"intro.html","id":"bacterial-count-and-storage-temperature","chapter":"2 Introduction","heading":"2.2.2 Bacterial count and storage temperature","text":"Points jittered avoid overprinting.appear linear relationship.Consider transformation?Log transformed bacteria counts appear linear relationship temperature.","code":"\nbacteria <- read.csv(here(\"data\", \"bacteria.csv\"))\n\nbacteria |>\nggplot(aes(x = temp, y = count)) +\ngeom_jitter(width = 0.1, height = .1) +\ngeom_smooth(method = lm, se = FALSE)\nbacteria |>\nggplot(aes(x = temp, y = log(count))) +\ngeom_jitter(width = 0.1, height = .1) +\ngeom_smooth(method = lm, se = FALSE)"},{"path":"intro.html","id":"yield-and-rainfall","chapter":"2 Introduction","heading":"2.2.3 Yield and Rainfall","text":"dataset Ramsey Schafer (2002). data corn yields rainfall `ex0915’ library(Sleuth3) (F. L. Ramsey et al. 2016).\nVariables:Yield: corn yield (bushels/acre)Rainfall: rainfall (inches/year)Year: year.","code":"\nlibrary(GGally)\nlibrary(Sleuth3)\nggpairs(ex0915[, c(2, 3, 1)], upper = list(\ncontinuous = \"points\", combo = \"facethist\", discrete = \"facetbar\", na =\n\"na\"\n), lower = list(\ncontinuous = \"cor\", combo = \"box_no_facet\", discrete = \"count\", na =\n\"na\"\n))"},{"path":"intro.html","id":"driving","chapter":"2 Introduction","heading":"2.2.4 Driving","text":"Example : Weisberg (2005).Study fuel consumption varies 50 US states District Columbia effect state gasoline tax consuption.Variable:FuelC: Gasoline sold road use, thousands gallonsDrivers:Number licensed drivers stateIncome: Per person personal income year 2000, thousands dollarsMiles: Miles Federal-aid highway miles statePop: 2001 population age 16 overTax: Gasoline state tax rate, cents per gallonState: State nameWe use scatterplot matrix.Drivers FuelC state totals larger populous states.Income per person, want make variables comparable.Transform variables:FuelC2:FuelC/PopDrivers2: Drivers/PopMiles2:log\\(_2\\)(Miles)FuelC decreases tax increases lot variation.Fuel weakly related number variables.graphical representations dataset:\nParallel coordinates package MASS (Venables Ripley 2002).can fit linear model predicting FuelC using variables. particularly interested relationship Tax FuelC need take account (adjust ) predictors.\ncan also look data model fit using conditional visualisation (C. B. Hurley, O’Connell, Domijan 2022) R package condvis2 (C. Hurley, OConnell, Domijan 2022). run code , open interactive plot separate window. shows low-dimensional visualisation, constructed showing relationship response FuelC predictor Tax, conditional remaining predictors. conditioning values can selected within shiny app.","code":"\ndriving <- read.table(here(\"data\", \"fuel2001.txt\"), header = TRUE)\ndriving <- driving[, -c(5, 8)]\npairs(driving[, c(2, 1, 3, 4, 5, 6)])\ndriving2 <- driving |>\nmutate(Drivers = Drivers / Pop,\nFuelC = FuelC / Pop,\nMiles = log(Miles, 2))\n\n\npairs(driving2[, c(2, 1, 3, 4, 5, 6)])\nlibrary(MASS)\nparcoord(driving2[, c(2, 6, 1, 3, 4, 5)])\nfit1 <- lm(FuelC~., driving2)\nlibrary(condvis2)\ncondvis(driving2, fit1, sectionvars = \"Tax\")"},{"path":"intro.html","id":"fuel","chapter":"2 Introduction","heading":"2.2.5 Fuel Consumption","text":"Information recorded fuel usage average temperature (\\(^\\)) \ncourse one week eight office complexes similar size. Data Bowerman Schafer (1990).expect fuel use driven weather conditions.Fuel use: response dependent variable. Denoted \\(Y\\).Temperature: Explanatory predictor variable. Denoted \\(X\\).observe n=8 pairs: \\((x_{}, y_{}), =1,...,8\\).scatterplot shows fuel use decreases roughly linearly \ntemperature increases.assume ’s underlying true line: \\[\\mbox{Fuel} =\\beta_{0} + \\beta_{1}\\mbox{Temp} + \\epsilon\\], generally: \\(y =\\beta_{0} + \\beta_{1}x + \\epsilon.\\)intercept (\\(\\beta_0\\)) slope (\\(\\beta_1\\)), unknown parameters \\(\\epsilon\\) random error component.observation :\\(y_i =\\beta_{0} + \\beta_{1}x_i + \\epsilon_i\\).can estimate \\(\\beta_0\\) \\(\\beta_1\\) \navailable data.One method can used method \nordinary least squares.NOTE: models possible:","code":"\nTemp <- c(28, 28, 32.5, 39, 45.9, 57.8, 58.1, 62.5)\nFuel <- c(12.4, 11.7, 12.4, 10.8, 9.4, 9.5, 8, 7.5)\n\nFuelTempData <- data.frame(cbind(Temp, Fuel))\nFuelTempData |>\nggplot(aes(x = Temp, y = Fuel)) +\ngeom_point() +\ngeom_smooth(method = lm, se = FALSE)\nFuelTempData |>\nggplot(aes(x = Temp, y = Fuel)) +\ngeom_point() +\ngeom_smooth(method = loess, se = FALSE)"},{"path":"intro.html","id":"elections-and-economy","chapter":"2 Introduction","heading":"2.2.6 Elections and Economy","text":"Variables:year: 1952 2012growth: inflation-adjusted growth average personal incomevote: percentage votes incumbent party’s candidate US presidential electionsThe better economy performing, better incumbent party’s candidate , biggest exceptions 1952 (Korean War) 1968 (Vietnam War).Example (Gelman, Hill, Vehtari 2020) available package (Gelman, Hill, Vehtari 2025).","code":"\nlibrary(rosdata)\nhibbs |> \nggplot(aes(x = growth, y = vote, label = year)) + \ngeom_text()"},{"path":"SLR.html","id":"SLR","chapter":"3 Simple Linear regression","heading":"3 Simple Linear regression","text":"","code":""},{"path":"SLR.html","id":"OLS","chapter":"3 Simple Linear regression","heading":"3.1 Ordinary least squares","text":"seen introductory examples Section 2.2.5.Fuel consumption example, `best fitting line’ summarise linear trend?\n\\[y_i =\\beta_{0} + \\beta_{1}x_i + \\epsilon_i.\\]method ordinary least squares chooses \\(\\beta_{0}\\), \\(\\beta_{1}\\) minimise:\\[\\begin{align*}\nS(\\beta_{0}, \\beta_{1}) & =\\sum_{=1}^{n}\\epsilon_i^2 \\\\\n& = \\sum_{=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2\\\\\n\\end{align*}\\]least squares estimators \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\) must satisfy: \\(\\frac{\\delta S}{\\delta \\beta_0} = 0\\) \\(\\frac{\\delta S}{\\delta \\beta_1} = 0\\).\\[\\begin{align*}\n\\frac{\\delta S}{\\delta \\beta_0} & = - 2\\sum_{=1}^{n} (y_i-\\beta_0-\\beta_1x_i) \\\\\n\\frac{\\delta S}{\\delta \\beta_1} & = - 2\\sum_{=1}^{n} x_i(y_i-\\beta_0-\\beta_1x_i).\n\\end{align*}\\]Setting 0 \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\) gives:\\[\\begin{align}\n\\sum_{=1}^{n} (y_i-\\hat{\\beta}_0-\\hat{\\beta}_1x_i) & =0 \\tag{3.1}\\\\\n\\sum_{=1}^{n} x_i(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1x_i)& =0. \\tag{3.2}\n\\end{align}\\]\nequations ((3.1) (3.2)) called normal equations.\n(3.1):\\[\\begin{align*}\n\\sum_{=1}^{n} y_i-n\\hat{\\beta}_0-\\hat{\\beta}_1\\sum_{=1}^{n}x_i & =0\\\\\n\\hat{\\beta}_0& =\\bar{y}-\\hat{\\beta}_1\\bar{x}.\n\\end{align*}\\]Substitute (3.2):\\[\\begin{align*}\n\\sum_{=1}^{n}x_i( y_i-\\bar{y}+\\hat{\\beta}_1\\bar{x}-\\hat{\\beta}_1x_i) & =0\\\\\n\\sum_{=1}^{n}x_i( y_i-\\bar{y}) & =\\hat{\\beta}_1\\sum_{=1}^{n}x_i(x_i-\\bar{x})\\\\\n\\hat{\\beta}_1& = \\frac{\\sum_{=1}^{n}x_i( y_i-\\bar{y})}{\\sum_{=1}^{n}x_i(x_i-\\bar{x})}\\\\\n& =\\frac{\\sum_{=1}^{n}(x_i-\\bar{x})( y_i-\\bar{y})}{\\sum_{=1}^{n}(x_i-\\bar{x})^2}\\\\\n& =\\frac{S_{xy}}{S_{xx}}.\n\\end{align*}\\]notation:\n\\[\\begin{align*}\nS_{xx} & =\\sum_{=1}^{n}(x_{} - \\bar{x})^{2} =\\sum_{=1}^{n}x_{}^{2} - n\\bar{x}^2 \\\\\nS_{yy} & =\\sum_{=1}^{n}(y_{} - \\bar{y})^{2}   =\\sum_{=1}^{n}y_{}^{2} - n\\bar{y}^2 \\\\\nS_{xy} & =\\sum_{=1}^{n}(x_{} - \\bar{x})(y_{} - \\bar{y}) = \\sum_{=1}^{n}x_{}y_{} - n\\bar{x}\\bar{y}\n\\end{align*}\\], equation OLS fitted line given :\n\\[\\hat{y} =\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x,\\]\\[\\hat{\\beta}_{1} = \\frac{S_{xy}}{S_{xx}}\\]\n\\[\\hat{\\beta}_{0} = \\bar{y}-\\hat{\\beta}_1\\bar{x}.\\]","code":""},{"path":"SLR.html","id":"residuals","chapter":"3 Simple Linear regression","heading":"3.1.1 Residuals","text":"fitted value observation :\n\\[\\hat{y}_i =\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_i\\]residuals computed :\n\\[e_i = y_i-\\hat{y}_i\\]","code":""},{"path":"SLR.html","id":"some-algebraic-implications-of-the-ols-fit","chapter":"3 Simple Linear regression","heading":"3.1.2 Some algebraic implications of the OLS fit","text":"\\(\\sum_{=1}^n e_i = \\sum_{=1}^n (y_i - \\hat{y}_i) = 0\\) (residuals sum 0)\\(\\sum_{=1}^n e_i = \\sum_{=1}^n (y_i - \\hat{y}_i) = 0\\) (residuals sum 0)\\(\\sum_{=1}^n x_i e_i = \\sum_{=1}^n x_i(y_i - \\hat{y}_i) = 0\\)\\(\\sum_{=1}^n x_i e_i = \\sum_{=1}^n x_i(y_i - \\hat{y}_i) = 0\\)\\(\\sum_{=1}^n y_i = \\sum_{=1}^n \\hat{y}_i\\) ((3.1))\\(\\sum_{=1}^n y_i = \\sum_{=1}^n \\hat{y}_i\\) ((3.1))\\(\\bar{y} = \\hat{\\beta}_0+\\hat{\\beta}_1\\bar{x}\\) (OLS line always goes mean sample)\\(\\bar{y} = \\hat{\\beta}_0+\\hat{\\beta}_1\\bar{x}\\) (OLS line always goes mean sample)\\(\\sum_{=1}^n \\hat{y}_ie_i  = 0\\) ((3.1) (3.2)).\\(\\sum_{=1}^n \\hat{y}_ie_i  = 0\\) ((3.1) (3.2)).","code":""},{"path":"SLR.html","id":"ols-estimates-for-the-fuel-consumption-example","chapter":"3 Simple Linear regression","heading":"3.1.3 OLS Estimates for the Fuel Consumption Example","text":"Calculate \\(\\hat{\\beta}_{0}\\) \\(\\hat{\\beta}_{1}\\):\\[\\begin{align*}\n\\hat{\\beta}_{1} & = \\frac{S_{xy}}{S_{xx}} =   \\frac{\\sum_{=1}^{n}x_{}y_{} - n\\bar{x}\\bar{y}}{\\sum_{=1}^{n}x_{}^{2} - n\\bar{x}^{2}} \\\\\n& =\\frac{-179.65}{1404.355} = -0.128\n\\end{align*}\\]\\(\\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1}\\bar{x} = 10.212 - ( - 0.128)(43.98) =15.84\\)equation fitted line \\(\\hat{y}= 15.84 - 0.128 x\\).","code":"\nx <- Temp\ny <- Fuel\nn <- 8\n\ncbind(x, y, xsq = x^2, ysq = y^2, xy = x * y)##         x    y     xsq    ysq     xy\n## [1,] 28.0 12.4  784.00 153.76 347.20\n## [2,] 28.0 11.7  784.00 136.89 327.60\n## [3,] 32.5 12.4 1056.25 153.76 403.00\n## [4,] 39.0 10.8 1521.00 116.64 421.20\n## [5,] 45.9  9.4 2106.81  88.36 431.46\n## [6,] 57.8  9.5 3340.84  90.25 549.10\n## [7,] 58.1  8.0 3375.61  64.00 464.80\n## [8,] 62.5  7.5 3906.25  56.25 468.75\nsum(x)## [1] 351.8\nsum(y)## [1] 81.7\nmean(x)## [1] 43.975\nmean(y)## [1] 10.2125\nsum(x^2)## [1] 16874.76\nsum(y^2)## [1] 859.91\nsum(x * y)## [1] 3413.11\nSxx <- sum(x^2) - n * mean(x)^2\nSxx## [1] 1404.355\nSyy <- sum(y^2) - n * mean(y)^2\nSyy## [1] 25.54875\nSxy <- sum(x * y) - n * mean(x) * mean(y)\nSxy## [1] -179.6475"},{"path":"SLR.html","id":"interpretation-of-the-fitted-simple-linear-regression-line-parameter-estimates","chapter":"3 Simple Linear regression","heading":"3.1.4 Interpretation of the fitted simple linear regression line: Parameter estimates","text":"-0.128 estimated change mean fuel use 1\\(^\\) increase temperature.theory, 15.84 estimated mean fuel use temperature 0\\(^\\).However, reason believe good estimate data contains information fuel-temperature relationship 28\\(^\\).","code":""},{"path":"SLR.html","id":"predicting","chapter":"3 Simple Linear regression","heading":"3.1.5 Predicting","text":"fitted line allows us predict fuel use temperature within range data.example, \\(x=30^\\):\n\\[\\hat{y}_i = 15.84 - 0.128 \\times 30 = 12.\\]\n12 units fuel estimated fuel use \\(30^\\).E.g. \\(x=40\\); \\(\\hat{y} = 10.721\\), \\(x=50\\); \\(\\hat{y} = 9.442\\).","code":""},{"path":"SLR.html","id":"the-formal-simple-linear-regression-model","chapter":"3 Simple Linear regression","heading":"3.2 The formal simple linear regression model","text":"SLR model tries capture two features:linear trend anda linear trend andfluctuations (scatter trend).\nfluctuations (scatter trend).random variations experimental conditions expect get value \\(y\\) even keep repeating experiment various fixed \\(x\\) values.SLR model tries model scatter regression line. make assumptions behaviour chance fluctuations.","code":""},{"path":"SLR.html","id":"model","chapter":"3 Simple Linear regression","heading":"3.2.1 Model","text":"SLR model form:\\[y_{} = \\beta_{0} + \\beta_{1}x_{} + \\epsilon_{}, \\hspace{0.5cm}  \\epsilon_{} \\sim N(0, \\sigma^{2}), \\hspace{0.5cm} =1,...,n. \\]\\(\\beta_0\\) \\(\\beta_1\\) unknown parameters\\(\\beta_0\\) \\(\\beta_1\\) unknown parameters\\(y\\) \\(\\epsilon\\) random\\(y\\) \\(\\epsilon\\) random\\(x\\) assumed non-random\\(x\\) assumed non-randomWe use errors \\(\\epsilon_{}\\) model chance fluctuations regression line (.e. underlying true line).SLR model assumes errors, .e. vertical distances observed point regression line, , average, equal zero. also assumes normally distributed.Another assumption \\(\\epsilon_{}\\) values independent identically distributed (IID).","code":""},{"path":"SLR.html","id":"assumptions","chapter":"3 Simple Linear regression","heading":"3.2.2 Assumptions","text":"\\(\\mathbb{E}[\\epsilon_{}] = 0\\), \\(\\mathbb{E}[y_{}] = \\beta_0 + \\beta_1x_i+ \\mathbb{E}[\\epsilon_i] = \\beta_0 + \\beta_1x_i\\).\\(\\mathbb{E}[\\epsilon_{}] = 0\\), \\(\\mathbb{E}[y_{}] = \\beta_0 + \\beta_1x_i+ \\mathbb{E}[\\epsilon_i] = \\beta_0 + \\beta_1x_i\\).Var(\\(\\epsilon_i\\)) = \\(\\sigma^2\\). Equivalently Var(\\(y_{}\\)) = \\(\\sigma^2\\).Var(\\(\\epsilon_i\\)) = \\(\\sigma^2\\). Equivalently Var(\\(y_{}\\)) = \\(\\sigma^2\\).\\(\\epsilon_i\\) independent (therefore \\(y_{}\\) also ).\\(\\epsilon_i\\) independent (therefore \\(y_{}\\) also ).\\(\\epsilon_i \\sim N(0, \\sigma^2)\\). Equivalently \\(y_i \\sim N(\\beta_0 + \\beta_1x_i, \\sigma^2)\\).\\(\\epsilon_i \\sim N(0, \\sigma^2)\\). Equivalently \\(y_i \\sim N(\\beta_0 + \\beta_1x_i, \\sigma^2)\\).NOTE: \\(x_i\\) random model says \n\\(\\mathbb{E}[y_{}|x_i] = \\beta_0 + \\beta_1x_i\\) Var(\\(y_{}|x_i\\)) = \\(\\sigma^2\\).","code":""},{"path":"SLR.html","id":"estimation-of-sigma2","chapter":"3 Simple Linear regression","heading":"3.2.3 Estimation of \\(\\sigma^2\\)","text":"NOTE: \\(\\sigma^2\\) = Var(\\(\\epsilon_i\\))errors \\(\\epsilon_i\\) observable, residuals, \\(e_i\\) similar properties.estimate \\(\\sigma^2\\) \\[\\hat{\\sigma}^2 = \\frac{\\sum_{=1}^n e_i^2}{n-2}.\\]\\(n-2\\) degrees freedom \\(\\sum_{=1}^n e_i^2\\) called residual sum squares, denoted \\(\\mbox{SSE}\\).","code":""},{"path":"SLR.html","id":"review-of-some-probability-results","chapter":"3 Simple Linear regression","heading":"3.2.4 Review of some probability results","text":"Let \\(U\\), \\(W\\) \\(Z\\) three random variables:\\(\\mathbb{E}[U]\\) = mean distribution \\(U\\)Var \\((U) = \\mathbb{E}[U^2] - (\\mathbb{E}[U])^2\\)Cov(\\(U,U\\)) = Var(\\(U\\))Cov(\\(U, W\\)) = \\(\\mathbb{E}[UW] - \\mathbb{E}[U]\\mathbb{E}[W]\\)\\(U\\) \\(W\\) uncorrelated independent Cov(\\(U,W\\)) = 0\\(\\mbox{Corr}(U,W) = \\frac{\\mbox{Cov}(U,W)}{\\sqrt{\\mbox{Var}(U)\\mbox{Var}(W)}}\\)constants \\(\\) \\(b\\):\\(\\mathbb{E}[aU+bW] = \\mathbb{E}[U] + b\\mathbb{E}[W]\\)Var(\\(aU \\pm bW\\)) = \\(^2\\)Var[\\(U\\)] + \\(b^2\\)Var[\\(W\\)] \\(\\pm\\) \\(2ab\\)Cov(\\(U\\),\\(W\\))Cov(\\(aU+bW, cZ\\)) = \\(ac\\)Cov(\\(U\\),\\(Z\\)) + \\(bc\\)Cov(\\(W\\),\\(Z\\))","code":""},{"path":"SLR.html","id":"prop","chapter":"3 Simple Linear regression","heading":"3.2.5 Properties of the estimates","text":"model holds:\\(\\hat{\\beta}_1 \\sim N\\left(\\beta_1,\\frac{\\sigma^2}{S_{xx}}\\right)\\)\\(\\hat{\\beta}_1 \\sim N\\left(\\beta_1,\\frac{\\sigma^2}{S_{xx}}\\right)\\)\\(\\hat{\\beta}_0 \\sim N\\left(\\beta_0, \\sigma^2\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right)\\right)\\)\\(\\hat{\\beta}_0 \\sim N\\left(\\beta_0, \\sigma^2\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right)\\right)\\)Cov\\((\\hat{\\beta}_0, \\hat{\\beta}_1) = -\\sigma^2\\frac{\\bar{x}}{S_{xx}}\\)Cov\\((\\hat{\\beta}_0, \\hat{\\beta}_1) = -\\sigma^2\\frac{\\bar{x}}{S_{xx}}\\)\\(\\hat{y} \\sim N\\left(\\beta_0 + \\beta_1x, \\sigma^2\\left(\\frac{1}{n}+ \\frac{(x-\\bar{x})^2}{S_{xx}}\\right)\\right)\\)\\(\\hat{y} \\sim N\\left(\\beta_0 + \\beta_1x, \\sigma^2\\left(\\frac{1}{n}+ \\frac{(x-\\bar{x})^2}{S_{xx}}\\right)\\right)\\)\\((n-2)\\frac{\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi^2_{(n-2)}\\)\\((n-2)\\frac{\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi^2_{(n-2)}\\)\\(\\mathbb{E}[\\hat{\\sigma}^2] = \\sigma^2\\)\\(\\mathbb{E}[\\hat{\\sigma}^2] = \\sigma^2\\)Proof (1):First show \\(\\mathbb{E}[\\hat{\\beta}_1] = \\beta_1\\).\\[\\begin{align*}\n\\hat{\\beta}_1 & = \\frac{S_{xy}}{S_{xx}} \\\\\n& = \\frac{\\sum_{=1}^{n}(x_i-\\bar{x})( y_i-\\bar{y})}{\\sum_{=1}^{n}(x_i-\\bar{x})^2}  \\\\\n& = \\frac{\\sum_{=1}^{n}(x_i-\\bar{x})y_i}{\\sum_{=1}^{n}(x_i-\\bar{x})^2}  \\\\\n& =  \\sum_{= 1}^n a_iy_i\n\\end{align*}\\]\\(a_i\\) depend \\(x\\) random.linearity expectation:\\[\\begin{align*}\n\\mathbb{E}[\\hat{\\beta}_1] & = \\sum_{= 1}^n a_i\\mathbb{E}[y_i]\\\\\n& = \\sum_{= 1}^n a_i (\\beta_0+\\beta_1x_i) \\mbox{  (model assumptions)}\\\\\n& = \\beta_0\\sum_{= 1}^n a_i+\\beta_1\\sum_{= 1}^n a_i x_i\n\\end{align*}\\]\\[\\sum_{= 1}^n a_i = \\frac{\\sum_{=1}^{n}(x_i-\\bar{x})}{\\sum_{=1}^{n}(x_i-\\bar{x})^2} = 0,\\]\\[\\begin{align*}\n\\sum_{= 1}^n a_i x_i & =  \\frac{\\sum_{=1}^{n}x_i(x_i-\\bar{x})}{\\sum_{=1}^{n}(x_i-\\bar{x})^2} \\\\\n& =  \\frac{\\sum_{=1}^{n}x_i^2-n\\bar{x}^2}{S_{xx}} \\\\\n& = \\frac{S_{xx}}{S_{xx}} = 1.\n\\end{align*}\\]\\(\\mathbb{E}[\\hat{\\beta}_1]  = \\beta_1\\) required.Second, show Var(\\(\\hat{\\beta}_1\\)) = \\(\\frac{\\sigma^2}{S_{xx}}\\)\\[\\begin{align*}\n\\mbox{Var}(\\hat{\\beta}_1) & =  \\mbox{Var} \\left( \\sum_{= 1}^n a_i y_i \\right)\\\\\n& =  \\sum_{= 1}^n a_i^2 \\mbox{Var}(y_i) \\mbox{(since $y_i$s independent)}\\\\\n  & =  \\sigma^2 \\sum_{= 1}^n a_i^2\\\\\n  & =  \\sigma^2 \\sum_{= 1}^n \\left( \\frac{x_i-\\bar{x}}{\\sum_{=1}^{n}(x_i-\\bar{x})^2} \\right)^2\\\\\n& =  \\sigma^2  \\frac{\\sum_{= 1}^n (x_i-\\bar{x})^2}{(\\sum_{=1}^{n}(x_i-\\bar{x})^2)^2} \\\\\n  & =  \\sigma^2  \\frac{S_{xx}}{(S_{xx})^2} \\\\\n  & =   \\frac{ \\sigma^2}{S_{xx}} \\mbox{   (required)}\n\\end{align*}\\]Finally, normality assumption follows \\(\\hat{\\beta}_1\\) linear combination normal random variables (\\(y_i\\)s).Proof (2):First show \\(\\mathbb{E}[\\hat{\\beta}_0] = \\beta_0\\).\\[\\begin{align*}\n\\mathbb{E}[\\hat{\\beta}_0] & = \\mathbb{E}[\\bar{y} - \\hat{\\beta}_1\\bar{x}]\\\\\n& = \\mathbb{E}[\\bar{y}] - \\beta_1 \\bar{x}\\\\\n& = \\frac{1}{n}\\sum_{= 1}^n\\mathbb{E}[y_i] - \\beta_1 \\bar{x}\\\\\n& = \\frac{1}{n}\\sum_{= 1}^n (\\beta_0+ \\beta_1 x_i) - \\beta_1 \\bar{x}\\\\\n& = \\frac{1}{n}( n\\beta_0 + \\beta_1 \\sum_{= 1}^n x_i) - \\beta_1 \\bar{x}\\\\\n& = \\beta_0 + \\beta_1 \\bar{x} - \\beta_1 \\bar{x}\\\\\n& = \\beta_0 \\mbox{ (required)}\n\\end{align*}\\]Second, show Var(\\(\\hat{\\beta}_0\\)) = \\(\\sigma^2\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right)\\)\\[\\begin{align*}\n\\mbox{Var}(\\hat{\\beta}_0) & =\\mbox{Var}(\\bar{y} - \\hat{\\beta}_1\\bar{x}) \\\\\n& = \\mbox{Var}(\\bar{y}) + \\bar{x}^2 \\mbox{Var}(\\hat{\\beta}_1) - 2\\bar{x}\\mbox{Cov}(\\bar{y}, \\hat{\\beta}_1)\n\\end{align*}\\]\\[\\begin{align*}\n\\mbox{Cov}(\\bar{y}, \\hat{\\beta}_1) & =\\mbox{Cov}\\left( \\frac{1}{n}\\sum_{= 1}^n y_i, \\sum_{= 1}^n a_iy_i \\right)\\\\\n& =\\sum_{= 1}^n  \\sum_{j = 1}^n \\frac{1}{n} a_i\\mbox{Cov}(y_i, y_j)\\\\\n& =\\frac{1}{n} \\sum_{= 1}^n  \\sum_{j = 1}^n a_i\\mbox{Cov}(y_i, y_j)\\\\\n& =\\frac{1}{n} \\sum_{= 1}^n   a_i\\mbox{Cov}(y_i, y_i) \\mbox{ (since $y_i$ indep.)} \\\\\n& =\\frac{\\sigma^2}{n} \\sum_{= 1}^n   a_i\\\\\n& = 0\n\\end{align*}\\]\\[\\begin{align*}\n\\mbox{Var}(\\hat{\\beta}_0) & = \\mbox{Var}(\\bar{y}) + \\bar{x}^2 \\mbox{Var}(\\hat{\\beta}_1) \\\\\n& =  \\frac{1}{n^2} \\sum_{= 1}^n  \\mbox{Var}(y_i) + \\bar{x}^2 \\frac{\\sigma^2}{S_{xx}}\\\\\n& =  \\frac{1}{n^2} n\\sigma^2 + \\bar{x}^2 \\frac{\\sigma^2}{S_{xx}}\\\\\n& =  \\sigma^2\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right) \\mbox{ (required)}\n\\end{align*}\\]Finally, normality assumption follows \\(\\hat{\\beta}_0\\) linear combination normal random variables (\\(y_i\\)s \\(\\hat{\\beta}_1\\)).Proof (3):\\[\\begin{align*}\n\\mbox{Cov}(\\hat{\\beta}_0, \\hat{\\beta}_1) & =\\mbox{Cov}(\\bar{y} - \\hat{\\beta}_1\\bar{x}, \\hat{\\beta}_1)\\\\\n& =\\mbox{Cov}(\\bar{y}, \\hat{\\beta}_1) - \\mbox{Cov}(\\hat{\\beta}_1\\bar{x}, \\hat{\\beta}_1) \\\\\n& = 0 - \\bar{x}\\mbox{Cov}(\\hat{\\beta}_1, \\hat{\\beta}_1) \\\\\n& = -\\bar{x} \\frac{\\sigma^2}{S_{xx}}\n\\end{align*}\\]Proof (4):First show \\(\\mathbb{E}[\\hat{y}] = \\beta_0 + \\beta_1 x\\).\\[\\begin{align*}\n\\mathbb{E}[\\hat{y}] & = \\mathbb{E}[\\hat{\\beta}_0 + \\hat{\\beta}_1x]\\\\\n& = \\mathbb{E}[\\hat{\\beta}_0] + \\mathbb{E}[\\hat{\\beta}_1]x\\\\\n& = \\beta_0 + \\beta_1 x \\mbox{ (required)}\n\\end{align*}\\]Second, show Var(\\(\\hat{y}\\)) = \\(\\sigma^2\\left(\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{S_{xx}}\\right)\\)\\[\\begin{align*}\n\\mbox{Var}(\\hat{y}) & =\\mbox{Var}(\\hat{\\beta}_0 + \\hat{\\beta}_1 x) \\\\\n   & =\\mbox{Var}(\\bar{y} - \\hat{\\beta}_1\\bar{x} + \\hat{\\beta}_1 x) \\\\\n   & =\\mbox{Var}(\\bar{y} + \\hat{\\beta}_1(x - \\bar{x})) \\\\\n    & =\\mbox{Var}(\\bar{y}) + (x - \\bar{x})^2 \\mbox{Var}(\\hat{\\beta}_1) + 2(x - \\bar{x})\\mbox{Cov}(\\bar{y}, \\hat{\\beta}_1)\\\\\n        & =\\frac{\\sigma^2}{n} + (x - \\bar{x})^2 \\frac{\\sigma^2}{S_{xx}}\\\\\n        & =\\sigma^2\\left(\\frac{1}{n} +  \\frac{(x - \\bar{x})^2}{S_{xx}}\\right) \\quad \\mbox{ (required)}.\n\\end{align*}\\]Finally, normality assumption follows \\(\\hat{y}\\) linear combination \\(y_i\\)s.","code":""},{"path":"SLR.html","id":"special-cases","chapter":"3 Simple Linear regression","heading":"3.2.6 Special cases","text":"\\(x = 0\\), \\(\\hat{y} = \\hat{\\beta}_0\\).\\(x = 0\\), \\(\\hat{y} = \\hat{\\beta}_0\\).\\(x = x_i\\), \\(\\hat{y} = \\hat{y}_i\\).\\(x = x_i\\), \\(\\hat{y} = \\hat{y}_i\\).\\[\\begin{align*}\n\\mbox{Var}(\\hat{y}_i) & =\\sigma^2\\left(\\frac{1}{n} +  \\frac{(x_i - \\bar{x})^2}{S_{xx}}\\right)\\\\\n     & =\\sigma^2h_{ii}\n\\end{align*}\\]NOTE: \\(h_{ii} = \\left(\\frac{1}{n} +  \\frac{(x_i - \\bar{x})^2}{S_{xx}}\\right)\\) distance value (see later!)","code":""},{"path":"SLR.html","id":"simple-linear-regression-models-in-r","chapter":"3 Simple Linear regression","heading":"3.3 Simple linear regression models in R","text":"","code":""},{"path":"SLR.html","id":"r","chapter":"3 Simple Linear regression","heading":"3.3.1 R","text":"useful code :","code":"\nmylm <- lm(Fuel ~ Temp, data = FuelTempData)\nsummary(mylm)## \n## Call:\n## lm(formula = Fuel ~ Temp, data = FuelTempData)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -0.5663 -0.4432 -0.1958  0.2879  1.0560 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 15.83786    0.80177  19.754 1.09e-06 ***\n## Temp        -0.12792    0.01746  -7.328  0.00033 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.6542 on 6 degrees of freedom\n## Multiple R-squared:  0.8995, Adjusted R-squared:  0.8827 \n## F-statistic: 53.69 on 1 and 6 DF,  p-value: 0.0003301\nanova(mylm)## Analysis of Variance Table\n## \n## Response: Fuel\n##           Df  Sum Sq Mean Sq F value    Pr(>F)    \n## Temp       1 22.9808  22.981  53.695 0.0003301 ***\n## Residuals  6  2.5679   0.428                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nmylm |> \n  broom::augment(FuelTempData) |>\n  head()## # A tibble: 6 × 8\n##    Temp  Fuel .fitted  .resid  .hat .sigma  .cooksd .std.resid\n##   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl>    <dbl>      <dbl>\n## 1  28    12.4   12.3   0.144  0.307  0.712 0.0154       0.264 \n## 2  28    11.7   12.3  -0.556  0.307  0.651 0.231       -1.02  \n## 3  32.5  12.4   11.7   0.720  0.219  0.617 0.217        1.24  \n## 4  39    10.8   10.8  -0.0489 0.143  0.716 0.000542    -0.0807\n## 5  45.9   9.4    9.97 -0.566  0.128  0.663 0.0628      -0.927 \n## 6  57.8   9.5    8.44  1.06   0.261  0.460 0.623        1.88\nmylm |> \n  broom::augment(FuelTempData) |>\n  ggplot(aes(x = Temp, y = Fuel)) +\n  geom_point() +\n  geom_line(aes(x = Temp, y = .fitted))"},{"path":"SLR.html","id":"statistical-inference","chapter":"3 Simple Linear regression","heading":"3.4 Statistical inference","text":"","code":""},{"path":"SLR.html","id":"r-simulation","chapter":"3 Simple Linear regression","heading":"3.4.1 R simulation:","text":"Reminder: linear relationship \\(\\mathbb{E}[y_{}] = \\beta_{0} + \\beta_{1}x_{}\\) underlying true line values parameters (intercept \\(\\beta_0\\) slope \\(\\beta_1\\)) unknown.Reminder: linear relationship \\(\\mathbb{E}[y_{}] = \\beta_{0} + \\beta_{1}x_{}\\) underlying true line values parameters (intercept \\(\\beta_0\\) slope \\(\\beta_1\\)) unknown.estimate parameters \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\).estimate parameters \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\).parameter estimates sampling distributions.parameter estimates sampling distributions.simulate dataset, choose true model parameters \\(\\beta_0\\) \\(\\beta_1\\) \\(\\sigma^2\\).Choose sample size \\(n\\):Simulate \\(x_i\\) \\(\\epsilon_i\\) \\(= 1,..., n\\):Calculate \\(y_i\\):Let’s look made data:Red true model green line estimated simmulated dataset.Let’s compare estimated parameters \\(\\beta_0\\) \\(\\beta_1\\) \\(\\sigma^2\\):can think single realisations sampling distribution estimatorsSome useful calculations sampling distributions estimators:Let’s plot sampling dists estimators:\\(\\hat{\\beta}_1 \\sim N\\left(\\beta_1,\\frac{\\sigma^2}{S_{xx}}\\right)\\)\\(\\hat{\\beta}_0 \\sim N\\left(\\beta_0, \\sigma^2\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right)\\right)\\)\\((n-2)\\frac{\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi^2_{(n-2)}\\)Now let’s repeat simulation process 500 times:Plot \\(N = 500\\) fitted lines:Plot estimates compare sampling distribution curves:","code":"\nbeta_0 <- 1\nbeta_1 <- 2\nsigma_sq <- 10\nn <- 20 # number of observations in the sample\nset.seed(12)\nx <- runif(n, -2, 2)\neps <- rnorm(n, 0, sqrt(sigma_sq))\ny <- beta_0 + x * beta_1 + eps\nplot(x, y)\nabline(beta_0, beta_1, col = 2) # true model\npoints(mean(x), mean(y), col = 3)\nfit <- lm(y ~ x) # fit the model\nabline(coef(fit)[1], coef(fit)[2], col = 3)\nhat_beta_0 <- coef(fit)[1]\nhat_beta_1 <- coef(fit)[2]\ns <- sqrt(sum(residuals(fit)^2) / (n - 2))\nhat_beta_0 ## (Intercept) \n##    1.073254\nhat_beta_1 ##        x \n## 2.107893\ns^2## [1] 6.541065\nx_bar <- mean(x)\nSxx <- sum(x^2) - n * (x_bar^2)\n\nvar_beta_0 <- sigma_sq * (1 / n + x_bar^2 / Sxx)\nvar_beta_1 <- sigma_sq / Sxx\n\nest_cov <- -sigma_sq * x_bar / Sxx # estimated covariance from one sample\n\nse_fit <- sqrt(sigma_sq * (1 / n + (x - x_bar)^2 / Sxx))\ndata.frame(hat_beta_1)|>\n  ggplot(aes(xdist = dist_normal(beta_1, sqrt(var_beta_1)))) +\n  stat_slab() + \n  geom_point(aes(x = hat_beta_1, y = 0)) +\n  ggtitle(\"sampling distribution\")+\n  xlab(expression(hat(beta)[1]))+\n  ylab(\"density\")\ndata.frame(hat_beta_0)|>\n  ggplot(aes(xdist = dist_normal(beta_0, sqrt(var_beta_0)))) +\n  stat_slab() + \n  geom_point(aes(x = hat_beta_0, y = 0)) +\n  ggtitle(\"sampling distribution\")+\n  xlab(expression(hat(beta)[0]))+\n  ylab(\"density\")\ndata.frame(s^2)|>\n  ggplot(aes(xdist = dist_chisq(n-2))) +\n  stat_slab() + \n  geom_point(aes(x = s^2 *(n - 2) / sigma_sq, y = 0)) +\n  ggtitle(\"sampling distribution\")+\n  xlab(expression((n - 2)*hat(sigma)^2 / sigma^2 ))+\n  ylab(\"density\")\nN <- 500 # number of simulations\nestimates <- matrix(0, N, 3) # object to save parameter estimates\n\nfor (i in 1:N)\n{\n  # x <- runif(n, -5, 5)\n  eps <- rnorm(n, 0, sqrt(sigma_sq))\n  y <- beta_0 + x * beta_1 + eps\n\n  fit <- lm(y ~ x)\n  estimates[i, 1] <- coef(fit)[1]\n  estimates[i, 2] <- coef(fit)[2]\n  estimates[i, 3] <- anova(fit)[[\"Mean Sq\"]][2] # sigamsq\n}\nestimates |> \n  ggplot(aes(x = beta_hat_0)) +\n  geom_dots() +\n  stat_slab(aes(xdist = dist_normal(beta_0, sqrt(var_beta_0))), \n            col = 1, fill = NA) + \n  ggtitle(\"sampling distribution and simulation estimates\")+\n  xlab(expression(hat(beta)[0])) +\n  ylab(\"Density\")\nestimates |> \n  ggplot(aes(x = beta_hat_1)) +\n  geom_dots() +\n  stat_slab(aes(xdist = dist_normal(beta_1, sqrt(var_beta_1))), \n            col = 1, fill = NA, height = 0.75)  + \n  ggtitle(\"sampling distribution and simulation estimates\")+\n  xlab(expression(hat(beta)[1])) +\n  ylab(\"Density\")\nestimates |> \n  ggplot(aes(x = sigma_sq_hat*(n - 2) / sigma_sq)) +\n  geom_dots()+\n  stat_slab(aes(xdist = dist_chisq(n-2)), \n            col = 1, fill = NA) + \n  ggtitle(\"sampling distribution and simulation estimates\")+\n  xlab(expression(expression((n - 2)*hat(sigma)^2 / sigma^2 ))) +\n  ylab(\"Density\")"},{"path":"SLR.html","id":"inference-for-beta_1","chapter":"3 Simple Linear regression","heading":"3.4.2 Inference for \\(\\beta_1\\)","text":"","code":""},{"path":"SLR.html","id":"confidence-interval","chapter":"3 Simple Linear regression","heading":"3.4.2.1 Confidence Interval","text":"\\(\\hat{\\beta}_1\\) estimates \\(\\beta_1\\), example change fuel use 1\\(^\\) increase temperature.like construct confidence interval \\(\\beta_1\\). give us interval confident true \\(\\beta_1\\) lies.key obtaining C.. fact :\\[\\hat{\\beta}_1 \\sim N(\\beta_1, \\frac{\\sigma^2}{S_{xx}}).\\]Equivalently\\[\\frac{\\hat{\\beta}_{1} - \\beta_{1}}{\\sigma/\\sqrt{S_{xx}}} \\sim N(0,1).\\], replace \\(\\sigma\\) \\(\\hat{\\sigma}\\) :\\[\\frac{\\hat{\\beta}_{1} - \\beta_{1}}{\\hat{\\sigma}/\\sqrt{S_{xx}}} \\sim  t_{n-2}.\\]df \\(n-2\\) df associated estimate \\(\\sigma\\).general, :\\[\\frac{\\mbox{Est - parameter}}{\\mbox{S.E.(Est)}} \\sim \\mbox{distribution}.\\]C.. parameter given :\\[\\mbox{Est} \\pm \\mbox{(quantile distribution)} \\times \\mbox{S.E.(Est)}.\\]\\((1-\\alpha)\\times 100\\%\\) confidence interval \\(\\beta_{1}\\):\\[\\hat{\\beta}_{1} \\pm t_{n-2}(\\alpha/2) \\times \\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}}.\\]fuel use data, \\(95\\%\\) C.. \\(\\beta_1\\) :\\[\\begin{align*}\n\\hat{\\beta}_1 &\\pm t_6(0.025) \\times \\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}}\\\\\n& = -0.128 \\pm 2.45 \\times 0.018\\\\\n& = (-0.171, -0.085)\n\\end{align*}\\]\\(95\\%\\) confident average fuel use drop 0.085 0.171.","code":""},{"path":"SLR.html","id":"hypothesis-test","chapter":"3 Simple Linear regression","heading":"3.4.2.2 Hypothesis test","text":"settings may wish test:\\(H_{0}: \\beta_{1} = 0\\) versus \\(H_{}: \\beta_{1} \\ne 0\\).null hypothesis \\(\\mathbb{E}[y] = \\beta_0\\) .e. \\(\\mathbb{E}[y]\\) linearly related \\(x\\).\\(H_0\\):\\[t_{obs} = \\frac{\\hat{\\beta}_{1} - 0}{\\hat{\\sigma}/\\sqrt{S_{xx}}} \\sim t_{n-2}.\\]P-value = \\(P[T_{n-2} \\geq |t_{obs}|]\\)Reject \\(H_0\\) small p-values, typically \\(p< 0.05\\).fuel use example:\\[t_{obs} = \\frac{-0.128-0}{0.018} = -7.33\\]p-value \\(< 0.001\\), reject \\(H_0\\) conclude \\(\\beta_{1} \\ne 0\\).also test \\(H_0: \\beta_1=b\\) computing:\\[\\frac{\\hat{\\beta}_{1} - b}{\\mbox{S.E}.(\\hat{\\beta}_{1})}.\\]","code":""},{"path":"SLR.html","id":"inference-for-beta_0","chapter":"3 Simple Linear regression","heading":"3.4.3 Inference for \\(\\beta_0\\)","text":"\\(95\\%\\) C.. \\(\\beta_0\\) :\\[\\hat{\\beta}_{0} \\pm t_{n-2}(\\alpha/2) \\times \\mbox{S.E.}(\\hat{\\beta}_{0}).\\]Note:\n\\(\\mbox{S.E.}(\\hat{\\beta}_0) = \\sqrt{\\hat{\\sigma}^2\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right)}\\)fuel use data:\\[\\begin{align*}\n& = 15.84 \\pm 2.45 \\times 0.8018\\\\\n& = (13.88, 17.80)\n\\end{align*}\\]can also test particular value \\(\\beta_0\\), e.g.\\(H_0: \\beta_0 = 0\\) vs. \\(H_A: \\beta_0 \\neq 0\\)null hypothesis \\(\\mathbb{E}[y] = \\beta_1 x\\) .e. line passes origin.test statistic \\[\\frac{\\hat{\\beta}_{0} - \\beta_0}{\\mbox{S.E.}(\\hat{\\beta}_{0})} = 19.75.\\]fuel data.P-value \\(= 2P[T_6 \\geq 19.75] <0.001.\\)Note: illustration, practice particular data . ?","code":""},{"path":"SLR.html","id":"inference-for-mean-response","chapter":"3 Simple Linear regression","heading":"3.4.4 Inference for mean response","text":"Suppose want estimate \\(\\mu=\\mathbb{E}[y]\\) particular value \\(x\\).\\(x_0\\) let:\\[\\mu_0 = \\mathbb{E}[y_0] = \\beta_0 + \\beta_1 x_0\\]can estimate \\(\\mu_0\\) :\\[\\hat{y}_0 = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0.\\]e.g. estimate mean fuel use temperature \\(50^\\) \\[\\hat{y}_0 = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times 50 =15.84-0.128 \\times 50 =9.44.\\]\\(95\\%\\) C.. \\(\\mu_0\\) given \\[\\begin{align*}\n\\mbox{Est} &\\pm t_{n-2}(\\alpha/2) \\times \\mbox{S.E.(Est)}\\\\\n\\hat{y}_0 &\\pm t_{n-2}(\\alpha/2) \\times \\mbox{S.E.}_{\\mbox{fit}}(\\hat{y}_0)\n\\end{align*}\\]\\[\\begin{align*}\n\\mbox{S.E.}_{\\mbox{fit}}(\\hat{y}_0)& =\\hat{\\sigma}\\sqrt{h_{00}}\\\\\n& = \\hat{\\sigma}\\sqrt{\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}}\\\\\n& = 0.65 \\times \\sqrt{0.15}=0.254\n\\end{align*}\\]\\(95\\%\\) C.. \\[\\begin{align*}\n& = 9.44 \\pm 2.45 \\times 0.254\\\\\n& = (8.8, 10.1)\n\\end{align*}\\]interval contains true mean fuel use \\(50^\\) \\(95\\%\\) confidence.","code":""},{"path":"SLR.html","id":"inference-for-prediction","chapter":"3 Simple Linear regression","heading":"3.4.5 Inference for prediction","text":"Suppose want predict response particular value \\(x\\).\\(x_0\\), let \\(y_0\\) unobserved response.model:\\[y_0 =\\mu_0 + \\epsilon= \\beta_0 + \\beta_1 x_0 + \\epsilon\\]estimate (predict) :\\[\\hat{y}_0 = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0.\\]Note: estimation random variable called prediction.example, predict fuel use temp \\(50^\\) :\\[\\hat{y}_0 = 15.84-0.128\\times 50=9.44\\]Note prediction future response equals estimate mean response, however associated standard errors (hence confidence intervals) different.Confidence intervals random variables called prediction intervals (PIs).prediction \\(y_0\\) \\(\\hat{y}_0\\) prediction error \\(y_0-\\hat{y}_0\\) variance:\\[\\begin{align*}\n\\mbox{Var}(y_0-\\hat{y}_0)& = \\mbox{Var}(y_0)+ \\mbox{Var}(\\hat{y}_0)\\mbox{       (indep $y_0$ sample)}\\\\\n& =\\sigma^2 + \\sigma^2\\times h_{00} \\\\\n& = \\sigma^2(1+h_{00})\\\\\n& = \\sigma^2\\left(1+\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}\\right)\n\\end{align*}\\]\\(\\mbox{S.E.}\\) prediction :\\[\\mbox{S.E.}_{\\mbox{pred}} (\\hat{y}_0) = \\hat{\\sigma}\\sqrt{1+\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}}\\]\\(95\\%\\) prediction interval \\[\\hat{y}_0 \\pm t_{n-2}(\\alpha/2) \\times \\mbox{S.E.}_{\\mbox{pred}} (\\hat{y}_0)\\]\\(x_0\\) = 50, \\(\\hat{y}_0 =9.442\\):\\[\\mbox{S.E.}_{\\mbox{pred}}= \\sqrt{1+0.15} \\times 0.654 = 0.702\\]\\(95\\%\\) P.. :\\[\\begin{align*}\n& = 9.44 \\pm 2.45 \\times 0.702\\\\\n& = (7.72, 11.16)\n\\end{align*}\\]\\(95\\%\\) sure interval contains actual fuel use week temp = \\(50^\\).","code":""},{"path":"SLR.html","id":"plotting-cis-and-pis-in-r","chapter":"3 Simple Linear regression","heading":"3.4.6 Plotting CIs and PIs in R","text":"","code":"\nmylm |> \n  broom::augment(FuelTempData, interval = c(\"confidence\")) |>\n  head(4)## # A tibble: 4 × 10\n##    Temp  Fuel .fitted .lower .upper  .resid  .hat .sigma  .cooksd .std.resid\n##   <dbl> <dbl>   <dbl>  <dbl>  <dbl>   <dbl> <dbl>  <dbl>    <dbl>      <dbl>\n## 1  28    12.4    12.3   11.4   13.1  0.144  0.307  0.712 0.0154       0.264 \n## 2  28    11.7    12.3   11.4   13.1 -0.556  0.307  0.651 0.231       -1.02  \n## 3  32.5  12.4    11.7   10.9   12.4  0.720  0.219  0.617 0.217        1.24  \n## 4  39    10.8    10.8   10.2   11.5 -0.0489 0.143  0.716 0.000542    -0.0807\nFuelTempData |>\n  ggplot(aes(x = Temp, y = Fuel)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE)\nmylm |> \n  broom::augment(FuelTempData, interval = c(\"prediction\")) |>\n  ggplot(aes(x = Temp, y = Fuel))+\n  geom_point() +\n  geom_line(aes(y = .fitted)) +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper), fill = \"pink\", alpha = 0.4) \nnewx <- seq(30, 60)\n\nCI <- predict(mylm,\n  newdata = data.frame(Temp  = newx), \n  interval = c(\"confidence\"),\n  level = 0.95, type = \"response\")\n\nCI |> head(4)##        fit      lwr      upr\n## 1 12.00021 11.17760 12.82281\n## 2 11.87228 11.08013 12.66444\n## 3 11.74436 10.98149 12.50724\n## 4 11.61644 10.88152 12.35136\nPI <- predict(mylm,\n  newdata = data.frame(Temp = newx), \n  interval = c(\"prediction\"),\n  level = 0.95, type = \"response\")\n\nPI |> head(4)##        fit       lwr      upr\n## 1 12.00021 10.200424 13.79999\n## 2 11.87228 10.086217 13.65835\n## 3 11.74436  9.971085 13.51764\n## 4 11.61644  9.855011 13.37787"},{"path":"SLR.html","id":"analysis-of-variance-for-s.l.r.","chapter":"3 Simple Linear regression","heading":"3.5 Analysis of variance (for s.l.r.)","text":"analysis variance method comparing fit two models dataset.particularly useful multiple regression.","code":""},{"path":"SLR.html","id":"vote-data-does-regression-on-growth-explain-vote","chapter":"3 Simple Linear regression","heading":"3.5.1 Vote Data: does regression on growth explain vote?","text":"simple linear regression, equivalent testing hypotheses slope parameter.words: comparing fit simple linear regression model fit null (intercept ) model.","code":"\nlin_mod <- lm(vote ~ growth, data = hibbs)\nanova(lin_mod)## Analysis of Variance Table\n## \n## Response: vote\n##           Df Sum Sq Mean Sq F value  Pr(>F)    \n## growth     1 273.63 273.632  19.321 0.00061 ***\n## Residuals 14 198.27  14.162                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"SLR.html","id":"anova-decomposition","chapter":"3 Simple Linear regression","heading":"3.5.2 ANOVA decomposition","text":"\\[\\begin{align*}\n\\mbox{data} & = \\mbox{fit} + \\mbox{residual} \\\\\ny_i & = \\hat{y}_i +e_i\\\\\ny_i - \\bar{y} & = \\hat{y}_i - \\bar{y} +e_i \\\\\n\\sum_{= 1}^n ( y_i - \\bar{y})^2 & =\\sum_{= 1}^n(\\hat{y}_i - \\bar{y} +e_i)^2\\\\\n& =\\sum_{= 1}^n(\\hat{y}_i - \\bar{y})^2 +\\sum_{= 1}^ne_i^2 + 2\\sum_{= 1}^n (\\hat{y}_i - \\bar{y})e_i\n\\end{align*}\\]last term zero (normal equations):\\(\\sum_{= 1}^n\\hat{y}_ie_i = 0\\) \\(\\sum_{= 1}^ne_i = 0\\).decomposition:\\[\\sum_{= 1}^n ( y_i - \\bar{y})^2 =\\sum_{= 1}^n(\\hat{y}_i - \\bar{y})^2 +\\sum_{= 1}^ne_i^2\\]called ANOVA decomposition.calculations summarised ANOVA table.","code":""},{"path":"SLR.html","id":"for-the-voting-data","chapter":"3 Simple Linear regression","heading":"3.5.3 For the voting data","text":"","code":"\nSST <- sum((hibbs$vote - mean(hibbs$vote))^2)\nSSR <- sum((predict(lin_mod) - mean(hibbs$vote))^2)\nSSE <- sum(residuals(lin_mod)^2)\nSST## [1] 471.905\nSSR## [1] 273.6323\nSSE## [1] 198.2727\nSSR/SST## [1] 0.5798462"},{"path":"SLR.html","id":"anova-table","chapter":"3 Simple Linear regression","heading":"3.5.4 ANOVA table","text":"general, ANOVA table method partitioning variability response variable explained model fitted left .exact form ANOVA table depend model fitted.hypothesis tested model also depend model fitted.ANOVA table simple linear regression model:NOTE:total sum squares, \\(\\mbox{SST} = \\sum_{=1}^{n}(y_{} - \\bar{y})^{2}\\) sum squares \\(y\\) mean. total sum squares depend \\(x\\). (NB: \\(S_{yy}\\))total sum squares, \\(\\mbox{SST} = \\sum_{=1}^{n}(y_{} - \\bar{y})^{2}\\) sum squares \\(y\\) mean. total sum squares depend \\(x\\). (NB: \\(S_{yy}\\))regression sum squares, \\(\\mbox{SSR} = \\sum_{=1}^{n}(\\hat{y}_{} - \\bar{y})^{2}\\). Note \\(\\hat{y}_{}\\) depends \\(x\\).regression sum squares, \\(\\mbox{SSR} = \\sum_{=1}^{n}(\\hat{y}_{} - \\bar{y})^{2}\\). Note \\(\\hat{y}_{}\\) depends \\(x\\).residual/error sum squares, \\(\\mbox{SSE} = \\sum_{=1}^{n}e_{}^{2} = \\sum_{=1}^{n}(y_{} - \\hat{y}_{})^{2}\\).residual/error sum squares, \\(\\mbox{SSE} = \\sum_{=1}^{n}e_{}^{2} = \\sum_{=1}^{n}(y_{} - \\hat{y}_{})^{2}\\).\\(\\mbox{SST} = \\mbox{SSR} + \\mbox{SSE}\\).\\(\\mbox{SST} = \\mbox{SSR} + \\mbox{SSE}\\).sums squares associated degrees freedom (df).\nsums squares associated degrees freedom (df).\nMS = SS/dfMS = SS/dfThe mean squared error \\(\\mbox{MSE}\\) estimates \\(\\sigma^{2}\\).mean squared error \\(\\mbox{MSE}\\) estimates \\(\\sigma^{2}\\).coefficient determination :\n\\[R^{2} = \\frac{\\mbox{SSR}}{\\mbox{SST}} = 1 - \\frac{\\mbox{SSE}}{\\mbox{SST}}.\\]coefficient determination :\n\\[R^{2} = \\frac{\\mbox{SSR}}{\\mbox{SST}} = 1 - \\frac{\\mbox{SSE}}{\\mbox{SST}}.\\]\\(R^{2}\\) always 0 1 measures proportion variation \\(y\\) explained regression \\(x\\).\\(R^{2}\\) always 0 1 measures proportion variation \\(y\\) explained regression \\(x\\).","code":"\nsummary(lin_mod)## \n## Call:\n## lm(formula = vote ~ growth, data = hibbs)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -8.9929 -0.6674  0.2556  2.3225  5.3094 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  46.2476     1.6219  28.514 8.41e-14 ***\n## growth        3.0605     0.6963   4.396  0.00061 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.763 on 14 degrees of freedom\n## Multiple R-squared:  0.5798, Adjusted R-squared:  0.5498 \n## F-statistic: 19.32 on 1 and 14 DF,  p-value: 0.00061"},{"path":"SLR.html","id":"special-cases-1","chapter":"3 Simple Linear regression","heading":"3.5.5 Special cases","text":"\\(R^{2}=1\\), \\(\\mbox{SSR} = \\mbox{SST}\\), \\(\\mbox{SSE}\\) = 0.\\(e_{}=0\\), \\(= 1, \\dots,n\\), data fall straight line.\\(R^{2}=0\\), \\(\\mbox{SSR} = 0\\), \\(\\mbox{SSE} = \\mbox{SST}\\).\\(\\hat{y}_{}=\\bar{y}\\), \\(\\hat{\\beta}_1=0\\).","code":""},{"path":"SLR.html","id":"does-regression-on-x-explain-y","chapter":"3 Simple Linear regression","heading":"3.5.6 Does regression on x explain y?","text":"simple linear regression amounts testing:\\[H_0: \\beta_1 = 0\\]\\[H_A: \\beta_1 \\ne 0\\]can use t-test , equivalent test based F distribution. see later, F-tests wide range applications.\\(H_0\\) holds, \\(\\mbox{SSR}\\) small \\(\\mbox{SSE}\\) large. Therefore large values \\(\\mbox{SSR}\\) relative \\(\\mbox{SSE}\\) provide evidence \\(H_0\\).F-statistic :\\[F=\\frac{\\mbox{SSR}/df_R}{\\mbox{SSE}/df_E}.\\]\\(df_R=1\\) degrees freedom \\(\\mbox{SSR}\\) \\(df_E=n-2\\) degrees freedom \\(\\mbox{SSE}\\).\\(H_0\\),\\(F \\sim F_{1,n-2}\\).dividing SS \\(df\\) put common scale, \\(H_0\\) true:\\[\\mbox{SSR}/1 \\approx \\mbox{SSE}/(n-2)\\]\n\n\\[F_{obs}\\approx 1.\\]Large values \\(F_{obs}\\) provide evidence \\(H_0\\).P-value: \\(P( F_{1,n-2} \\geq F_{obs})\\).","code":""},{"path":"SLR.html","id":"notes-on-the-anova-table-not-examinable","chapter":"3 Simple Linear regression","heading":"3.5.7 Notes on the ANOVA table (not examinable)","text":"\\(\\mathbb{E}[\\mbox{MSR}] = \\sigma^2 + \\beta_1^2 S_{xx}\\).Proof:\\[\\begin{align*}\n\\mbox{MSR} & = \\sum_{= 1}^n (\\hat{y}_i - \\bar{y})^2/1\\\\\n& = \\sum_{= 1}^n (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i - \\bar{y})^2\\\\\n& = \\sum_{= 1}^n (\\bar{y} - \\hat{\\beta}_1 \\bar{x} + \\hat{\\beta}_1x_i - \\bar{y})^2\\\\\n& =\\hat{\\beta}_1^2\\sum_{= 1}^n (x_i - \\bar{x})^2\\\\\n& =\\hat{\\beta}_1^2 S_{xx}\n\\end{align*}\\]\\[\\begin{align*}\n\\mathbb{E}[\\mbox{MSR}] & = \\mathbb{E}[\\hat{\\beta}_1^2 S_{xx}]\\\\\n& =S_{xx} \\mathbb{E}[\\hat{\\beta}_1^2]\\\\\n& = S_{xx} \\left(\\mbox{Var}(\\hat{\\beta}_1) + \\mathbb{E}[\\hat{\\beta}_1]^2 \\right)\\\\\n& = S_{xx} \\left(\\frac{\\sigma^2}{S_{xx}} + \\beta_1^2 \\right)\\\\\n& = \\sigma^2 + \\beta_1^2 S_{xx}\n\\end{align*}\\]\\(\\mathbb{E}[\\mbox{MSE}] = \\sigma^2\\).Proof:\\[\\begin{align*}\n\\mbox{MSE} & = \\frac{\\sum_{= 1}^n (y_i - \\hat{y}_i)^2}{n-2}\\\\\n& = \\frac{\\sum_{= 1}^ne_i^2}{n-2}\n\\end{align*}\\]\\[\\begin{align*}\n\\mathbb{E}[\\mbox{MSE}] & = \\frac{1}{n-2} \\mathbb{E}\\left[\\sum_{= 1}^ne_i^2 \\right]\\\\\n& = \\frac{1}{n-2} \\sum_{= 1}^n\\mathbb{E}[e_i^2]\\\\\n& = \\frac{1}{n-2} \\sum_{= 1}^n\\left( \\mbox{Var}(e_i) + \\mathbb{E}[e_i]^2 \\right)\n\\end{align*}\\]NOTE: \\(\\mathbb{E}[\\epsilon_i] = 0\\), \\(\\mbox{Var}(\\epsilon_i) = \\sigma^2\\), \\(\\mathbb{E}[e_i] = 0\\), \\(\\mbox{Var}(e_i)= \\sigma^2(1- h_{ii})\\). revisit later course.\\[\\begin{align*}\n\\mathbb{E}[\\mbox{MSE}] & = \\frac{1}{n-2} \\sum_{= 1}^n \\left( \\sigma^2( 1 - h_{ii}) + 0 \\right)\\\\\n& =\\frac{1}{n-2} \\sum_{= 1}^n \\sigma^2 \\left(1-\\left(\\frac{1}{n} +\\frac{(x_i - \\bar{x})^2}{S_{xx}}\\right)\\right)\\\\\n& =\\frac{1}{n-2} \\sum_{= 1}^n \\left(\\sigma^2 - \\frac{\\sigma^2}{n} -\\frac{\\sigma^2 (x_i - \\bar{x})^2}{S_{xx}}\\right)\\\\\n& =\\frac{1}{n-2} \\left(\\sigma^2 n- \\sigma^2 -\\frac{\\sigma^2 }{S_{xx}}\\sum_{= 1}^n(x_i - \\bar{x})^2\\right)\\\\\n& = \\frac{1}{n-2} \\left((n-2) \\sigma^2\\right)\\\\\n& =\\sigma^2\n\\end{align*}\\]\\(H_0\\), \\(\\beta_1 = 0\\) \\(\\mathbb{E}[\\mbox{MSE}] = \\mathbb{E}[\\mbox{MSR}]\\).\\(H_0\\), \\(\\beta_1 = 0\\) \\(\\mathbb{E}[\\mbox{MSE}] = \\mathbb{E}[\\mbox{MSR}]\\).\\(\\mbox{MSE} = \\hat{\\sigma}^2\\) can computed using formula:\\(\\mbox{MSE} = \\hat{\\sigma}^2\\) can computed using formula:\\[\\hat{\\sigma}^2 = \\frac{S_{yy} - \\hat{\\beta}_1^2S_{xx}}{n-2}.\\]","code":""},{"path":"SLR.html","id":"sample-correlation","chapter":"3 Simple Linear regression","heading":"3.6 Sample correlation","text":"relationship \\(X\\) \\(Y\\) can examined using scatterplot \\((x_i, y_i)\\).Sample correlation measures strength direction linear association \\(X\\) \\(Y\\). defined :\\[r = \\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}}\\]\\(r\\) estimate population correlation (\\(\\rho\\)) \\(X\\) \\(Y\\).","code":"\ncor(hibbs$vote, hibbs$growth)## [1] 0.7614763\ncor(hibbs$vote, hibbs$growth)^2## [1] 0.5798462"},{"path":"SLR.html","id":"connection-between-correlation-and-regression","chapter":"3 Simple Linear regression","heading":"3.6.0.1 Connection between correlation and regression:","text":"\\(\\hat{\\beta}_1=\\sqrt{\\frac{\\mbox{SST}}{S_{xx}}}r=\\frac{s_y}{s_x}r\\) \\(s_y\\) \\(s_x\\) standard deviations \\(y_i\\)’s \\(x_i\\)’s.\nNote change role \\(X\\) \\(Y\\) resulting regression line slope \\(\\frac{s_x}{s_y}r\\).\\(\\hat{\\beta}_1=\\sqrt{\\frac{\\mbox{SST}}{S_{xx}}}r=\\frac{s_y}{s_x}r\\) \\(s_y\\) \\(s_x\\) standard deviations \\(y_i\\)’s \\(x_i\\)’s.\nNote change role \\(X\\) \\(Y\\) resulting regression line slope \\(\\frac{s_x}{s_y}r\\).\\(r^2 =R^2\\) coefficient determination.\\(r^2 =R^2\\) coefficient determination.SLR model variable \\(x\\) treated fixed \\(y\\) \\(\\epsilon\\) random. convenient think predictor variable fixed even israndom interested behaviour \\(y\\) various fixed \\(x\\) values.SLR model variable \\(x\\) treated fixed \\(y\\) \\(\\epsilon\\) random. convenient think predictor variable fixed even israndom interested behaviour \\(y\\) various fixed \\(x\\) values.One can calculate \\(r\\) pair variables (see next page), correlation assumes variables bivariately normally distributed.One can calculate \\(r\\) pair variables (see next page), correlation assumes variables bivariately normally distributed.Whereas correlation treats variables symmetrically, regression, exploratory variable used explain predict response variable.Whereas correlation treats variables symmetrically, regression, exploratory variable used explain predict response variable.Father son heights (Galton data)","code":""},{"path":"SLR.html","id":"examples-of-correlation","chapter":"3 Simple Linear regression","heading":"3.6.1 Examples of correlation","text":"Anscombe data. Can found Fox, Weisberg, Price (2022).graphs , correlation \\(r = -0.06\\).","code":""},{"path":"SLR.html","id":"comparison-of-the-correlation-and-coefficient-of-determination-for-two-data-sets.","chapter":"3 Simple Linear regression","heading":"3.6.2 Comparison of the correlation and coefficient of determination for two data sets.","text":"","code":"\ncor(X1, Y1)^2## [1] 0.6699889\nsummary(lm(Y1 ~ X1))[8]## $r.squared\n## [1] 0.6699889\ncor(X2, Y2)^2## [1] 0.6895371\nsummary(lm(Y2 ~ X2))[8]## $r.squared\n## [1] 0.6895371"},{"path":"SLR.html","id":"assessing-the-simple-linear-regression-model-assumptions","chapter":"3 Simple Linear regression","heading":"3.7 Assessing the simple linear regression model assumptions","text":"","code":""},{"path":"SLR.html","id":"assumptions-1","chapter":"3 Simple Linear regression","heading":"3.7.1 Assumptions","text":"SLR model, assume \\(y_{} \\sim\\) N(\\(\\beta_{0} + \\beta_{1}x_{}, \\sigma^{2}\\)) \\(y_{}\\)’s independent.Equivalently, since \\(\\epsilon_{} = y_{} - \\beta_{0} - \\beta_{1}x_{}\\), SLR model assumes \\(\\epsilon_{} \\sim\\) N(0, \\(\\sigma^{2}\\)) \\(\\epsilon_{}\\)’s independent identically distributed.want check following:linear relationship, .e. \\(\\mathbb{E}\\)[\\(y_{}\\)] = \\(\\beta_{0} + \\beta_{1}x_{}\\). data follow linear relationship simple linear regression model appropriate.linear relationship, .e. \\(\\mathbb{E}\\)[\\(y_{}\\)] = \\(\\beta_{0} + \\beta_{1}x_{}\\). data follow linear relationship simple linear regression model appropriate.\\(\\epsilon_{}\\)’s constant variance, .e. Var(\\(\\epsilon_{}\\)) = \\(\\sigma^{2}\\) \\(\\). constant variance, line summarise data okay parameter estimate standard errors, estimates \\(\\sigma\\) etc, based incorrect assumptions.\\(\\epsilon_{}\\)’s constant variance, .e. Var(\\(\\epsilon_{}\\)) = \\(\\sigma^{2}\\) \\(\\). constant variance, line summarise data okay parameter estimate standard errors, estimates \\(\\sigma\\) etc, based incorrect assumptions.\\(\\epsilon_{}\\)’s independent.\\(\\epsilon_{}\\)’s independent.\\(\\epsilon_{}\\)’s normally distributed (mean 0).\\(\\epsilon_{}\\)’s normally distributed (mean 0).","code":""},{"path":"SLR.html","id":"violations-and-consequences","chapter":"3 Simple Linear regression","heading":"3.7.2 Violations and consequences","text":"Linearity:straight linear relationship may inadequate.straight linear relationship may appropriate data.linearity violated least squares estimates can biased standard errors may inaccurate.Constant variance:variance constant least squares estimate unbiased standard errors inaccurate.Independence:lack independence least squares estimates unbiased standard errors seriously affected.Normality:Violations normality much impact estimates standard errors.Tests C..’s usually seriously affected C.L.T.","code":""},{"path":"SLR.html","id":"graphical-tools-for-assessment","chapter":"3 Simple Linear regression","heading":"3.7.3 Graphical tools for assessment","text":"Plot \\(y_i\\) versus \\(x_i\\). satisfactory use simple linear regression.Sometimes patterns plot \\(y_i\\) versus \\(x_i\\) difficult detect total variability response variable much larger variability around regression line. Scatterplots residuals vs fits better finding patterns linear component variation responses removed, leaving clearer picture curvature spread. plot alerts user nonlinearity, non-constant variance presence outliers.Plot \\(e_i\\) versus \\(\\hat{y}_i\\) (\\(x_i\\)).satisfactory use simple linear regression.linearity violated \\(\\mathbb{E}[y]\\) monotonic \\(x\\) \\(\\mbox{Var}(y)\\) constant, try transforming \\(x\\) use simple linear regression.linearity violated \\(\\mathbb{E}[y]\\) monotonic, try quadratic regression \\(\\mathbb{E}[y] = \\beta_0+\\beta_1 x+\\beta_2 x^2\\) (look later).linearity violated \\(\\mbox{Var}(y)\\) increases \\(\\mathbb{E}[y]\\), try transforming y use simple linear regression.distribution \\(y\\) \\(\\mathbb{E}[y]\\) skewed, .e. non-normal, use simple linear regression report skewness.linearity violated \\(\\mbox{Var}(y)\\) increases \\(\\mathbb{E}[y]\\), use weighted regression (look later).Normal probability plotThe model assumes normality \\(y\\) \\(\\mathbb{E}[y]\\), , equivalently, normality \\(\\epsilon\\).model assumes normality \\(y\\) \\(\\mathbb{E}[y]\\), , equivalently, normality \\(\\epsilon\\).residuals \\(e_i\\) approximate \\(\\epsilon\\) therefore normal distribution.residuals \\(e_i\\) approximate \\(\\epsilon\\) therefore normal distribution.normal probability (quantile) plot plot \\(z_i\\) versus \\(e_i\\), \\(z_i\\) quantiles standard normal distribution.normal probability (quantile) plot plot \\(z_i\\) versus \\(e_i\\), \\(z_i\\) quantiles standard normal distribution.plot roughly follow straight line pattern.plot roughly follow straight line pattern.Residuals vs. time orderIf data collected time, serial correlation general time trend may occur.data collected time, serial correlation general time trend may occur.plot \\(e_i\\) vs. \\(t_i\\) (time \\(^{th}\\) observation) may examined patterns.plot \\(e_i\\) vs. \\(t_i\\) (time \\(^{th}\\) observation) may examined patterns.Everytime use SLR also draw graphs 1) 3). Also plot 4) appropriate.","code":"\nplot(lin_mod,1)\nplot(lin_mod,2)"},{"path":"SLR.html","id":"cigarette","chapter":"3 Simple Linear regression","heading":"3.7.4 Cigarette Data","text":"FDA data cigarettes, response carbon monoxide, predictor nicotine. Data McIntyre (1994).assess fit construct residual plotsPlot 1: residuals increasing fit, non constant variance.Plot 2: indication assumption Normality unreasonable.3 unusual observations: 3, 19, 25. Obs 3 large negative residual. point upper right fitted line plot. high influence point, meaning big effect fitted line obtained. Obs 19 25 large positive residuals.attempt improve fit, refit model without observation 3.Diagnostic plots:Plot 1: linear pattern, hint non-constant variance.","code":"\nsummary(fit)## \n## Call:\n## lm(formula = carbon.monoxide ~ nicotine)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.3273 -1.2228  0.2304  1.2700  3.9357 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   1.6647     0.9936   1.675    0.107    \n## nicotine     12.3954     1.0542  11.759 3.31e-11 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.828 on 23 degrees of freedom\n## Multiple R-squared:  0.8574, Adjusted R-squared:  0.8512 \n## F-statistic: 138.3 on 1 and 23 DF,  p-value: 3.312e-11\n# studres(fit)"},{"path":"SLR.html","id":"transformations","chapter":"3 Simple Linear regression","heading":"3.7.4.1 Transformations:","text":"can pick best transformation?Examine fitted line plot: linearity, constant variance.Examine fitted line plot: linearity, constant variance.Examine residual vs fit plot: relationship, constant variance, outliers.Examine residual vs fit plot: relationship, constant variance, outliers.Check normality residuals.Check normality residuals.Check sensitivity: whether fit change substantially extreme points removed.Check sensitivity: whether fit change substantially extreme points removed.One can also compare \\(R^2\\) long \\(y\\) values scale. Furthermore, \\(R^2\\) doesn’t follow distribution, can’t compare \\(R^2\\) two models know one meaningfully better.\nNote: interpretation changes transformations.One can also compare \\(R^2\\) long \\(y\\) values scale. Furthermore, \\(R^2\\) doesn’t follow distribution, can’t compare \\(R^2\\) two models know one meaningfully better.Note: interpretation changes transformations.Row 1: delete outlierRow 2: use square root transformation predictor. diminishes influence outlier. residual plot hints small amount bias.Row 3: take square root transformations response predictor.Row 4: take log transformations response predictor.","code":""},{"path":"SLR.html","id":"another-example-diamonds","chapter":"3 Simple Linear regression","heading":"3.7.5 Another Example: Diamonds","text":"Dataset Wickham (2016).SLR model seem good option!Let’s fit anyway explore model diagnostics:Try transformation:","code":"\ndiamonds |> glimpse()## Rows: 53,940\n## Columns: 10\n## $ carat   <dbl> 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.…\n## $ cut     <ord> Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver…\n## $ color   <ord> E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,…\n## $ clarity <ord> SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, …\n## $ depth   <dbl> 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64…\n## $ table   <dbl> 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58…\n## $ price   <int> 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34…\n## $ x       <dbl> 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.…\n## $ y       <dbl> 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.…\n## $ z       <dbl> 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.…\ndiamonds |> ggplot(aes(x = carat, y = price)) + geom_point() \ndiamond_mod <- lm(price~carat, diamonds)\nsummary(diamond_mod)## \n## Call:\n## lm(formula = price ~ carat, data = diamonds)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -18585.3   -804.8    -18.9    537.4  12731.7 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -2256.36      13.06  -172.8   <2e-16 ***\n## carat        7756.43      14.07   551.4   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1549 on 53938 degrees of freedom\n## Multiple R-squared:  0.8493, Adjusted R-squared:  0.8493 \n## F-statistic: 3.041e+05 on 1 and 53938 DF,  p-value: < 2.2e-16\nplot(diamond_mod, 1)\nplot(diamond_mod, 2)\ndiamond_mod2 <- lm(log(price)~log(carat), diamonds)\nplot(diamond_mod2, 1)\nplot(diamond_mod2, 2)"},{"path":"SLR.html","id":"a-note-on-the-galton-paradox","chapter":"3 Simple Linear regression","heading":"3.8 A note on the Galton paradox","text":"","code":""},{"path":"SLR.html","id":"the-galton-paradox","chapter":"3 Simple Linear regression","heading":"3.8.1 The Galton paradox","text":"Sons average taller fathers (1 inch approx)Taller average fathers taller average sons. Regression towards mean: although true, tall people, son’s height average less father’s.suggestion generation offspring near average previous generation many generations offspring uniform heigth. However, observations showed sons variable fathers.apparent paradox?","code":"\napply(father.son, 2, mean)##   father      son \n## 67.68683 68.68423\napply(father.son, 2, sd)##   father      son \n## 2.745827 2.816194"},{"path":"SLR.html","id":"two-regressions","chapter":"3 Simple Linear regression","heading":"3.8.2 Two regressions","text":"Regressing \\(y\\) \\(x\\), treats \\(x\\) variable fixed vertical distances minimized.\nHowevever, regressing \\(x\\) \\(y\\), .e. trying predict fathers’ heights sons’ treats sons’ heights \\(y\\) fixed least squares criterion minimizes horizontal distances.","code":""},{"path":"SLR.html","id":"regression-vs-orthogonal-regression","chapter":"3 Simple Linear regression","heading":"3.8.3 Regression vs orthogonal regression","text":"","code":""},{"path":"multiple-regression.html","id":"multiple-regression","chapter":"4 Multiple regression","heading":"4 Multiple regression","text":"","code":""},{"path":"multiple-regression.html","id":"introductory-examples-1","chapter":"4 Multiple regression","heading":"4.1 Introductory examples","text":"Setup: response variable \\(y\\), predictors \\(x_1\\), \\(x_2\\), …, \\(x_k\\).","code":""},{"path":"multiple-regression.html","id":"example-1-fuel-use","chapter":"4 Multiple regression","heading":"4.1.1 Example 1: Fuel Use","text":"Example Section 2. Information recorded fuel usage average temperature (\\(^\\)) course one week eight office complexes similar size. Data Bowerman Schafer (1990).\\(y\\) = fuel use,\\(x_1\\) = temperature,\\(x_2\\) = chill index.Data:wish use \\(x_1\\) \\(x_2\\) predict \\(y\\). give accurate predictions either \\(x_1\\) \\(x_2\\) alone.multiple linear regression model : fuel use \\(\\approx\\) linear function temperature chill index.precisely:\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon.\\], \\(\\epsilon\\) unobserved error, \\(\\beta_0, \\beta_1, \\beta_2\\) unknown parameters.\\(\\mathbb{E}[\\epsilon ] = 0\\) \\[\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2.\\]SLR can check model appropriateness plotting \\(y\\) vs \\(x\\) observing whether points fall close line. construct 3-d plot \\(y\\), \\(x_1\\), \\(x_2\\) points fall close plane.given set values \\(x_1\\) \\(x_2\\), say \\(x_1 = 45.9\\) \\(x_2 = 8\\), model says mean fuel use :\\[\\mathbb{E}[y] = \\beta_0 + \\beta_1 \\times 45.9 + \\beta_2 \\times 8.\\]\\(x_1 = x_2 = 0\\) \\(\\mathbb{E}[y] = \\beta_0\\), model intercept.interpret \\(\\beta_1\\) suppose \\(x_1 = t\\) \\(x_2 = c\\). \\[\\mathbb{E}[y]=\\beta_0 + \\beta_1 \\times t + \\beta_2 \\times c.\\]Now suppose \\(x_1\\) increases \\(1\\) \\(x_2\\) stays fixed:\\[\\mathbb{E}[y]=\\beta_0 + \\beta_1 \\times (t + 1) + \\beta_2 \\times c.\\]Substracting find \\(\\beta_1\\) increase \\(\\mathbb{E}[y]\\) associated 1 unit increase \\(x_1\\) fixed \\(x_2\\)..e. two weeks chill index whose temperature differed \\(1^o\\) mean fuel use difference \\(\\beta_1\\).","code":""},{"path":"multiple-regression.html","id":"example-2-categorical-predictors","chapter":"4 Multiple regression","heading":"4.1.2 Example 2: Categorical predictors","text":"Suppose wish predict fuel efficiency different car types. Data Cryer Miller (1991). data :\\(y\\) = gallons per mile (gpm),\\(x_1\\) = car weight (w),\\(x_2\\) = transmission type (ttype): 1 = automatic 0 = manual.use model\\[\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2.\\]\\(\\beta_0\\) = mean gpm cars weight \\(w = 0\\) ttype = manual.\n\\(\\beta_1\\) = change mean gpm weight increases 1 ttype.\n\\(\\beta_2\\) = change mean gpm car weight changed manual automatic.model says :\\[\\begin{align*}\n\\mathbb{E}[y] & = \\beta_0 + \\beta_1 x_1 \\quad \\mbox{ manual}\\\\\n& = \\beta_0 + \\beta_2 + \\beta_1 x_1 \\quad \\mbox{ automatic.}\n\\end{align*}\\]Therefore fitting two lines different intercepts slope.data look like:Suppose data look like :suggests fit two lines different intercepts different slopes. introduce third predictor:\\[\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1x_2,\\]giving:\\[\\begin{align*}\n\\mathbb{E}[y] & = \\beta_0 + \\beta_1 x_1 \\quad \\mbox{ manual}\\\\\n& = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) x_1 \\quad \\mbox{ automatic.}\n\\end{align*}\\]term \\(x_1x_2\\) called interaction term.:\\(\\beta_2\\) = difference intercept\\(\\beta_3\\) = difference slope.","code":""},{"path":"multiple-regression.html","id":"example-3-polynomials","chapter":"4 Multiple regression","heading":"4.1.3 Example 3: Polynomials","text":"one predictor \\(x\\) plot \\(y\\) vs \\(x\\) exhibits quadratic pattern.\\[\\mathbb{E}[y] = \\beta_0 + \\beta_1 x + \\beta_2 x^2.\\]also called quadratic regression model , generally, polynomial regression model.Higher order polynomial regression models can also used needed.Link: http://www.rpubs.com/kdomijan/333155","code":""},{"path":"multiple-regression.html","id":"example-4-nonlinear-relationships","chapter":"4 Multiple regression","heading":"4.1.4 Example 4: Nonlinear relationships","text":"example,\\[y = \\alpha x_1 ^{\\beta x_2} \\epsilon.\\]Nonlinear models can sometimes linearized, example:\\[log(y) = log(\\alpha) + \\beta x_2 log(x_1) + log(\\epsilon).\\]: \\(x = x_2 log(x_1)\\).NOTE: term linear refers linearity regression parameters.general form multiple linear regression model (two explanatory variables):\\[y = \\beta_0 f_0(x_1, x_2) + \\beta_1 f_1(x_1, x_2) + \\beta_2 f_2(x_1, x_2) + \\dots\\]\\(f_j(x_1, x_2)\\) known functions explanatory variables.extension two explanatory variables straightforward.","code":""},{"path":"multiple-regression.html","id":"diamonds-data","chapter":"4 Multiple regression","heading":"4.1.5 Diamonds data","text":"","code":"\ndiamonds |> ggplot(aes(x = carat, y = log(price), col = color)) + geom_point() +\n  facet_wrap(~clarity)\n# model.orig <- lm(log(price) ~ poly(carat,2) + color + clarity +cut,  diamonds)\n# summary(model.orig)"},{"path":"multiple-regression.html","id":"cigarette-data-continued","chapter":"4 Multiple regression","heading":"4.1.6 Cigarette Data continued","text":"Data 3.7.4.\nConsider second predictor (weight):Regression (nicotine )Regression (weight )Regression (predictors)Regression (quadratic)","code":"\nsummary(fit)## \n## Call:\n## lm(formula = carbon.monoxide ~ nicotine)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.3273 -1.2228  0.2304  1.2700  3.9357 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   1.6647     0.9936   1.675    0.107    \n## nicotine     12.3954     1.0542  11.759 3.31e-11 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.828 on 23 degrees of freedom\n## Multiple R-squared:  0.8574, Adjusted R-squared:  0.8512 \n## F-statistic: 138.3 on 1 and 23 DF,  p-value: 3.312e-11\nsummary(fit2)## \n## Call:\n## lm(formula = carbon.monoxide ~ weight)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -6.524 -2.533  0.622  2.842  7.268 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)  \n## (Intercept)  -11.795      9.722  -1.213   0.2373  \n## weight        25.068      9.980   2.512   0.0195 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.289 on 23 degrees of freedom\n## Multiple R-squared:  0.2153, Adjusted R-squared:  0.1811 \n## F-statistic: 6.309 on 1 and 23 DF,  p-value: 0.01948\nfit3 <- lm(carbon.monoxide ~ weight + nicotine)\nsummary(fit3)## \n## Call:\n## lm(formula = carbon.monoxide ~ weight + nicotine)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.3304 -1.2249  0.2314  1.2677  3.9371 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  1.61398    4.44663   0.363    0.720    \n## weight       0.05883    5.02395   0.012    0.991    \n## nicotine    12.38812    1.24473   9.952 1.32e-09 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.87 on 22 degrees of freedom\n## Multiple R-squared:  0.8574, Adjusted R-squared:  0.8444 \n## F-statistic: 66.13 on 2 and 22 DF,  p-value: 4.966e-10\nnicotine.sq <- nicotine^2\nfit4 <- lm(carbon.monoxide ~ nicotine + nicotine.sq)\nsummary(fit4)## \n## Call:\n## lm(formula = carbon.monoxide ~ nicotine + nicotine.sq)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -2.9857 -1.1052  0.1834  0.8654  3.4145 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   -1.784      1.453  -1.227  0.23264    \n## nicotine      20.111      2.775   7.248 2.92e-07 ***\n## nicotine.sq   -3.730      1.267  -2.945  0.00749 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.583 on 22 degrees of freedom\n## Multiple R-squared:  0.8977, Adjusted R-squared:  0.8884 \n## F-statistic: 96.53 on 2 and 22 DF,  p-value: 1.284e-11"},{"path":"multiple-regression.html","id":"least-squares-estimation-for-multiple-regression","chapter":"4 Multiple regression","heading":"4.2 Least squares estimation for multiple regression","text":"model states :\\[y = \\beta_0 + \\beta_1x_{1} + \\beta_2x_{2} + ... + \\beta_kx_k + \\epsilon,\\]\\(k<n\\).observation :\\[y_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + ... + \\beta_kx_{ik} + \\epsilon_i.\\]can write compactly using matrix notation.Let \\(\\mathbf{Y}\\) response vector:\\[\\mathbf{Y} =\\begin{bmatrix}\ny_{1} \\\\\ny_{2} \\\\\n\\vdots\\\\\ny_{n}\n\\end{bmatrix}\\]Let \\(\\mathbf{X}\\) \\(n \\times p\\) matrix, \\(p = k+1\\):\\[\\mathbf{X} = \\begin{bmatrix}\n1 & x_{11} & x_{12} & x_{13} & \\dots  & x_{1k} \\\\\n1 & x_{21} & x_{22} & x_{23} & \\dots  & x_{2k} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 &  x_{n1} & x_{n2} & x_{n3} & \\dots  & x_{nk}\n\\end{bmatrix}\\]Let \\(\\boldsymbol{\\beta}\\) \\(p\\)-dim parameter vector:\\[\\boldsymbol{\\beta} =\\begin{bmatrix}\n\\beta_{0} \\\\\n\\beta_{1} \\\\\n\\vdots\\\\\n\\beta_{k}\n\\end{bmatrix}\\]Let \\(\\boldsymbol{\\epsilon}\\) \\(n\\)-dim error vector:\\[\\boldsymbol{\\epsilon} =\\begin{bmatrix}\n\\epsilon_{1} \\\\\n\\epsilon_{2} \\\\\n\\vdots\\\\\n\\epsilon_{n}\n\\end{bmatrix}\\]model states :\\[\\mathbf{Y}=\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}.\\]vector fitted values :\\[\\hat{\\mathbf{Y}}=\\mathbf{X}\\hat{\\boldsymbol{\\beta}}.\\]corresponding residual values :\\[\\mathbf{e}=\\mathbf{Y}-\\hat{\\mathbf{Y}}.\\]OLS estimates minimise:\\[S(\\boldsymbol{\\beta}) = \\sum_{=1}^{n}(y_i-\\beta_0-\\beta_1x_{i1}- ... - \\beta_kx_{ik})^2\\]\\(\\boldsymbol{\\beta}\\).Therefore OLS estimates satisfy:\\[\\frac{\\delta S(\\boldsymbol{\\beta})}{\\delta \\beta_j} = 0, \\quad \\forall j\\]evaluate \\(\\hat{\\boldsymbol{\\beta}}\\)\\[\\frac{\\delta S(\\boldsymbol{\\beta})}{\\delta \\beta_0} = -2 \\sum_{=1}^{n}(y_i-\\beta_0-\\beta_1x_{i1}- ... - \\beta_kx_{ik})\\]\\[\\frac{\\delta S(\\boldsymbol{\\beta})}{\\delta \\beta_j} = -2 \\sum_{=1}^{n} x_{ij}(y_i-\\beta_0-\\beta_1x_{i1}- ... - \\beta_kx_{ik}), \\quad \\forall j = 1,...,k.\\]OLS estimates \\(\\boldsymbol{\\beta}\\) satisfy:\\[\\sum_{=1}^{n}(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1x_{i1}- ... - \\hat{\\beta}_kx_{ik}) = 0\\]\\[\\sum_{=1}^{n}(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1x_{i1}- ... - \\hat{\\beta}_kx_{ik})x_{ij} = 0, \\quad \\forall j = 1,...,k.\\]normal equations (see (3.1) (3.2)) can written :\\[\\sum_{=1}^{n}e_i = 0\\]\\[\\sum_{=1}^{n}x_{ij}e_i = 0, \\quad \\forall j = 1,...,k.\\]can combine one matrix equation:\\[\\mathbf{X}^T\\mathbf{e}= \\mathbf{0}\\]equivalently:\\[\\mathbf{X}^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})= \\mathbf{0}\\]Therefore OLS estimator \\(\\hat{\\boldsymbol{\\beta}}\\) satisfies:\\[\\begin{align}\n\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} &= \\mathbf{X}^T\\mathbf{Y}\\\\\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{Y}.\\tag{4.1}\n\\end{align}\\]Matrix \\(\\mathbf{X}^T\\mathbf{X}\\) non-singular (.e inverse) iff \\(rank(\\mathbf{X}) =p\\), .e. \\(\\mathbf{X}\\) full rank columns \\(\\mathbf{X}\\) linearly independent.","code":""},{"path":"multiple-regression.html","id":"estimation-of-sigma2-varepsilon","chapter":"4 Multiple regression","heading":"4.2.1 Estimation of \\(\\sigma^2\\) = Var\\((\\epsilon)\\)","text":"point estimate \\(\\sigma^2\\) mean squared error:\\[\\hat{\\sigma}^2 = \\mbox{MSE} = \\frac{\\mbox{SSE}}{n-p} = \\frac{\\sum_{=1}^n e_i^2}{n-p}.\\]","code":""},{"path":"multiple-regression.html","id":"estimation-of-varhatbeta","chapter":"4 Multiple regression","heading":"4.2.2 Estimation of Var\\((\\hat{\\beta})\\)","text":"\\[\\mbox{Var}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^T\\mathbf{X})^{-1} \\sigma^2.\\]\\[\\widehat{\\mbox{Var}(\\hat{\\boldsymbol{\\beta}})} = (\\mathbf{X}^T\\mathbf{X})^{-1} \\hat{\\sigma}^2.\\]","code":""},{"path":"multiple-regression.html","id":"prediction-from-multiple-linear-regression-model","chapter":"4 Multiple regression","heading":"4.3 Prediction from multiple linear regression model","text":"seen already, predict multiple regression model use:\\[\\hat{y}_i = \\hat{\\beta}_0+ \\hat{\\beta}_1x_{i1}+ \\cdots+\\hat{\\beta}_kx_{ik}\\]\\[\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\]particular set \\(x_0\\) values predict response \\(y_0\\) :\\[\\hat{y}_0 = \\mathbf{x}_0^T\\hat{\\boldsymbol{\\beta}}\\]\\(\\mathbf{x}_0^T = ( 1, x_{01},x_{02},..., x_{0k})\\).also use \\(\\hat{y}_0\\) estimate \\(\\mathbb{E}(y_0)\\), mean \\(y_0\\) given set \\(x_0\\) values.\\(\\mbox{S.E.}\\) estimate mean \\(\\mathbb{E}(y_0)\\) :\\[\\mbox{S.E.}_{\\mbox{fit}} (\\hat{y}_0)= \\hat{\\sigma}\\sqrt{\\mathbf{x}_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_0}.\\]\\(1-\\alpha\\) confidence interval expected response \\(\\mathbf{x}_0\\) given :\\[\\hat{y}_0 \\pm t_{n-p}(\\alpha/2) \\times \\mbox{S.E.}_{\\mbox{fit}} (\\hat{y}_0).\\]\\(\\mbox{S.E.}\\) predicted \\(y_0\\):\\[\\mbox{S.E.}_{\\mbox{pred}}(\\hat{y}_0) = \\hat{\\sigma}\\sqrt{1+ \\mathbf{x}_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_0}.\\]Note: \\[\\mbox{S.E.}_{\\mbox{pred}}(\\hat{y}_0)= \\sqrt{\\hat{\\sigma}^2+\\mbox{S.E.}_{\\mbox{fit}}(\\hat{y}_0)^2}\\]","code":""},{"path":"multiple-regression.html","id":"regression-models-in-matrix-notation-examples","chapter":"4 Multiple regression","heading":"4.4 Regression models in matrix notation: examples","text":"","code":""},{"path":"multiple-regression.html","id":"example-1-slr","chapter":"4 Multiple regression","heading":"4.4.1 Example 1: SLR","text":"\\(\\mathbf{X}\\) matrix :\\[\\mathbf{X} = \\begin{bmatrix}\n1 & x_{1}\\\\\n\\vdots & \\vdots\\\\\n1 &x_{n}\n\\end{bmatrix}\\]estimate coefficients \\(\\hat{\\boldsymbol{\\beta}}\\):\\[\\begin{align*}\n\\mathbf{X}^T\\mathbf{X} &= \\begin{bmatrix}\nn & \\sum x_{}\\\\\n\\sum x_{}& \\sum x_{}^2\n\\end{bmatrix}\\\\\n(\\mathbf{X}^T\\mathbf{X})^{-1} & = \\frac{1}{n \\sum x_{}^2 - (\\sum x_{})^2}\\begin{bmatrix}\n\\sum x_{}^2& -\\sum x_{}\\\\\n-\\sum x_{}  & n\n\\end{bmatrix} \\\\\n& = \\frac{1}{n (\\sum x_{}^2 - n\\bar{x}^2)}\\begin{bmatrix}\n\\sum x_{}^2  & -n\\bar{x}  \\\\\n-n\\bar{x}  & n\n\\end{bmatrix} \\\\\n& = \\frac{1}{S_{xx}}\\begin{bmatrix}\n\\sum x_{}^2/n  & -\\bar{x}  \\\\\n-\\bar{x}  & 1\n\\end{bmatrix} \\\\\n\\mathbf{X}^T\\mathbf{Y} &= \\begin{bmatrix}\n\\sum y_{}  \\\\\n\\sum x_{}y_{}\n\\end{bmatrix} = \\begin{bmatrix}\nn\\bar{y}  \\\\\n\\sum x_{}y_{}\n\\end{bmatrix}\\\\\n\\hat{\\boldsymbol{\\beta}} & = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\\\\n& = \\frac{1}{S_{xx}}\\begin{bmatrix}\n\\bar{y}\\sum x_{}^2 -\\bar{x} \\sum x_{}y_i \\\\\n-n \\bar{x} \\bar{y} + \\sum x_{}y_i\n\\end{bmatrix}\n\\end{align*}\\]algebra, gives:\\[\\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}}\\]\n\\[\\hat{\\beta}_0= \\bar{y} - \\hat{\\beta}_1\\bar{x}\\], \\[\\begin{align*}\n\\mbox{Var}(\\hat{\\boldsymbol{\\beta}})& = (\\mathbf{X}^T\\mathbf{X})^{-1}\\sigma^2\\\\\n& = \\frac{\\sigma^2}{S_{xx}}\n\\begin{bmatrix}\n\\sum x_{}^2/n& -\\bar{x} \\\\\n-\\bar{x}& 1\n\\end{bmatrix}\n\\end{align*}\\]gives\\[\\mbox{Var}(\\hat{\\beta}_0) = \\sigma^2\\left(\\frac{1}{n}+ \\frac{\\bar{x}^2}{S_{xx}}\\right),\\]\\[\\mbox{Var}(\\hat{\\beta}_1) = \\frac{\\sigma^2}{S_{xx}},\\]\\[\\mbox{Cov}(\\hat{\\beta}_0, \\hat{\\beta}_1) = -\\bar{x}\\frac{\\sigma^2}{S_{xx}}.\\]","code":""},{"path":"multiple-regression.html","id":"example-2","chapter":"4 Multiple regression","heading":"4.4.2 Example 2","text":"Example Stapleton (2009), Problem 3.1.1, pg 81.scale 2 pans. measurement given scale difference weight pan 1 pan 2, plus random error \\(\\epsilon\\).Suppose \\(\\mathbb{E}[\\epsilon] = 0\\) \\(\\mbox{Var}(\\epsilon) = \\sigma^2\\) \\(\\epsilon_i\\) independent.Suppose also two objects weight \\(\\beta_1\\) \\(\\beta_2\\) 4 measurements taken:Pan 1: object 1, Pan 2: emptyPan 1: empty, Pan 2: object 2Pan 1: object 1, Pan 2: object 2Pan 1: object 1 2, Pan 2: emptyLet \\(y_1\\), \\(y_2\\), \\(y_3\\) \\(y_4\\) four observations. :\\[\\begin{align*}\ny_1 & = \\beta_1 + \\epsilon_1\\\\\ny_2 & =- \\beta_2 + \\epsilon_2\\\\\ny_3 & = \\beta_1 - \\beta_2 + \\epsilon_3\\\\\ny_4 & = \\beta_1 + \\beta_2 + \\epsilon_4\\\\\n\\end{align*}\\]\\(\\mathbf{X} = \\begin{bmatrix}\n1 &0  \\\\\n0 & -1  \\\\\n1 & -1  \\\\\n1 &  1\n\\end{bmatrix}\\)\\(\\mathbf{Y} = \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\ny_3  \\\\\ny_4\n\\end{bmatrix}\\)\\(\\boldsymbol{\\beta} = \\begin{bmatrix}\n\\beta_1 \\\\\n\\beta_2\n\\end{bmatrix}\\)\\(\\boldsymbol{\\epsilon} = \\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\epsilon_3  \\\\\n\\epsilon_4\n\\end{bmatrix}\\)model :\\[\\mathbf{Y} = \\begin{bmatrix}\n1 &0  \\\\\n0 & -1  \\\\\n1 & -1  \\\\\n1 &  1\n\\end{bmatrix} \\times\\begin{bmatrix}\n\\beta_1 \\\\\n\\beta_2\n\\end{bmatrix} +  \\boldsymbol{\\epsilon} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\]OLS estimates \\(\\boldsymbol{\\beta}\\) given :\\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{Y}.\\]\\[\\begin{align*}\n\\mathbf{X}^T\\mathbf{X} & = \\begin{bmatrix}\n1 &  0  &  1 & 1\\\\\n0 & -1  & -1 & 1\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1 &  0  \\\\\n0 & -1  \\\\\n1 & -1  \\\\\n1 &  1\\\\\n\\end{bmatrix}\\\\\n& = \\begin{bmatrix}\n3 &  0 \\\\\n0 &  3 \\\\\n\\end{bmatrix}\\\\\n& = 3\\begin{bmatrix}\n1 &  0 \\\\\n0 &  1 \\\\\n\\end{bmatrix}\\\\\n\\mathbf{X}^T\\mathbf{Y} &= \\begin{bmatrix}\ny_1 + y_3 + y_4\\\\\n-y_2 - y_3 + y_4\\\\\n\\end{bmatrix}\\\\\n\\hat{\\boldsymbol{\\beta}} & = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{Y}\\\\\n& = \\frac{1}{3}\\begin{bmatrix}\n1 &  0 \\\\\n0 &  1 \\\\\n\\end{bmatrix} \\begin{bmatrix}\ny_1 + y_3 + y_4\\\\\n-y_2 - y_3 + y_4\\\\\n\\end{bmatrix}\\\\\n& = \\frac{1}{3}\\begin{bmatrix}\ny_1 + y_3 + y_4\\\\\n-y_2 - y_3 + y_4\\\\\n\\end{bmatrix}\\\\\n\\mbox{Var}(\\hat{\\boldsymbol{\\beta}}) &= (\\mathbf{X}^T\\mathbf{X})^{-1}  \\sigma^2  = \\frac{1}{3}\\begin{bmatrix}\n1 &  0 \\\\\n0 &  1 \\\\\n\\end{bmatrix}  \\sigma^2.\\\\\n\\end{align*}\\]Can improve experiment 4 measurements yield estimates \\(\\boldsymbol{\\beta}\\) smaller variance?present design: \\(\\mbox{Var}(\\hat{\\beta}_i) = \\frac{\\sigma^2}{3}\\)present design: \\(\\mbox{Var}(\\hat{\\beta}_i) = \\frac{\\sigma^2}{3}\\)Let \\(\\mathbf{X} = \\begin{bmatrix}\n1 & -1  \\\\\n1 & -1  \\\\\n1 &  1  \\\\\n1 &  1\n\\end{bmatrix}\\),Let \\(\\mathbf{X} = \\begin{bmatrix}\n1 & -1  \\\\\n1 & -1  \\\\\n1 &  1  \\\\\n1 &  1\n\\end{bmatrix}\\),\\(\\mbox{Var}(\\hat{\\beta}_i) = \\frac{\\sigma^2}{4}\\)Let \\(\\mathbf{X} = \\begin{bmatrix}\n1 &  0  \\\\\n1 &  0  \\\\\n0 &  1  \\\\\n0 &  1\n\\end{bmatrix}\\),\\(\\mbox{Var}(\\hat{\\beta}_i) = \\frac{\\sigma^2}{2}\\).","code":""},{"path":"multiple-regression.html","id":"the-formal-multiple-regression-model-and-properties","chapter":"4 Multiple regression","heading":"4.5 The formal multiple regression model and properties","text":"","code":""},{"path":"multiple-regression.html","id":"concepts-random-vectors-covariance-matrix-multivariate-normal-distribution-mvn.","chapter":"4 Multiple regression","heading":"4.5.1 Concepts: random vectors, covariance matrix, multivariate normal distribution (MVN).","text":"Let \\(Y_1,...,Y_n\\) r.v.s defined common probability space.\\(\\mathbf{Y}\\) random vector.Let \\(\\mu_i = \\mathbb{E}[y_i]\\) \\(\\boldsymbol{\\mu} = \\begin{bmatrix}\n\\mu_1 \\\\\n\\vdots \\\\\n\\mu_n\n\\end{bmatrix}\\).\\(\\boldsymbol{\\mu}\\) mean vector write:\\[\\mathbb{E}[\\mathbf{Y}] = \\boldsymbol{\\mu}.\\]Let \\(\\sigma_{ij} = Cov(y_i, y_j)\\). \\(\\boldsymbol{\\Sigma}\\) covariance matrix \\(\\mathbf{Y}\\), \\(\\boldsymbol{\\Sigma}_{ij} = [\\sigma_{ij}].\\)write:\\[\\mbox{Var}(\\mathbf{Y}) = \\boldsymbol{\\Sigma}.\\]Aside:\\(\\mbox{Cov}(Y_i,Y_j) = \\mathbb{E}[(Y_i-\\mathbb{E}[Y_i])(Y_j-\\mathbb{E}[Y_j])]\\)\\(\\mbox{Cov}(Y_i,Y_i) = \\mbox{Var}(Y_i)\\)\\(Y_i,Y_j\\) independent \\(\\mbox{Cov}(Y_i,Y_j) = 0\\).\\(Y_i,Y_j\\) bivariate normal distribution, \\(\\mbox{Cov}(Y_i,Y_j) = 0\\), \\(Y_i,Y_j\\) independent.Fact: Suppose \\(\\mathbf{Y}\\) mean \\(\\boldsymbol{\\mu}\\) variance \\(\\boldsymbol{\\Sigma}\\). vector constants \\(\\mathbf{b}\\) matrix constants \\(\\mathbf{C}\\):\\[\\mathbb{E}[\\mathbf{C}\\mathbf{Y} + \\mathbf{b}] = \\mathbf{C}\\boldsymbol{\\mu} + \\mathbf{b}\\]\\[\\mbox{Var}( \\mathbf{C}\\mathbf{Y} + \\mathbf{b} ) = \\mathbf{C}\\boldsymbol{\\Sigma} \\mathbf{C}^T.\\]Defn: random \\(n\\) - dim vector \\(\\mathbf{Y}\\) said MVN distribution \\(\\mathbf{Y}\\) can written \n\\[\\mathbf{Y} = \\mathbf{}\\mathbf{Z} + \\boldsymbol{\\mu}\\]\n:\\(Z_1, Z_2, ..., Z_p\\) iid N(0,1),\\(Z_1, Z_2, ..., Z_p\\) iid N(0,1),\\(\\mathbf{}\\) \\(n \\times p\\) matrix constants \\(\\mathbf{}\\) \\(n \\times p\\) matrix constants \\(\\boldsymbol{\\mu}\\) \\(n\\) vector constants.\\(\\boldsymbol{\\mu}\\) \\(n\\) vector constants.Notes:Random vector \\(\\mathbf{Z}\\) multivariate normal mean \\(\\mathbf{0}\\) covariance \\(\\mathbf{}_p\\) since \\(Z_i\\)s independent covariances 0. write: \\[\\mathbf{Z} \\sim N_p(\\mathbf{0}, \\mathbf{}_p).\\]Random vector \\(\\mathbf{Z}\\) multivariate normal mean \\(\\mathbf{0}\\) covariance \\(\\mathbf{}_p\\) since \\(Z_i\\)s independent covariances 0. write: \\[\\mathbf{Z} \\sim N_p(\\mathbf{0}, \\mathbf{}_p).\\]\\(\\mathbb{E}[\\mathbf{Y}] =  \\mathbb{E}[\\mathbf{}\\mathbf{Z} + \\boldsymbol{\\mu}] = \\boldsymbol{\\mu}\\),\\(\\mathbb{E}[\\mathbf{Y}] =  \\mathbb{E}[\\mathbf{}\\mathbf{Z} + \\boldsymbol{\\mu}] = \\boldsymbol{\\mu}\\),\\(\\mbox{Var}(\\mathbf{Y}) = \\mathbf{}\\mathbf{}^T\\),\\(\\mbox{Var}(\\mathbf{Y}) = \\mathbf{}\\mathbf{}^T\\),\\(\\mathbf{Y} \\sim N_n (\\boldsymbol{\\mu}, \\mathbf{}\\mathbf{}^T)\\).\\(\\mathbf{Y} \\sim N_n (\\boldsymbol{\\mu}, \\mathbf{}\\mathbf{}^T)\\).","code":""},{"path":"multiple-regression.html","id":"multiple-regression-model","chapter":"4 Multiple regression","heading":"4.5.2 Multiple regression model","text":"\\[\\mathbf{Y}=\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}.\\]\\(\\mathbf{Y}=\\) \\(n\\) - dimensional response random vector.\\(\\boldsymbol{\\beta}=\\) unknown \\(p\\) - dimensional parameter vector.\\(\\mathbf{X}=\\) \\(n \\times p\\) matrix constants.\\(\\boldsymbol{\\epsilon}=\\) \\(n\\) - dimensional error vector.Assumptions:Linearity: \\(\\mathbb{E}[\\boldsymbol{\\epsilon} ] =\\mathbf{0}\\), hence \\(\\mathbb{E}[\\mathbf{Y}] = \\mathbf{X}\\boldsymbol{\\beta}\\).Linearity: \\(\\mathbb{E}[\\boldsymbol{\\epsilon} ] =\\mathbf{0}\\), hence \\(\\mathbb{E}[\\mathbf{Y}] = \\mathbf{X}\\boldsymbol{\\beta}\\).Constant variance 0 covariances \\(\\mbox{Var}(\\boldsymbol{\\epsilon}) = \\sigma^2 I_n\\) \\(\\mbox{Var}(\\mathbf{Y}) = \\sigma^2 I_n\\).Constant variance 0 covariances \\(\\mbox{Var}(\\boldsymbol{\\epsilon}) = \\sigma^2 I_n\\) \\(\\mbox{Var}(\\mathbf{Y}) = \\sigma^2 I_n\\).MVN distribution: \\(\\boldsymbol{\\epsilon} \\sim N_n(\\mathbf{0},\\sigma^2 I_n )\\) \\(\\mathbf{Y} \\sim N_n(\\mathbf{X}\\boldsymbol{\\beta},\\sigma^2 I_n )\\)MVN distribution: \\(\\boldsymbol{\\epsilon} \\sim N_n(\\mathbf{0},\\sigma^2 I_n )\\) \\(\\mathbf{Y} \\sim N_n(\\mathbf{X}\\boldsymbol{\\beta},\\sigma^2 I_n )\\)Notes: diagonal entries covariance matrix MVN distribution 0, \\(Y_1, ..., Y_n\\) independent.Theorem 4.1  Let \\(\\hat{\\boldsymbol{\\beta}}\\) OLS estimator \\(\\boldsymbol{\\beta}\\). model assumptions hold:\\[\\hat{\\boldsymbol{\\beta}} \\sim N_p(\\boldsymbol{\\beta}, (\\mathbf{X}^T\\mathbf{X})^{-1}\\sigma^2)\\]Corollary: \\(\\hat{\\beta}_j \\sim N(\\beta_j, c_{jj}\\sigma^2)\\), \\(c_{jj}\\) \\(jj\\) entry \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\) \\(j = 0, ..., k\\).Theorem 4.2  Let \\(\\hat{\\sigma}^2 = \\frac{\\mbox{SSE}}{n-p}\\). model assumptions hold:\\[(n-p)\\frac{\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi ^2_{(n-p)}\\]\\[\\mathbb{E}[\\hat{\\sigma}^2] =\\sigma^2\\]distribution \\(\\hat{\\sigma}^2\\) independent \\(\\hat{\\boldsymbol{\\beta}}\\).Corollary:\\[\\frac{\\hat{\\beta}_j - \\beta_j }{\\hat{\\sigma} \\sqrt{c_{jj}}} \\sim t_{n-p}\\]\ncan tests obtain CIs \\(\\beta_j\\)","code":""},{"path":"multiple-regression.html","id":"the-hat-matrix","chapter":"4 Multiple regression","heading":"4.6 The hat matrix","text":"vector fitted values:\\[\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{Y} = \\mathbf{H}\\mathbf{Y}.\\]hat matrix (also known projection matrix):\\[\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\]dimension \\(n \\times n\\)\nsymmetric (\\(\\mathbf{H}^T = \\mathbf{H}\\)) \nidempotent (\\(\\mathbf{H}^2 = \\mathbf{H}\\mathbf{H} = \\mathbf{H}\\)).\\[\\hat{\\mathbf{Y}}= \\mathbf{H}\\mathbf{Y}\\]\\[\\mathbf{e}= \\mathbf{Y} - \\hat{\\mathbf{Y}} =\\mathbf{Y} - \\mathbf{H}\\mathbf{Y} = (\\mathbf{} - \\mathbf{H})\\mathbf{Y}\\]\\[\\mbox{SSE} = \\mathbf{e}^T\\mathbf{e} = \\mathbf{Y}^T (\\mathbf{} - \\mathbf{H})\\mathbf{Y}\\]","code":""},{"path":"multiple-regression.html","id":"the-qr-decomposition-of-a-matrix","chapter":"4 Multiple regression","heading":"4.6.1 The QR Decomposition of a matrix","text":"seen OLS estimates \\(\\boldsymbol{\\beta}\\) can found using:\\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{Y}.\\]Inverting \\(\\mathbf{X}^T\\mathbf{X}\\) matrix can sometimes introduce significant rounding errors calculations software packages use QR decomposition design matrix \\(\\mathbf{X}\\) compute parameter estimates. E.g. take look documentation lm method R.work?need find \\(n \\times p\\) matrix \\(\\mathbf{Q}\\) \\(p \\times p\\) matrix \\(\\mathbf{R}\\) :\\[\\mathbf{X}=\\mathbf{Q}\\mathbf{R}\\]\\(\\mathbf{Q}\\) orthonormal columns, .e. \\(\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{}_p\\)\\(\\mathbf{Q}\\) orthonormal columns, .e. \\(\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{}_p\\)\\(\\mathbf{R}\\) upper triangular matrix.\\(\\mathbf{R}\\) upper triangular matrix.several methods computing \\(\\mathbf{Q}\\mathbf{R}\\) factorization (won’t study , high quality code computation exists publicly available Lapack package {http://www.netlib.org/lapack/lug/}).can show :\\[\\begin{align*}\n\\mathbf{X} &=\\mathbf{Q}\\mathbf{R} \\\\\n\\mathbf{X}^T\\mathbf{X} &=(\\mathbf{Q}\\mathbf{R})^T(\\mathbf{Q}\\mathbf{R}) = \\mathbf{R}^T\\mathbf{R}\\\\\n\\end{align*}\\]:\\[\\begin{align*}\n(\\mathbf{X}^T\\mathbf{X})\\hat{\\boldsymbol{\\beta}} & =\\mathbf{X}^T\\mathbf{Y}\\\\\n(\\mathbf{R}^T\\mathbf{R})\\hat{\\boldsymbol{\\beta}} & =\\mathbf{R}^T\\mathbf{Q}^T\\mathbf{Y}\\\\\n\\mathbf{R}\\hat{\\boldsymbol{\\beta}} & = \\mathbf{Q}^T\\mathbf{Y}\\\\\n\\end{align*}\\]Since \\(\\mathbf{R}\\) triangular matrix can use backsolving easy equation solve.can also show hat matrix becomes:\\[\\mathbf{H} = \\mathbf{Q}\\mathbf{Q}^T\\]\n","code":""},{"path":"multiple-regression.html","id":"anova-for-multiple-regression","chapter":"4 Multiple regression","heading":"4.7 ANOVA for multiple regression","text":"Recap: ANOVA decomposition\\[\\begin{align*}\n\\mbox{SSR} & = \\sum_{=1}^n(\\hat{y}_i - \\bar{y})^2 = \\sum_{=1}^n \\hat{y}_i ^2- n\\bar{y}^2 \\\\\n\\mbox{SSE} & = \\sum_{=1}^n(y_i - \\hat{y}_i)^2 = \\sum_{=1}^n e_i^2\\\\\n\\mbox{SST} & = \\sum_{=1}^n(y_i - \\bar{y})^2 = \\sum_{=1}^n y_i ^2- n\\bar{y}^2 \\\\\n\\end{align*}\\]Theorem 4.3  \\(\\mbox{SST} = \\mbox{SSR} + \\mbox{SSE}\\)Proof: follows decomposition response = fit + residual.\\[\\begin{align*}\n\\mathbf{Y} & = \\hat{\\mathbf{Y}}  + \\mathbf{e}\\\\\n\\mathbf{Y}^T\\mathbf{Y} & = (\\hat{\\mathbf{Y}} + \\mathbf{e})^T (\\hat{\\mathbf{Y}} + \\mathbf{e})\\\\\n& = \\hat{\\mathbf{Y}}^T\\hat{\\mathbf{Y}} + \\mathbf{e}^T\\mathbf{e}+ 2\\hat{\\mathbf{Y}}^T\\mathbf{e} \\\\\n\\end{align*}\\]\\(\\hat{\\mathbf{Y}}^T = \\hat{\\boldsymbol{\\beta}}^T\\mathbf{X}^T\\) \\(\\mathbf{X}^T\\mathbf{e} = 0\\), normal equations, \n\\(\\hat{\\mathbf{Y}}^T\\mathbf{e} = 0\\).Alternatively: \\(\\hat{\\mathbf{Y}} = \\mathbf{H}\\mathbf{Y}\\), \\(\\mathbf{e} = (\\mathbf{} - \\mathbf{H})\\mathbf{Y}\\), \n\\(\\hat{\\mathbf{Y}}^T\\mathbf{e} = \\mathbf{Y}^T\\mathbf{H}(\\mathbf{} - \\mathbf{H})\\mathbf{Y} = 0\\), since \\(\\mathbf{H}^2=\\mathbf{H}\\).Therefore,\\[\\mathbf{Y}^T\\mathbf{Y}  = \\hat{\\mathbf{Y}}^T\\hat{\\mathbf{Y}} + \\mathbf{e}^T\\mathbf{e}\\]\\[\\sum_{=1}^n y_i^2 =\\sum_{=1}^n \\hat{y}_i^2 + \\sum_{=1}^n e_i^2\\]substracting \\(n\\bar{y}^2\\) sides completes proof.ANOVA table:\\[\\begin{align*}\n\\mbox{SSR} & = \\sum_{=1}^n(\\hat{y}_i - \\bar{y})^2, \\;\\;\\;\\; df =p-1 \\\\\n\\mbox{SSE} & = \\sum_{=1}^n(y_i - \\hat{y}_i)^2, \\;\\;\\;\\; df = n-p \\\\\n\\mbox{SST} & = \\sum_{=1}^n(y_i - \\bar{y})^2, \\;\\;\\;\\; df =n-1.\n\\end{align*}\\]\\(\\beta_1 = \\beta_2 = ... = \\beta_k = 0\\) \\(\\hat{\\beta}_j \\approx 0\\) \\(j = 1,...,k\\) \\(\\hat{y}_i \\approx \\bar{y}\\)., \\(\\mbox{SSE} \\approx \\mbox{SST}\\) \\(\\mbox{SSR} \\approx 0\\). Small values \\(\\mbox{SSR}\\) relative \\(\\mbox{SSE}\\) provide indication \\(\\beta_1 = \\beta_2 = ... = \\beta_k = 0\\).\\[\\begin{align*}\n&H_0: \\beta_1 = \\beta_2 = ... = \\beta_k = 0\\\\\n&H_A: \\mbox{ }\\beta_j = 0 \\mbox{ }j = 1,...,k\n\\end{align*}\\]\\(H_0\\):\\[F= \\frac{\\mbox{SSR}/(p-1)}{\\mbox{SSE}/(n-p)} \\sim F_{(p-1, n-p)}\\]P-value \\(P( F_{(p-1, n-p)} \\geq F_{obs})\\), \\(F_{obs}\\) observed \\(F\\)-value.Coefficient determination \\(R^2 = \\frac{\\mbox{SSR}}{\\mbox{SST}}\\), \\(0 \\leq R^2 \\leq 1\\).\\(R^2\\) proportion variability \\(Y\\) explained regression \\(X_1,...,X_k\\).Adjusted \\(R^2\\) modified version \\(R^2\\) adjusted number predictors model. R uses:\\[R^2_{Adj} = 1-(1- R^2)\\frac{n-1}{n-p-1}.\\]\n\n\n\n\n\n\n","code":""},{"path":"multiple-regression.html","id":"way-anova-model","chapter":"4 Multiple regression","heading":"4.8 1-way ANOVA model","text":"","code":""},{"path":"multiple-regression.html","id":"example","chapter":"4 Multiple regression","heading":"4.8.1 Example:","text":"study carried examine effects caffeine. Thirty students randomly assigned one :control, caffeinelow dose caffeinelow dose caffeine plus sugarThe response \\(y\\) index measuring unrest 2 hrs later.(Example Draper Smith (1966).)Let \\(y_{ij}\\) response \\(j^{th}\\) person \\(^{th}\\) group, \\(j=1,...,10\\), \\(=1,2,3\\).Let \\(n_i\\) number assigned group \\(\\).Model:\\(y_{ij} = \\mu_{} + \\epsilon_{ij}\\), \\(\\quad\\) \\(\\epsilon_{ij}\\) iid \\(N(0, \\sigma^2)\\),\n\\(\\mu_i\\) population mean dose \\(\\).equivalently:\\(y_{ij} = \\mu + \\alpha_{} + \\epsilon_{ij}\\), \\(\\quad\\) \\(\\epsilon_{ij}\\) iid \\(N(0, \\sigma^2)\\), \\(\\mu\\) overall population mean \\(\\alpha_i\\) effect receiving treatment \\(\\).O.L.S estimates Model 1 :\\[\\begin{align*}\nS(\\mu_1, ..., \\mu_g)& =\\sum_{=1}^g\\sum_{j=1}^{n_i}\\epsilon_{ij}^2 = \\sum_{=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\mu_i)^2,\\\\\n\\frac{\\delta S(\\mu_1, ..., \\mu_g)}{\\delta \\mu_i}& = -2\\sum_{j=1}^{n_i}(y_{ij}-\\mu_i), \\quad  \\forall = 1,...,g \\\\\n\\end{align*}\\]Setting equal 0 evaluating \\(\\hat{\\mu}_i\\) gives:\\[\\begin{align*}\n\\sum_{j=1}^{n_i}(y_{ij}-\\hat{\\mu}_i) & =0.\\\\\n\\sum_{j=1}^{n_i}y_{ij}-n_i\\hat{\\mu}_i & =0.\\\\\n\\hat{\\mu}_i =\\sum_{j=1}^{n_i}y_{ij}/n_i & =\\bar{y}_{.}\\\\\n\\end{align*}\\]NOTE: \\(\\bar{y}_{.}\\) average responses level \\(\\) \\(X\\).Model 1 \\(g=3\\) parameters model 2 4 parameters -parameterised (\\(\\mu_i = \\mu + \\alpha_{}\\)).Usually constraint \\(\\sum \\alpha_i = 0\\) \\(\\alpha_3 = 0\\) imposed.hypothesis interest model :\\[\\begin{align*}\n& H_0: \\mu_1=\\mu_2 = ...= \\mu_g\\\\\n& H_A: \\mbox{} \\mu_i \\mbox{ .}\\\\\n\\end{align*}\\]Equivalently:\\[\\begin{align*}\n& H_0: \\alpha_i=0, \\hspace{1cm}\\forall = 1,...,g\\\\\n& H_A: \\mbox{} \\alpha_i = 0.\\\\\n\\end{align*}\\]Calculations can summarised ANOVA table::\\(\\mbox{SSG} = \\sum_{= 1}^gn_{}(\\bar{y}_{.} - \\bar{y}_{..})^{2}\\)\\(\\mbox{SSG} = \\sum_{= 1}^gn_{}(\\bar{y}_{.} - \\bar{y}_{..})^{2}\\)\\(\\mbox{SSE} = \\sum_{=1}^g\\sum_{j = 1}^{n_i}(y_{ij} - \\bar{y}_{.})^{2}\\)\\(\\mbox{SSE} = \\sum_{=1}^g\\sum_{j = 1}^{n_i}(y_{ij} - \\bar{y}_{.})^{2}\\)\\(\\mbox{SST} = \\sum_{=1}^g\\sum_{j = 1}^{n_i}(y_{ij} - \\bar{y}_{..})^{2}\\)\\(\\mbox{SST} = \\sum_{=1}^g\\sum_{j = 1}^{n_i}(y_{ij} - \\bar{y}_{..})^{2}\\)\\(H_{0}\\): \\[F_{obs} = \\frac{\\mbox{MSG}}{\\mbox{MSE}} \\sim F_{g-1, n-g}.\\]reject \\(H_{0}\\) large values \\(F_{obs}\\).","code":""},{"path":"multiple-regression.html","id":"one-way-anova-in-regression-notation","chapter":"4 Multiple regression","heading":"4.9 One way ANOVA in regression notation","text":"First set dummy variables:\\[X_i=  \\{\n\\begin{array}{ll}\n1 & \\quad\\mbox{observation gp }\\\\\n0 & \\quad\\mbox{ow}\\end{array}\\]Model: (effects model \\(y_{ij} = \\mu + \\alpha_{} + \\epsilon_{ij}\\))\\[Y = \\mu + \\alpha_1X_1 + \\alpha_2X_2 + \\alpha_3X_3 + \\epsilon\\]\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\]\\(\\mathbf{Y}\\) \\(30 \\times 1\\) vector responses \\[\\boldsymbol{\\beta} =\n\\begin{bmatrix}\n\\mu \\\\\n\\alpha_{1}   \\\\\n\\alpha_{2}  \\\\\n\\alpha_{3}\n\\end{bmatrix} \\quad \\quad \\quad \\mathbf{X} =\n\\begin{bmatrix}\n1 & 1 & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 1 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & 0 & 0 & 1\n\\end{bmatrix}\\]Note \\(\\mathbf{X}\\) matrix first column equals sum second, third fourth columns, therefore full rank \\(\\mathbf{X}^T\\mathbf{X}\\) inverse unique \\(\\hat{\\boldsymbol{\\beta}}\\).require \\(\\sum\\alpha_i = 0\\) solution unique. , require \\(\\alpha_3 = 0\\) drop last column \\(\\mathbf{X}\\).also derive solution first column \\(\\mathbf{X}\\) omitted. model becomes:\\[Y = \\mu_1X_1 + \\mu_2X_2 + \\mu_3X_3 + \\epsilon\\]\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\]\\[\\boldsymbol{\\beta} =\\begin{bmatrix}\n\\mu_{1} \\\\\n\\mu_{2}\\\\\n\\mu_{3}\n\\end{bmatrix}\\quad \\quad \\quad\n\\mathbf{X} = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n0 & 0 & 1\n\\end{bmatrix}\\]means model \\(y_{ij} = \\mu_i + \\epsilon_{ij}\\).\\[\\begin{align*}\n\\hat{\\boldsymbol{\\beta}} & = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\\\\n& = \\begin{bmatrix}\n10 & 0 & 0 \\\\\n0 & 10 & 0 \\\\\n0 & 0 & 10\n\\end{bmatrix}^{-1}\\begin{bmatrix}\n\\sum_{j=1}^{10} y_{1j}   \\\\\n\\sum_{j=1}^{10} y_{2j}\\\\\n\\sum_{j=1}^{10} y_{3j}\n\\end{bmatrix}\\\\\n& = \\frac{1}{10}\\mathbf{}_3\\begin{bmatrix}\ny_{1.}   \\\\\ny_{2.}\\\\\ny_{3.}\n\\end{bmatrix}\\\\\n& = \\begin{bmatrix}\n\\bar{y}_{1.}   \\\\\n\\bar{y}_{2.}\\\\\n\\bar{y}_{3.}\n\\end{bmatrix}\\\\\n& = \\begin{bmatrix}\n\\hat{\\mu}_{1}   \\\\\n\\hat{\\mu}_{2}\\\\\n\\hat{\\mu}_{3}\n\\end{bmatrix}\n\\end{align*}\\]fitted values :\\[\\hat{Y} = \\hat{\\mu}_1X_1 + \\hat{\\mu}_2X_2 + \\hat{\\mu}_3X_3\\]\\[\\hat{Y} = \\hat{\\mu}_i = \\bar{y}_{.}\\]\\(Y\\) comes group \\(\\).","code":""},{"path":"multiple-regression.html","id":"fitting-the-model-in-r","chapter":"4 Multiple regression","heading":"4.9.1 Fitting the model in R","text":"","code":""},{"path":"multiple-regression.html","id":"example-caffeine-in-r","chapter":"4 Multiple regression","heading":"4.9.1.1 Example: Caffeine in R","text":"Using aov, \\(Y\\) response, \\(X\\) group.Using lm\\(X_1\\) dropped. Compare ANOVA table 1-way ANOVA results.Using lm constrain sum effects equals 0.Using lm, without intercept (means model):1), 2) 3) give ANOVA table. 4), model include intercept, ANOVA table shows uncorrected SS, .e. \\(\\bar{y}\\) substracted:\\(SSR = \\sum \\hat{y}^2\\), \\(SSE = \\sum(y - \\hat{y})^{2}\\) \\(SST = \\sum y^{2}\\).also possible fit model explicitly specifying dummy variables.","code":"\ncoffee.data <- read.csv(\"data/coffee.csv\")\ncoffee.data |> glimpse()\n#Tell R this is a categorical variable\ncoffee.data$x <- as.factor(coffee.data$x)\ncoffee.data |> ggplot(aes(x = x, y = y)) + geom_boxplot()\nfit.oneway.anova <- aov(y~x, data = coffee.data) \nsummary(fit.oneway.anova)##             Df Sum Sq Mean Sq F value  Pr(>F)   \n## x            2   61.4  30.700   6.181 0.00616 **\n## Residuals   27  134.1   4.967                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(fit.oneway.anova)## Analysis of Variance Table\n## \n## Response: y\n##           Df Sum Sq Mean Sq F value   Pr(>F)   \n## x          2   61.4 30.7000  6.1812 0.006163 **\n## Residuals 27  134.1  4.9667                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nmodel.tables(fit.oneway.anova, \"means\")## Tables of means\n## Grand mean\n##       \n## 246.5 \n## \n##  x \n## x\n##     1     2     3 \n## 244.8 246.4 248.3\n# plot(fit.oneway.anova) #diagnostic plots\ncoefficients(fit.oneway.anova)## (Intercept)          x2          x3 \n##       244.8         1.6         3.5\nTukeyHSD(fit.oneway.anova, \"x\", ordered = TRUE)##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n##     factor levels have been ordered\n## \n## Fit: aov(formula = y ~ x, data = coffee.data)\n## \n## $x\n##     diff        lwr      upr     p adj\n## 2-1  1.6 -0.8711391 4.071139 0.2606999\n## 3-1  3.5  1.0288609 5.971139 0.0043753\n## 3-2  1.9 -0.5711391 4.371139 0.1562593\nplot(TukeyHSD(fit.oneway.anova, \"x\"))\nfit.coffee <- lm(y~x, data = coffee.data)\nsummary(fit.coffee)## \n## Call:\n## lm(formula = y ~ x, data = coffee.data)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -3.400 -2.075 -0.300  1.675  3.700 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 244.8000     0.7047 347.359  < 2e-16 ***\n## x2            1.6000     0.9967   1.605  0.12005    \n## x3            3.5000     0.9967   3.512  0.00158 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.229 on 27 degrees of freedom\n## Multiple R-squared:  0.3141, Adjusted R-squared:  0.2633 \n## F-statistic: 6.181 on 2 and 27 DF,  p-value: 0.006163\nanova(fit.coffee)## Analysis of Variance Table\n## \n## Response: y\n##           Df Sum Sq Mean Sq F value   Pr(>F)   \n## x          2   61.4 30.7000  6.1812 0.006163 **\n## Residuals 27  134.1  4.9667                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nmodel.matrix(fit.coffee)##    (Intercept) x2 x3\n## 1            1  0  0\n## 2            1  0  0\n## 3            1  0  0\n## 4            1  0  0\n## 5            1  0  0\n## 6            1  0  0\n## 7            1  0  0\n## 8            1  0  0\n## 9            1  0  0\n## 10           1  0  0\n## 11           1  1  0\n## 12           1  1  0\n## 13           1  1  0\n## 14           1  1  0\n## 15           1  1  0\n## 16           1  1  0\n## 17           1  1  0\n## 18           1  1  0\n## 19           1  1  0\n## 20           1  1  0\n## 21           1  0  1\n## 22           1  0  1\n## 23           1  0  1\n## 24           1  0  1\n## 25           1  0  1\n## 26           1  0  1\n## 27           1  0  1\n## 28           1  0  1\n## 29           1  0  1\n## 30           1  0  1\n## attr(,\"assign\")\n## [1] 0 1 1\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$x\n## [1] \"contr.treatment\"\noptions(contrasts=c('contr.sum', 'contr.sum'))\nfit.coffee <- lm(y~x, data = coffee.data)\nsummary(fit.coffee)## \n## Call:\n## lm(formula = y ~ x, data = coffee.data)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -3.400 -2.075 -0.300  1.675  3.700 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 246.5000     0.4069 605.822  < 2e-16 ***\n## x1           -1.7000     0.5754  -2.954  0.00642 ** \n## x2           -0.1000     0.5754  -0.174  0.86333    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.229 on 27 degrees of freedom\n## Multiple R-squared:  0.3141, Adjusted R-squared:  0.2633 \n## F-statistic: 6.181 on 2 and 27 DF,  p-value: 0.006163\nanova(fit.coffee)## Analysis of Variance Table\n## \n## Response: y\n##           Df Sum Sq Mean Sq F value   Pr(>F)   \n## x          2   61.4 30.7000  6.1812 0.006163 **\n## Residuals 27  134.1  4.9667                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nmodel.matrix(fit.coffee)##    (Intercept) x1 x2\n## 1            1  1  0\n## 2            1  1  0\n## 3            1  1  0\n## 4            1  1  0\n## 5            1  1  0\n## 6            1  1  0\n## 7            1  1  0\n## 8            1  1  0\n## 9            1  1  0\n## 10           1  1  0\n## 11           1  0  1\n## 12           1  0  1\n## 13           1  0  1\n## 14           1  0  1\n## 15           1  0  1\n## 16           1  0  1\n## 17           1  0  1\n## 18           1  0  1\n## 19           1  0  1\n## 20           1  0  1\n## 21           1 -1 -1\n## 22           1 -1 -1\n## 23           1 -1 -1\n## 24           1 -1 -1\n## 25           1 -1 -1\n## 26           1 -1 -1\n## 27           1 -1 -1\n## 28           1 -1 -1\n## 29           1 -1 -1\n## 30           1 -1 -1\n## attr(,\"assign\")\n## [1] 0 1 1\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$x\n## [1] \"contr.sum\"\noptions(contrasts=c('contr.treatment', 'contr.treatment')) # to reset\nfit.coffee <- lm(y ~ x - 1, data = coffee.data)\nsummary(fit.coffee)## \n## Call:\n## lm(formula = y ~ x - 1, data = coffee.data)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -3.400 -2.075 -0.300  1.675  3.700 \n## \n## Coefficients:\n##    Estimate Std. Error t value Pr(>|t|)    \n## x1 244.8000     0.7047   347.4   <2e-16 ***\n## x2 246.4000     0.7047   349.6   <2e-16 ***\n## x3 248.3000     0.7047   352.3   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.229 on 27 degrees of freedom\n## Multiple R-squared:  0.9999, Adjusted R-squared:  0.9999 \n## F-statistic: 1.223e+05 on 3 and 27 DF,  p-value: < 2.2e-16\nanova(fit.coffee)## Analysis of Variance Table\n## \n## Response: y\n##           Df  Sum Sq Mean Sq F value    Pr(>F)    \n## x          3 1822929  607643  122344 < 2.2e-16 ***\n## Residuals 27     134       5                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nmodel.matrix(fit.coffee)##    x1 x2 x3\n## 1   1  0  0\n## 2   1  0  0\n## 3   1  0  0\n## 4   1  0  0\n## 5   1  0  0\n## 6   1  0  0\n## 7   1  0  0\n## 8   1  0  0\n## 9   1  0  0\n## 10  1  0  0\n## 11  0  1  0\n## 12  0  1  0\n## 13  0  1  0\n## 14  0  1  0\n## 15  0  1  0\n## 16  0  1  0\n## 17  0  1  0\n## 18  0  1  0\n## 19  0  1  0\n## 20  0  1  0\n## 21  0  0  1\n## 22  0  0  1\n## 23  0  0  1\n## 24  0  0  1\n## 25  0  0  1\n## 26  0  0  1\n## 27  0  0  1\n## 28  0  0  1\n## 29  0  0  1\n## 30  0  0  1\n## attr(,\"assign\")\n## [1] 1 1 1\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$x\n## [1] \"contr.treatment\"\n# or using dummy variables\n\nd1 <- as.numeric(coffee.data$x == 1)\nd2 <- as.numeric(coffee.data$x == 2)\nd3 <- as.numeric(coffee.data$x == 3)\n\nfit.coffee.dummy <- lm(coffee.data$y ~ d1 +d2)\nsummary(fit.coffee.dummy)## \n## Call:\n## lm(formula = coffee.data$y ~ d1 + d2)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -3.400 -2.075 -0.300  1.675  3.700 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 248.3000     0.7047 352.326  < 2e-16 ***\n## d1           -3.5000     0.9967  -3.512  0.00158 ** \n## d2           -1.9000     0.9967  -1.906  0.06730 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.229 on 27 degrees of freedom\n## Multiple R-squared:  0.3141, Adjusted R-squared:  0.2633 \n## F-statistic: 6.181 on 2 and 27 DF,  p-value: 0.006163\n#no intercept\nfit.coffee.dummy2 <- lm(coffee.data$y ~ d1 +d2 +d3 - 1)\nsummary(fit.coffee.dummy2)## \n## Call:\n## lm(formula = coffee.data$y ~ d1 + d2 + d3 - 1)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -3.400 -2.075 -0.300  1.675  3.700 \n## \n## Coefficients:\n##    Estimate Std. Error t value Pr(>|t|)    \n## d1 244.8000     0.7047   347.4   <2e-16 ***\n## d2 246.4000     0.7047   349.6   <2e-16 ***\n## d3 248.3000     0.7047   352.3   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.229 on 27 degrees of freedom\n## Multiple R-squared:  0.9999, Adjusted R-squared:  0.9999 \n## F-statistic: 1.223e+05 on 3 and 27 DF,  p-value: < 2.2e-16"},{"path":"multiple-regression.html","id":"confidence-intervals-and-hypothesis-tests-for-linear-combinations-of-boldsymbolbeta","chapter":"4 Multiple regression","heading":"4.10 Confidence intervals and hypothesis tests for linear combinations of \\(\\boldsymbol{\\beta}\\)","text":"theory OLS:\\[\\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta},(\\mathbf{X}^T\\mathbf{X})^{-1}\\sigma^2)\\]\\[\\mathbf{c}^T\\hat{\\boldsymbol{\\beta}} \\sim N(\\mathbf{c}^T\\boldsymbol{\\beta},\\mathbf{c}^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{c}\\sigma^2)\\]caffeine example (1 way ANOVA model): suppose want compare treatment means control mean, , want CI :\\[\\frac{\\mu_2+\\mu_3}{2}-\\mu_1\\]Let \\(\\mathbf{c}^T= (-1, 1/2, 1/2)\\), \\(\\boldsymbol{\\beta}^T = (\\mu_1, \\mu_2, \\mu_3)\\).\\[\\mathbf{c}^T\\boldsymbol{\\beta} = -\\mu_1+\\mu_2/2+ \\mu_3/2\\]estimated :\\[\\mathbf{c}^T\\hat{\\boldsymbol{\\beta}} = -\\bar{y}_{1.}+\\bar{y}_{2.}/2+ \\bar{y}_{3.}/2\\]variance :\\[\\begin{bmatrix}\n-1 & 1/2 & 1/2\n\\end{bmatrix} \\frac{1}{10}\\mathbf{}_3 \\begin{bmatrix}\n-1 \\\\\n1/2 \\\\\n1/2\\end{bmatrix}\\sigma^2 = \\frac{3}{20}\\sigma^2\\], \\(100 \\times (1- \\alpha) \\%\\)CI :\\[\\mathbf{c}^T\\hat{\\boldsymbol{\\beta}} \\pm t_{27}(\\alpha/2) \\sqrt{\\frac{3}{20}\\hat{\\sigma}^2}\\]df : \\(n-g = 30-3 = 27\\).also test hypotheses e.g. \\(H_o: \\mathbf{c}^T\\boldsymbol{\\beta} = 0\\). test statistic:\\[\\frac{\\mathbf{c}^T\\hat{\\boldsymbol{\\beta}}}{\\sqrt{\\frac{3}{20}\\hat{\\sigma}^2}} \\sim t_{27}.\\]","code":""},{"path":"model-comparisons-and-testing-for-lack-of-fit.html","id":"model-comparisons-and-testing-for-lack-of-fit","chapter":"5 Model comparisons and testing for lack of fit","heading":"5 Model comparisons and testing for lack of fit","text":"","code":""},{"path":"model-comparisons-and-testing-for-lack-of-fit.html","id":"f-tests-for-comparing-two-models","chapter":"5 Model comparisons and testing for lack of fit","heading":"5.1 F-tests for comparing two models","text":"","code":""},{"path":"model-comparisons-and-testing-for-lack-of-fit.html","id":"example-1","chapter":"5 Model comparisons and testing for lack of fit","heading":"5.1.1 Example:","text":"Model : \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1\\)Model B: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\).e. Model , \\(\\beta_2=\\beta_3=0\\).Model reduced simpler model model B full model.\\(\\mbox{SSE}\\) Model B smaller \\(\\mbox{SSE}\\) Model reduction enough justify two extra parameters?:Model :\\[\\mbox{SST} = \\mbox{SSR}() + \\mbox{SSE}()\\]Model B:\\[\\mbox{SST} = \\mbox{SSR}(B) + \\mbox{SSE}(B)\\]Note:\\[\\mbox{SSE}()-\\mbox{SSE}(B)=\\mbox{SSR}(B)-\\mbox{SSR}()\\]","code":""},{"path":"model-comparisons-and-testing-for-lack-of-fit.html","id":"f-test-to-compare-models","chapter":"5 Model comparisons and testing for lack of fit","heading":"5.1.2 F-test to compare models:","text":"Model : \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + ... + \\beta_q x_q\\)Model B: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + ... + \\beta_k x_k\\)\\(q<k\\) Model nested within Model B.\\(H_0\\): \\(\\beta_{q+1} = \\beta_{q+2} = ... = \\beta_k = 0\\)\\(H_A\\): least one \\(\\beta_{q+1}, ... , \\beta_k \\neq 0.\\)\\[F =\\frac{(\\mbox{SSE}()-\\mbox{SSE}(B))/(k-q)}{\\mbox{SSE}(B)/(n-p)}.\\]\\(H_0\\), \\[F \\sim F_{(k-q),(n-p)},\\] \\(p = (k+1).\\)Note: Equivalently, F-test can written :\\[F =\\frac{(\\mbox{SSR}(B)-\\mbox{SSR}())/(k-q)}{\\mbox{SSE}(B)/(n-p)}.\\]Note: Models B must hierarchical F-test valid.","code":""},{"path":"model-comparisons-and-testing-for-lack-of-fit.html","id":"example-steam-data","chapter":"5 Model comparisons and testing for lack of fit","heading":"5.1.3 Example: Steam data","text":"data study undertaken understand factors caused energy consumption detergent manufacturing 25 month period. Example Draper Smith (1966).data variables :y = STEAM Pounds steam used monthly.x1 = TEMP Average atmospheric temperature (\\(^o\\)F).x2 = INV Inventory: pounds real fatty acid storage per month.x3 = PROD Pounds crude glycerin made.x4 = WIND Average wind velocity (mph).x5 = CDAY Calendar days per month.x6 = OPDAY Operating days per month.x7 = FDAY Days \\(32^o\\)F.x8 = WIND2 Average wind velocity squared.x9 = STARTS Number production start-ups month.Model : \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1\\)Model B: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\)\\(x_1\\) = TEMP, \\(x_2\\) = INV, \\(x_3\\) = PROD.\\(H_0\\): \\(\\beta_2 = \\beta_3 = 0\\)\\(H_A\\): least one \\(\\beta_2, \\beta_3 \\neq 0\\)\\[\\begin{align*}\nF_{obs} & =  \\frac{(\\mbox{SSE}()-\\mbox{SSE}(B))/(k-q)}{\\mbox{SSE}(B)/(n-p)}\\\\\n& = \\frac{(18.223-8.927)/(3-1)}{8.927/(25-4)}=10.93.\\\\\n\\end{align*}\\]\\(F_{(0.05,2,21)} = 3.467\\), \\(F_{(0.01,2,21)} = 5.780\\)P-value \\(<0.01\\), reject \\(H_0\\) conclude least one \\(\\beta_2\\), \\(\\beta_3\\) differ 0.","code":"\nlibrary(tidyverse)\nsteamdata <- read.csv(\"data/Steam.csv\")\nsteamdata |> glimpse()## Rows: 25\n## Columns: 10\n## $ STEAM  <dbl> 10.98, 11.13, 12.51, 8.40, 9.27, 8.73, 6.36, 8.50, 7.82, 9.14, …\n## $ TEMP   <dbl> 35.3, 29.7, 30.8, 58.8, 61.4, 71.3, 74.4, 76.7, 70.7, 57.5, 46.…\n## $ INV    <dbl> 5.20, 5.12, 6.19, 3.89, 6.28, 5.76, 3.45, 6.57, 5.69, 6.14, 4.8…\n## $ PROD   <dbl> 0.61, 0.64, 0.78, 0.49, 0.84, 0.74, 0.42, 0.87, 0.75, 0.76, 0.6…\n## $ WIND   <dbl> 7.4, 8.0, 7.4, 7.5, 5.5, 8.9, 4.1, 4.1, 4.1, 4.5, 10.3, 6.9, 6.…\n## $ CDAY   <int> 31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31, 31, 28, 31, 30,…\n## $ OPDAY  <int> 20, 20, 23, 20, 21, 22, 11, 23, 21, 20, 20, 21, 21, 19, 23, 20,…\n## $ FDAY   <int> 22, 25, 17, 22, 0, 0, 0, 0, 0, 0, 11, 12, 25, 18, 5, 7, 0, 0, 0…\n## $ WIND2  <dbl> 54.8, 64.0, 54.8, 56.3, 30.3, 79.2, 16.8, 16.8, 16.8, 20.3, 106…\n## $ STARTS <int> 4, 5, 4, 4, 5, 4, 2, 5, 4, 5, 4, 4, 5, 5, 4, 4, 6, 4, 3, 4, 4, …\nsteamdata |>\n  dplyr::select(STEAM, TEMP, INV, PROD) |>\n  pairs()\nmodelA <- lm(STEAM ~ TEMP, steamdata)\nmodelB <- lm(STEAM ~ TEMP + INV + PROD, steamdata)\nsummary(modelA)## \n## Call:\n## lm(formula = STEAM ~ TEMP, data = steamdata)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.6789 -0.5291 -0.1221  0.7988  1.3457 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 13.62299    0.58146  23.429  < 2e-16 ***\n## TEMP        -0.07983    0.01052  -7.586 1.05e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.8901 on 23 degrees of freedom\n## Multiple R-squared:  0.7144, Adjusted R-squared:  0.702 \n## F-statistic: 57.54 on 1 and 23 DF,  p-value: 1.055e-07\nsummary(modelB)## \n## Call:\n## lm(formula = STEAM ~ TEMP + INV + PROD, data = steamdata)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.2348 -0.4116  0.1240  0.3744  1.2979 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  9.514814   1.062969   8.951 1.30e-08 ***\n## TEMP        -0.079928   0.007884 -10.138 1.52e-09 ***\n## INV          0.713592   0.502297   1.421     0.17    \n## PROD         0.330497   3.267694   0.101     0.92    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.652 on 21 degrees of freedom\n## Multiple R-squared:  0.8601, Adjusted R-squared:  0.8401 \n## F-statistic: 43.04 on 3 and 21 DF,  p-value: 3.794e-09\nanova(modelA)## Analysis of Variance Table\n## \n## Response: STEAM\n##           Df Sum Sq Mean Sq F value    Pr(>F)    \n## TEMP       1 45.592  45.592  57.543 1.055e-07 ***\n## Residuals 23 18.223   0.792                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(modelB)## Analysis of Variance Table\n## \n## Response: STEAM\n##           Df Sum Sq Mean Sq  F value    Pr(>F)    \n## TEMP       1 45.592  45.592 107.2523 1.046e-09 ***\n## INV        1  9.292   9.292  21.8588 0.0001294 ***\n## PROD       1  0.004   0.004   0.0102 0.9203982    \n## Residuals 21  8.927   0.425                       \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(modelA, modelB)## Analysis of Variance Table\n## \n## Model 1: STEAM ~ TEMP\n## Model 2: STEAM ~ TEMP + INV + PROD\n##   Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n## 1     23 18.223                                  \n## 2     21  8.927  2    9.2964 10.934 0.0005569 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"model-comparisons-and-testing-for-lack-of-fit.html","id":"sequential-sums-of-squares","chapter":"5 Model comparisons and testing for lack of fit","heading":"5.2 Sequential sums of squares","text":"","code":""},{"path":"model-comparisons-and-testing-for-lack-of-fit.html","id":"example-3","chapter":"5 Model comparisons and testing for lack of fit","heading":"5.2.1 Example:","text":"Model : \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1\\)Model B: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2+ \\beta_3 x_3\\)noted earlier, reduction \\(\\mbox{SSE}\\) going Model B, equivalent increase \\(\\mbox{SSR}\\), .e. \\[\\mbox{SSE}()-\\mbox{SSE}(B)=\\mbox{SSR}(B)-\\mbox{SSR}().\\]can denote: \\[\\mbox{SSR}(B|)=\\mbox{SSR}(B)-\\mbox{SSR}().\\]sequential sums squares.\ncan write:\n\\[\\begin{align*}\n\\mbox{SST} & = \\mbox{SSR}(B) + \\mbox{SSE}(B)\\\\\n& = \\mbox{SSR}() +\\mbox{SSR}(B) - \\mbox{SSR}() + \\mbox{SSE}(B)\\\\\n& = \\mbox{SSR}() + \\mbox{SSR}(B|) + \\mbox{SSE}(B).\\\\\n\\mbox{SST} -  \\mbox{SSE}(B) &= \\mbox{SSR}() + \\mbox{SSR}(B|)\\\\\n\\mbox{SSR}(B) &= \\mbox{SSR}() + \\mbox{SSR}(B|).\\\\\n\\end{align*}\\]model appropriate, \\(\\mbox{SSR}(B|)\\) small.","code":""},{"path":"model-comparisons-and-testing-for-lack-of-fit.html","id":"example-steam-data-1","chapter":"5 Model comparisons and testing for lack of fit","heading":"5.2.2 Example: Steam data","text":"Model : \\(\\mathbb{E}[y] = \\beta_0\\)Model B: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1\\)Model C: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\)Model D: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\)ANOVA table,\\[\\begin{align*}\n\\mbox{SSR}(D)& =54.889\\\\\n& = \\mbox{SSR}(B|) + \\mbox{SSR}(C|B) + \\mbox{SSR}(D|C)\\\\\n\\end{align*}\\]can use F-test comparing two models test Seq SS.1):Model : \\(\\mathbb{E}[y] = \\beta_0\\)Model D: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\)\\(H_0\\): \\(\\beta_1 = \\beta_2 = \\beta_3 = 0\\)\\(H_a\\): \\(\\beta_i\\) 0\\(\\mbox{SSR}() = 0\\)\\(\\mbox{SSR}(D|) = \\mbox{SSR}(D) = 54.889.\\)\\[F_{obs} = \\frac{\\mbox{SSR}(D|)/(k-q)}{\\mbox{SSE}(D)/(n-p)} = \\frac{54.889/(3-0)}{8.927/(25-4)}=43.04 \\]P-value \\(< 0.001\\), reject \\(H_0\\) conclude \\(\\beta_i\\) 0.2):Model C: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\)Model D: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\)\\(H_0\\): \\(\\beta_3 = 0\\)\\(H_a\\): \\(\\beta_3 \\neq 0\\)\\(\\mbox{SSR}(D|C) = 0.004\\)\\[F_{obs} = \\frac{\\mbox{SSR}(D|C)/(k-q)}{\\mbox{SSE}(D)/(n-p)} = \\frac{0.004/1}{8.927/21} = 0.01\\]\\(F_{(0.1,1,21)} = 2.96096\\), P-value \\(>0.05\\).fail reject \\(H_0\\) conclude evidence \\(\\beta_3 \\neq 0\\), .e. \\(x_3\\) needed model.F-test equivalent t-test \\(\\beta_3\\):\n\\[T = 0.1\\]\n\\[F = (0.1)(0.1) = 0.01\\]p-value tests \\(= 0.92\\).Note: Seq SS values depend order variables added model (unless variables uncorrelated).anova aov functions R implement sequential sum squares (type ).\nFunction Anova(, type= 2) library(car) gives adjusted SS (type II)","code":"\nmodelB <- lm(STEAM ~ TEMP + INV + PROD, steamdata)\n\nanova(modelB)## Analysis of Variance Table\n## \n## Response: STEAM\n##           Df Sum Sq Mean Sq  F value    Pr(>F)    \n## TEMP       1 45.592  45.592 107.2523 1.046e-09 ***\n## INV        1  9.292   9.292  21.8588 0.0001294 ***\n## PROD       1  0.004   0.004   0.0102 0.9203982    \n## Residuals 21  8.927   0.425                       \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(lm(STEAM ~ PROD  + INV + TEMP, steamdata))## Analysis of Variance Table\n## \n## Response: STEAM\n##           Df Sum Sq Mean Sq F value    Pr(>F)    \n## PROD       1  5.958   5.958  14.015  0.001197 ** \n## INV        1  5.242   5.242  12.330  0.002076 ** \n## TEMP       1 43.690  43.690 102.776 1.524e-09 ***\n## Residuals 21  8.927   0.425                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nlibrary(car)\nAnova(modelB, type= 2)## Anova Table (Type II tests)\n## \n## Response: STEAM\n##           Sum Sq Df  F value    Pr(>F)    \n## TEMP      43.690  1 102.7760 1.524e-09 ***\n## INV        0.858  1   2.0183    0.1701    \n## PROD       0.004  1   0.0102    0.9204    \n## Residuals  8.927 21                       \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"model-comparisons-and-testing-for-lack-of-fit.html","id":"testing-for-lack-of-fit","chapter":"5 Model comparisons and testing for lack of fit","heading":"5.3 Testing for lack of fit","text":"replicate values response available \\(X\\) values, formal test model adequacy available.test based comparing fitted value average response level \\(X\\).NOTATION: Suppose \\(g\\) different values \\(X\\) \\(^{th}\\) , \\(n_i\\) observations \\(Y\\).Let \\(\\bar{y}_{.}=\\frac{1}{n_i}\\sum_{j=1}^{n_i} y_{ij}\\), \\(\\quad =1, ..., g.\\)Note: estimate group means 1-way ANOVA model (means model): \\(y_{ij} = \\mu_{} + \\epsilon_{ij}\\), \\(\\epsilon_{ij}\\) iid \\(N(0, \\sigma^2)\\).pure error sums squares,\\[\\begin{align*}\n\\mbox{SS}_{\\mbox{PE}}& =\\sum_{=1}^g \\sum_{j=1}^{n_i} (y_{ij}- \\bar{y}_{.})^2\\\\\ndf_{PE} & = \\sum_{=1}^g (n_i-1)=n-g, \\hspace{1cm} \\mbox{} n=n_1+...+n_g.\\\\\n\\end{align*}\\]Therefore\\[\\frac{\\sum_{=1}^g \\sum_{j=1}^{n_i} (y_{ij}- \\bar{y}_{.})^2}{n-g}\\]estimator \\(\\sigma^2\\).NOTE:use replicates obtain estimate \\(\\sigma^2\\) independent fitted model (SLR).*estimator \\(\\sigma^2\\) corresponds \\(\\mbox{MSE}\\) ANOVA table 1-way ANOVA model.1-way ANOVA model \\(g\\) parameters. SLR model \\(2\\) parameters. latter restrictive requires linearity.1-way ANOVA model \\(g\\) parameters. SLR model \\(2\\) parameters. latter restrictive requires linearity.\\(df_{PE} = n-g\\),\\(df_{PE} = n-g\\),\\(df_{SLR} = n-2\\).\\(df_{SLR} = n-2\\).SLR model residual SS \\(\\geq\\) residual SS means model, .e. \\(\\mbox{SSE} \\geq \\mbox{SS}_{\\mbox{PE}}\\).large difference \\(\\mbox{SSE} - \\mbox{SS}_{\\mbox{PE}}\\) indicates lack fit regression line.\\(\\mbox{SS}(\\mbox{lack fit})= \\mbox{SSE} - \\mbox{SS}_{\\mbox{PE}} = \\sum_{,j} (\\hat{y}_{,j} - \\bar{y}_i)^2\\), sum squared distances SLR estimate means model estimate \\(\\mathbb{E}(Y_{,j})\\).Lack fit tested statistic:\\[F_{obs}=\\frac{\\left ( \\mbox{SSE}-\\mbox{SS}_{\\mbox{PE}} \\right )/(g-2)}{\\mbox{SS}_{\\mbox{PE}}/(n-g)}.\\]\\(H_0\\): Regression model fits well\\(H_A\\): Regression model displays lack fitUnder \\(H_0\\), \\(F_{obs} \\sim F_{g-2,n-g}\\).Note: generalises multiple predictors - pure error estimate \\(\\sigma^2\\) based SS \\(y_i\\) cases values predictors. \\(df_{SLR} = p\\) instead 2.Reject large values \\(F_{obs}\\).","code":""},{"path":"model-comparisons-and-testing-for-lack-of-fit.html","id":"example-voltage","chapter":"5 Model comparisons and testing for lack of fit","heading":"5.3.1 Example: Voltage","text":"Example Ramsey Schafer (2002) (case0802 library(Sleuth3)).Batches electrical fluids subjected constant voltages insulating properties fluid broke .\\(Y\\): time breakdown\\(X\\): VoltageThe scatterplot \\(Y\\) vs. \\(X\\) shows evidence non-linearity non-constant variance. response log transformed resolve .\\(H_0: \\beta_1=0\\)\\(H_A: \\beta_1 \\neq 0\\)\\(F = 78.4\\), \\(p<0.001\\). reject \\(H_0\\) conclude \\(\\beta_1 \\neq 0\\).\\(H_0:\\) S.L.R model appropriate/correct model\\(H_A:\\) S.L.R model lack fit.\\[F=\\frac{(180.07-173.75)/(7-2)}{173.75/(76-7)}=0.5\\]\\(F=0.50, p=0.773\\). conclude evidence lack fit.","code":"\nanova(lm(log(TIME)~VOLTAGE, data.voltage))## Analysis of Variance Table\n## \n## Response: log(TIME)\n##           Df Sum Sq Mean Sq F value   Pr(>F)    \n## VOLTAGE    1 190.15 190.151  78.141 3.34e-13 ***\n## Residuals 74 180.07   2.433                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(lm(log(TIME)~as.factor(VOLTAGE) ,data.voltage))## Analysis of Variance Table\n## \n## Response: log(TIME)\n##                    Df Sum Sq Mean Sq F value    Pr(>F)    \n## as.factor(VOLTAGE)  6 196.48  32.746  13.004 8.871e-10 ***\n## Residuals          69 173.75   2.518                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"model-comparisons-and-testing-for-lack-of-fit.html","id":"added-variable-plots","chapter":"5 Model comparisons and testing for lack of fit","heading":"5.4 Added variable plots","text":"simple linear regression can assess importance predictor :t-statistic\\(\\mbox{SSR}\\)\\(R^2\\)\\(Y\\)-\\(X\\) plot.analogues multiple regression assessing importance predictor presence predictors :t-statisticSeq/Extra SSpartial \\(R^2\\)added variable plot.","code":""},{"path":"model-comparisons-and-testing-for-lack-of-fit.html","id":"example-steam-vs.-temp-inv-prod","chapter":"5 Model comparisons and testing for lack of fit","heading":"5.4.1 Example: STEAM vs. TEMP, INV, PROD","text":"Model : \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1+ \\beta_2 x_2\\)Model B: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\)\\(x_1\\) = TEMP, \\(x_2\\) = INV, \\(x_3\\) = PROD.t-statistic PROD small: \\(T=0.10, p=0.920\\)t-statistic PROD small: \\(T=0.10, p=0.920\\)\\(\\mbox{SSR}(B|) = 0.004\\) also small.\\(\\mbox{SSR}(B|) = 0.004\\) also small.partial \\(R^2\\) PROD proportion variability response unexplained TEMP INV explained PRODThe partial \\(R^2\\) PROD proportion variability response unexplained TEMP INV explained PROD\\[\\begin{align*}\nR^2(\\mbox{PROD|TEMP, INV})& =\\frac{\\mbox{SSR}(B|)}{\\mbox{SSE}()}\n& = \\frac{0.004}{8.931} = 0.00045=0.045\\%\\\\\n\\end{align*}\\]added variable plot shows relationship response predictor, adjusting predictors model.‘Adjusting’ \\(Y\\) predictors \\(X_1,...,X_k\\) achieved computing residuals regression \\(Y\\) \\(X_1,...,X_k\\). resulting residuals can thought \\(Y\\) effect \\(X_1,...,X_k\\) removed.","code":""},{"path":"model-comparisons-and-testing-for-lack-of-fit.html","id":"example-added-variable-plot-for-prod.","chapter":"5 Model comparisons and testing for lack of fit","heading":"5.4.2 Example: Added variable plot for PROD.","text":".e. add PROD model containing predictors TEMP INV? (Response STEAM).Compute \\(e\\)(STEAM\\(|\\) TEMP, INV), .e. residuals regression STEAM TEMP INV.Compute \\(e\\)(PROD\\(|\\) TEMP, INV), .e. residuals regression PROD TEMP INV.AVP PROD: Plot \\(e\\)(STEAM\\(|\\) TEMP, INV) vs. \\(e\\)(PROD\\(|\\) TEMP, INV).can also :AVP INV: Plot \\(e\\)(STEAM\\(|\\) TEMP, PROD) vs. \\(e\\)(INV\\(|\\) TEMP, PROD)\nAVP TEMP: Plot \\(e\\)(STEAM\\(|\\) INV, PROD) vs. \\(e\\)(TEMP\\(|\\) INV, PROD)","code":""},{"path":"model-comparisons-and-testing-for-lack-of-fit.html","id":"example-steam-data-contd","chapter":"5 Model comparisons and testing for lack of fit","heading":"5.4.3 Example: Steam data cont’d","text":"Alternatively can use avPlots function library(car).","code":"\nfit1 <- lm(STEAM ~ TEMP + INV, steamdata)\nfit2 <- lm(PROD ~ TEMP + INV, steamdata)\nsummary(lm(resid(fit1)~ resid(fit2)))## \n## Call:\n## lm(formula = resid(fit1) ~ resid(fit2))\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.2348 -0.4116  0.1240  0.3744  1.2979 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)\n## (Intercept) 4.716e-18  1.246e-01   0.000    1.000\n## resid(fit2) 3.305e-01  3.122e+00   0.106    0.917\n## \n## Residual standard error: 0.623 on 23 degrees of freedom\n## Multiple R-squared:  0.0004869,  Adjusted R-squared:  -0.04297 \n## F-statistic: 0.0112 on 1 and 23 DF,  p-value: 0.9166\nlibrary(car)\navPlots(modelB) # in library(car)"},{"path":"model-comparisons-and-testing-for-lack-of-fit.html","id":"properties-of-avps","chapter":"5 Model comparisons and testing for lack of fit","heading":"5.4.4 Properties of AVPs:","text":"Estimated intercept 0.Estimated intercept 0.Slope line AVP PROD equals \\(\\hat{\\beta}\\) (coefficient PROD model TEMP, INV PROD predictors.Slope line AVP PROD equals \\(\\hat{\\beta}\\) (coefficient PROD model TEMP, INV PROD predictors.Residuals AVP equal residuals regression STEAM TEMP, INV PROD.Residuals AVP equal residuals regression STEAM TEMP, INV PROD.\\(R^2\\) AVP PROD partial \\(R^2\\) PROD, .e. \\(R^2\\)(PROD\\(|\\)TEMP,INV).\\(R^2\\) AVP PROD partial \\(R^2\\) PROD, .e. \\(R^2\\)(PROD\\(|\\)TEMP,INV).\\(\\hat{\\sigma}^2\\) AVP PROD \\(\\approx \\hat{\\sigma}^2\\) full model.\\(\\hat{\\sigma}^2\\) AVP PROD \\(\\approx \\hat{\\sigma}^2\\) full model.\\[\\hat{\\sigma}^2_{AVP}(n-2) = \\hat{\\sigma}^2_{full}(n-p)\\]points AVP clustered tightly around line variable important.AV plots may also show outliers, apparent adjusted association \\(Y\\) \\(X_j\\) due influence point.","code":""},{"path":"diagnostic-methods-in-more-details.html","id":"diagnostic-methods-in-more-details","chapter":"6 Diagnostic methods (in more details)","heading":"6 Diagnostic methods (in more details)","text":"","code":""},{"path":"diagnostic-methods-in-more-details.html","id":"model-assumptions","chapter":"6 Diagnostic methods (in more details)","heading":"6.1 Model assumptions","text":"assumptions can stated terms error vector:\\(\\mathbb{E}(\\boldsymbol{\\epsilon}) = \\mathbf{0}\\)\\(\\mbox{Var}(\\boldsymbol{\\epsilon}) = \\sigma^2\\mathbf{}_n\\)\\(\\boldsymbol{\\epsilon} \\sim N(\\mathbf{0}, \\sigma^2\\mathbf{}_n)\\)Since observe \\(\\boldsymbol{\\epsilon}\\) check assumptions directly. Instead observe residuals \\(\\mathbf{e}\\).","code":""},{"path":"diagnostic-methods-in-more-details.html","id":"residuals-1","chapter":"6 Diagnostic methods (in more details)","heading":"6.2 Residuals","text":"Residuals key assessing model problems.\\[\\mathbf{e} = \\mathbf{Y} - \\mathbf{\\hat{Y}} = \\mathbf{Y} - \\mathbf{H}\\mathbf{Y} = (\\mathbf{} - \\mathbf{H})\\mathbf{Y} \\]\\(\\boldsymbol{\\epsilon}\\) \\(\\mathbf{e}\\) differ?model correct,\\(\\mathbb{E}(\\mathbf{e}) = \\mathbb{E}(\\mathbf{Y}) - \\mathbb{E}(\\mathbf{\\hat{Y}})= \\mathbf{0}\\) ()\\(\\mbox{Var}(\\mathbf{e})  = (\\mathbf{} - \\mathbf{H})\\sigma^2\\) (different).Like errors, residuals mean 0, Var(\\(e_i) = (1-h_{ii})\\sigma^2\\) variance quite constant (variance smaller \\(h_{ii}\\) close 1).Note: results shows residuals may different variances even \\(y_i\\)s variance (\\(\\sigma^2\\)) precision fitted values depends pattern \\(X_i\\)s.Cov(\\(e_i, e_j) = -h_{ii}\\sigma^2\\), \\(\\neq j\\). residuals correlated, practice correlation generally important visible residual plots.plot:\\(e_i\\) vs \\(\\hat{y}_i\\) (residual vs fit),\\(e_i\\) vs \\(X_{ij}\\) (residual vs predictor \\(j\\)).SLR conclusions clear cut. multiple regression deviations ideal pattern indicate model problems precise diagnosis difficult.Figures :ideal pattern: random scatter points around 0 lineideal pattern: random scatter points around 0 linenon-constant variance, variability residuals changingnon-constant variance, variability residuals changingcurvature suggests : \\(\\mathbb{E}(e_i) \\neq 0\\), thus, \\(\\mathbb{E}(\\epsilon_i)\\neq 0\\)curvature suggests : \\(\\mathbb{E}(e_i) \\neq 0\\), thus, \\(\\mathbb{E}(\\epsilon_i)\\neq 0\\)curvature non-constant variancecurvature non-constant variance","code":""},{"path":"diagnostic-methods-in-more-details.html","id":"leverage-values","chapter":"6 Diagnostic methods (in more details)","heading":"6.3 Leverage values","text":"\\[\\begin{align*}\n\\mathbf{\\hat{Y}}  & = \\mathbf{H}\\mathbf{Y}\\\\\n\\mathbf{H}  & = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\\\\nh_{ii}  & = \\mathbf{X}_i^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}_i\\\\\n\\end{align*}\\]\\(h_{ii}\\) known leverage case \\(\\).\\(\\mathbf{X}_i\\) row \\(\\) \\(\\mathbf{X}\\) matrix column vector.SLR,\\[\\begin{align*}\nh_{ii}  & = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{S_{xx}}.\n\\end{align*}\\]similar formula can derived multiple regression:\\[h_{ii}   = \\frac{1}{n} + (\\mathbf{X}_i^* -\\mathbf{M})(\\mathbf{\\tilde{X}}^T\\mathbf{\\tilde{X}})^{-1}(\\mathbf{X}_i^* - \\mathbf{M}),\\]\\(\\mathbf{X}_i^*\\) row \\(\\) \\(\\mathbf{X}\\) matrix without column \\(\\mathbf{1}\\)s..e.\\(\\mathbf{X}_i =\\begin{bmatrix} 1 & \\mathbf{X}_i^* \\end{bmatrix}\\)\\(\\mathbf{M}\\) mean vector\\[\\mathbf{M} =\\begin{bmatrix} \\bar{X}_{.1}\\\\ \\vdots  \\\\ \\bar{X}_{.k} \\end{bmatrix}\\]\\(\\tilde{\\mathbf{X}}\\) matrix centered \\(x\\) - data values, .e.\\[\\tilde{\\mathbf{X}}  =\\begin{bmatrix} (X_{11} - \\bar{X}_{.1}) & \\dots & (X_{1k} - \\bar{X}_{.k}) \\\\ (X_{21} - \\bar{X}_{.1})  & \\dots & (X_{2k} - \\bar{X}_{.k})  \\\\ \\vdots & \\vdots &\\vdots \\\\ (X_{n1} - \\bar{X}_{.1})  & \\dots & (X_{nk} - \\bar{X}_{.k})  \\end{bmatrix} \\]\\(\\bar{X}_{.1}, \\dots, \\bar{X}_{.k}\\) means \\(k\\) predictors.Therefore \\(h_{ii}\\) measures `distance’ case \\(\\) average case \\(\\mathbf{M}\\).Example \\(k=2\\): contours constant \\(h_{ii}\\)points ellipse \\(h_{ii}\\) value.","code":""},{"path":"diagnostic-methods-in-more-details.html","id":"properties-of-h_ii","chapter":"6 Diagnostic methods (in more details)","heading":"6.3.1 Properties of \\(h_{ii}\\):","text":"\\(0 \\leq h_{ii} \\leq 1\\) model intercept \\(\\frac{1}{n} \\leq h_{ii} \\leq 1\\)\\(0 \\leq h_{ii} \\leq 1\\) model intercept \\(\\frac{1}{n} \\leq h_{ii} \\leq 1\\)\\(\\sum_{=1}^n h_{ii}  =p\\) \\(p\\) number parameters\\(\\sum_{=1}^n h_{ii}  =p\\) \\(p\\) number parameters\\(\\sum_{=1}^n h_{ij}  =\\sum_{j=1}^n h_{ij}  = 1\\), .e. row sum = col sum = 1.\\(\\sum_{=1}^n h_{ij}  =\\sum_{j=1}^n h_{ij}  = 1\\), .e. row sum = col sum = 1.\\(h_{ii}\\) large (rule thumb: large \\(\\geq 2 \\times \\mbox{average } h_{ii} = 2p/n\\)), case may :mistakea high influence case (.e. big impact results).\\(h_{ii}\\) extremely large (.e. close 1):\\(\\hat{y}_i \\approx y_i\\) \\(e_i \\approx 0\\) andVar(\\(\\hat{y}_i)= h_{ii}\\sigma^2 \\approx  \\sigma^2 \\approx\\) Var(\\(y_i\\)).case \\(\\mathbb{E}(y_i)\\) imprecisely estimated. practice, rarely happens.","code":""},{"path":"diagnostic-methods-in-more-details.html","id":"standardised-residuals","chapter":"6 Diagnostic methods (in more details)","heading":"6.4 Standardised residuals","text":"compare residuals (\\(e_i\\)) different observations take account variances may differ:\\[\\mbox{Var}(e_i) = (1-h_{ii})\\sigma^2.\\]can standardise dividing \\(\\sqrt{(1-h_{ii})}\\hat{\\sigma}\\), \\(\\hat{\\sigma}\\) estimate based SSE. get standardised residuals:\\[r_i=\\frac{e_i}{\\sqrt{(1-h_{ii})}\\hat{\\sigma}}.\\]: \\(\\mathbb{E}(r_i) = 0\\) \\(\\mbox{Var}(r_i)\\approx 1\\) (constant).Since \\(r_i\\), unlike \\(e_i\\), common scale easier/fairer compare .Standardised residuals useful detecting anomalous observations outliers.Note:Cases \\(|r_i| \\geq 2\\) well fit.\\(r_i\\) often used place \\(e_i\\) residual plots normal probability plots.\\(r_i\\) (\\(e_i\\)) independent.","code":""},{"path":"diagnostic-methods-in-more-details.html","id":"leave-one-out-methods","chapter":"6 Diagnostic methods (in more details)","heading":"6.5 Leave-one-out methods","text":"Remember \\(h_{ii}\\) approaches 1, variance residual approaches 0, indicating fitted value \\(\\hat{y}_i\\) pulled close observed value \\(y_i\\).leverage \\(h_{ii}\\) potential influence \\(^{th}\\) observation.Observations high leverage need inspected carefully might large influence fit.Note potential influence necessarily thing actual influence, since might case observation line rest data, fitting model without observation give prediction close observed \\(y_i\\) anyhow.One way examine actual influence case \\(\\) compare regression results case \\(\\) without case \\(\\).Denote fitted values cases included \\(\\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_n\\) usual.Denote fitted values case \\(\\) removed \\(\\hat{y}_{1()}, \\hat{y}_{2()}, ..., \\hat{y}_{n()}\\), \\(= 1,...,n\\).Cook’s distance measures influence \\(^{th}\\) case :\\[D_i = \\sum_{j=1}^n \\frac{(\\hat{y}_{j}-\\hat{y}_{j()})^2}{p \\hat{\\sigma}^2}.\\]Note proportional Euclidean distance (SS) fitted values obtained omitting \\(^{th}\\) observation \\(\\hat{y}_{j()}\\) fitted values based data \\(\\hat{y}_{j}\\).Typically examine case largest \\(D_i\\) , case large datasets, cases largest \\(D_i\\) values, largest influence.computation Cook’s distance \\(D_i\\) observations require refitting model \\(n\\) times? computationally expensive!turns can rewrite formula \\(D_i\\) function standardised residual \\(r_i\\) leverage \\(h_{ii}\\). end need fit model can compute \\(D_i\\) complete data regression results.Shortcut formula:\\[\\hat{y}_{j()}=\\hat{y}_{j}- \\frac{h_{ij}}{1-h_{ii}}e_i.\\]Thus:\\[\\hat{y}_{j}-\\hat{y}_{j()}= \\frac{h_{ij}}{1-h_{ii}}e_i\\]\\[\\begin{align*}\n\\sum_j(\\hat{y}_{j}-\\hat{y}_{j()})^2 & = \\frac{e_i^2}{(1-h_{ii})^2}\\sum_j h_{ij}^2\\\\\n& = \\frac{e_i^2 h_{ii}}{(1-h_{ii})^2},\\\\\n\\end{align*}\\]properties H matrix (symmetric idempotent).Hence:\\[\\begin{align*}\nD_i& = \\frac{e_i^2 h_{ii}}{p \\hat{\\sigma}^2(1-h_{ii})^2}\\\\\nD_i& =  \\frac{r_i^2h_{ii}}{p(1-h_{ii})}.\\\\\n\\end{align*}\\]general, high influence outlier cases either:Big \\(|r_i|\\) big \\(h_{ii}\\).Big \\(|r_i|\\) big \\(h_{ii}\\).Big \\(|r_i|\\) moderate \\(h_{ii}\\).Big \\(|r_i|\\) moderate \\(h_{ii}\\).Moderate \\(|r_i|\\) big \\(h_{ii}\\).Moderate \\(|r_i|\\) big \\(h_{ii}\\).","code":""},{"path":"diagnostic-methods-in-more-details.html","id":"other-influence-measures","chapter":"6 Diagnostic methods (in more details)","heading":"6.6 Other influence measures","text":"\\(\\hat{\\beta_j}-\\hat{\\beta}_{j()}\\) (effect leaving case \\(\\) )\\(\\hat{\\sigma}_{()}\\) (estimate \\(\\sigma\\) case \\(\\) omitted). Used : “studentised residual” = \\(\\frac{\\hat{\\epsilon_i}}{\\sqrt{1-h_{ii}}\\hat{\\sigma}_{()}}\\)","code":""},{"path":"diagnostic-methods-in-more-details.html","id":"testing-outliers","chapter":"6 Diagnostic methods (in more details)","heading":"6.7 Testing outliers","text":"individual case suspected outlier formal hypothesis test can performed verify.\nDetails test Chapter 9 Weisberg (2005).leverages influence see Chapter 2 Rodrı́guez (2007). detailed exposition linear model diagnostics see Chapter 11 12 Fox (2016).","code":""},{"path":"diagnostic-methods-in-more-details.html","id":"diagnostics-examples-two-case-studies","chapter":"6 Diagnostic methods (in more details)","heading":"6.8 Diagnostics examples (two case studies)","text":"","code":""},{"path":"diagnostic-methods-in-more-details.html","id":"example-1-brain-size-versus-body-gestation-period-and-litter","chapter":"6 Diagnostic methods (in more details)","heading":"6.8.1 Example 1: Brain size versus body gestation period and litter","text":"Example Ramsey Schafer (2002) (case0902 library(Sleuth3)).known body size mammals good predictor brain size interest know gestation period litter size also good predictors. data contains average values brain weight, body weight, gestation length litter size 96 species mammals.model fit well? Outliers, nonlinearity? NOTE: MTB can use brushing (set species ID variables) link graphs explore data.Clearly model defective.normal probability plot shows residuals come heavy tailed distribution.normal probability plot shows residuals come heavy tailed distribution.residual vs fit plot shows linear pattern majority data outliers. pattern hidden data clumped together. also observe unequal variance.residual vs fit plot shows linear pattern majority data outliers. pattern hidden data clumped together. also observe unequal variance.African elephant, Hippopotamus, Dolphin, Human large standardised residuals.African elephant, Hippopotamus Dolphin, large influence.\n\n\n\n\n\n\n\n\n\n\n\n\nAdded-variable plots can used detecting influential data.example, AVP body:Hippo African elephant large body given variables (litter gestation).Hippo African elephant large body given variables (litter gestation).Humans, dolphins African elephant large brain litter gestation.Humans, dolphins African elephant large brain litter gestation.Together Humans, dolphins hippos reduce body slope, African elephant, high-leverage point, line rest data.Together Humans, dolphins hippos reduce body slope, African elephant, high-leverage point, line rest data.\n\n\n\n\n\n\nraw data lot skeweness. naturally generates lot outliers obscures patterns data.Transform data taking logs improve model fit.skeweness gone. plot log(brain) vs log(body) shows strong linear pattern - apparent untransformed variables. Therefore log transformation two variables seems appropriate. variables also less skewed .biggest standardised residual now case 24 (Human) larger brain predicted model.Highest leverage case 1 (quokka), doesn’t high influence (also goes cases \\(h_{ii} > 0.083 = 2p/n\\), average \\(h_{ii} = p/n\\), except case 73 (hyrax) longer gestation).residual distribution shows skeweness, closer normal .residual vs fit plot shows evidence non-constant variance. Perhaps transformation better?answer original question, clearly gestation time litter size important predictors brain size presence body size.","code":"\nbrain.data <- read.csv(\"data/Species_brain.csv\")\n#library(car)\n#library(MASS)\nfit1 <- lm(BRAIN ~ BODY + GESTATION + LITTER, brain.data)\nsummary(fit1)## \n## Call:\n## lm(formula = BRAIN ~ BODY + GESTATION + LITTER, data = brain.data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1026.68   -62.08    17.29    51.73   988.76 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -225.29213   83.05875  -2.712  0.00797 ** \n## BODY           0.98588    0.09428  10.457  < 2e-16 ***\n## GESTATION      1.80874    0.35445   5.103 1.79e-06 ***\n## LITTER        27.64864   17.41429   1.588  0.11579    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 224.6 on 92 degrees of freedom\n## Multiple R-squared:   0.81,  Adjusted R-squared:  0.8038 \n## F-statistic: 130.7 on 3 and 92 DF,  p-value: < 2.2e-16\n# anova(fit1)\nfit1 |> augment(brain.data) |> arrange(desc(.cooksd)) |> head()## # A tibble: 6 × 11\n##   SPECIES      BRAIN  BODY GESTATION LITTER .fitted .resid   .hat .sigma .cooksd\n##   <chr>        <dbl> <dbl>     <int>  <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n## 1 African ele…  4480  2800       655      1   3748.   732. 0.719    173. 24.3   \n## 2 Hippopotamus   590  1400       240      1   1617. -1027. 0.251    188.  2.34  \n## 3 Dolphin       1600   160       360      1    611.   989. 0.0791   198.  0.452 \n## 4 Human being   1300    65       270      1    355.   945. 0.0352   202.  0.168 \n## 5 Tapir          250   230       390      1    735.  -485. 0.0945   219.  0.134 \n## 6 Domestic pig   180   190       115      8    391.  -211. 0.170    224.  0.0548\n## # ℹ 1 more variable: .std.resid <dbl>\nfit1 |> augment(brain.data) |> arrange(desc(.hat)) |> head()## # A tibble: 6 × 11\n##   SPECIES    BRAIN   BODY GESTATION LITTER .fitted  .resid   .hat .sigma .cooksd\n##   <chr>      <dbl>  <dbl>     <int>  <dbl>   <dbl>   <dbl>  <dbl>  <dbl>   <dbl>\n## 1 African… 4480    2.8e+3       655    1    3748.    732.  0.719    173. 2.43e+1\n## 2 Hippopo…  590    1.4e+3       240    1    1617.  -1027.  0.251    188. 2.34e+0\n## 3 Domesti…  180    1.9e+2       115    8     391.   -211.  0.170    224. 5.48e-2\n## 4 Rat II      2.38 3.4e-1        21    8      34.2   -31.8 0.134    226. 8.98e-4\n## 5 Rat I       0.72 5  e-2        23    7.3    18.2   -17.5 0.102    226. 1.92e-4\n## 6 Tapir     250    2.3e+2       390    1     735.   -485.  0.0945   219. 1.34e-1\n## # ℹ 1 more variable: .std.resid <dbl>\nfit1 |> augment(brain.data) |> arrange(desc(abs(.std.resid))) |> head(5)## # A tibble: 5 × 11\n##   SPECIES      BRAIN  BODY GESTATION LITTER .fitted .resid   .hat .sigma .cooksd\n##   <chr>        <dbl> <dbl>     <int>  <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n## 1 African ele…  4480  2800       655      1   3748.   732. 0.719    173.  24.3  \n## 2 Hippopotamus   590  1400       240      1   1617. -1027. 0.251    188.   2.34 \n## 3 Dolphin       1600   160       360      1    611.   989. 0.0791   198.   0.452\n## 4 Human being   1300    65       270      1    355.   945. 0.0352   202.   0.168\n## 5 Tapir          250   230       390      1    735.  -485. 0.0945   219.   0.134\n## # ℹ 1 more variable: .std.resid <dbl>\nfit2 <- lm(log(BRAIN) ~ log(BODY) + log(GESTATION) + log(LITTER), brain.data)\nsummary(fit2)## \n## Call:\n## lm(formula = log(BRAIN) ~ log(BODY) + log(GESTATION) + log(LITTER), \n##     data = brain.data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.95415 -0.29639 -0.03105  0.28111  1.57491 \n## \n## Coefficients:\n##                Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)     0.85482    0.66167   1.292  0.19962    \n## log(BODY)       0.57507    0.03259  17.647  < 2e-16 ***\n## log(GESTATION)  0.41794    0.14078   2.969  0.00381 ** \n## log(LITTER)    -0.31007    0.11593  -2.675  0.00885 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.4748 on 92 degrees of freedom\n## Multiple R-squared:  0.9537, Adjusted R-squared:  0.9522 \n## F-statistic: 631.6 on 3 and 92 DF,  p-value: < 2.2e-16\n#anova(fit2)\nfit2 |> augment(brain.data) |> arrange(desc(abs(.std.resid))) |> head(3)## # A tibble: 3 × 11\n##   SPECIES     BRAIN  BODY GESTATION LITTER .fitted .resid   .hat .sigma .cooksd\n##   <chr>       <dbl> <dbl>     <int>  <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n## 1 Human being  1300    65       270      1    5.60  1.57  0.0225  0.447  0.0647\n## 2 Dolphin      1600   160       360      1    6.23  1.14  0.0312  0.462  0.0483\n## 3 Tapir         250   230       390      1    6.48 -0.954 0.0342  0.466  0.0370\n## # ℹ 1 more variable: .std.resid <dbl>\nfit2 |> augment(brain.data) |> arrange(desc(.hat)) |> head(6)## # A tibble: 6 × 11\n##   SPECIES    BRAIN   BODY GESTATION LITTER .fitted  .resid   .hat .sigma .cooksd\n##   <chr>      <dbl>  <dbl>     <int>  <dbl>   <dbl>   <dbl>  <dbl>  <dbl>   <dbl>\n## 1 Quokka     17.5    3.5         26    1     2.94  -0.0747 0.259   0.477 2.93e-3\n## 2 Domestic… 180    190          115    8     5.21  -0.0176 0.146   0.477 6.89e-5\n## 3 Gentle l…   7.8    0.22       145    2     1.85   0.205  0.116   0.477 6.93e-3\n## 4 Nutria     23      5          132    5.5   3.29  -0.157  0.114   0.477 3.95e-3\n## 5 Hyrax      20.5    3.8        225    2.4   3.61  -0.594  0.100   0.473 4.85e-2\n## 6 Hampster…   1.12   0.13        16    6.3   0.270 -0.156  0.0711  0.477 2.23e-3\n## # ℹ 1 more variable: .std.resid <dbl>\nfit2 |> augment(brain.data) |> arrange(desc(.cooksd)) |> head(3)## # A tibble: 3 × 11\n##   SPECIES      BRAIN  BODY GESTATION LITTER .fitted .resid   .hat .sigma .cooksd\n##   <chr>        <dbl> <dbl>     <int>  <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n## 1 Hippopotam…  590   1.4e3       240    1      7.31 -0.931 0.0645  0.467  0.0709\n## 2 Human being 1300   6.5e1       270    1      5.60  1.57  0.0225  0.447  0.0647\n## 3 Hyrax         20.5 3.8e0       225    2.4    3.61 -0.594 0.100   0.473  0.0485\n## # ℹ 1 more variable: .std.resid <dbl>"},{"path":"diagnostic-methods-in-more-details.html","id":"example-2-rat-data","chapter":"6 Diagnostic methods (in more details)","heading":"6.8.2 Example 2: Rat data","text":"Example Weisberg (2005).Experiment investigate amount drug present liver rat.BodyWt = body weight ratBodyWt = body weight ratLiverWt = measured sacrificeLiverWt = measured sacrificeDose = dose given, proportional body weightDose = dose given, proportional body weightEndDose = dose drug recovered sacrifice animalEndDose = dose drug recovered sacrifice animalExperimantal hypothesis: relationship EndDose 3 predictors.appears combination bodyweight dose relevant EndDose. However, individually related EndDose even though \\(X_1 \\propto X_3\\)Case 3 large influence (see Cook’s distance plot).Case 3 unusual combination dose bodyweight.rat given dose high bodyweight.Redo analysis case 3 removed:Experimental hypothesis validated coefficients large P-values.","code":"\nrats.data <- read.table(\"data/rat.txt\", header = TRUE)\nrats.data |>\n  dplyr::select(EndDose, BodyWt, LiverWt, Dose)|>\n  pairs()\n#library(car)\n#library(MASS)\nfit1 <- lm(EndDose ~ BodyWt +  LiverWt + Dose, data = rats.data)\nsummary(fit1)## \n## Call:\n## lm(formula = EndDose ~ BodyWt + LiverWt + Dose, data = rats.data)\n## \n## Residuals:\n##       Min        1Q    Median        3Q       Max \n## -0.100557 -0.063233  0.007131  0.045971  0.134691 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)  \n## (Intercept)  0.265922   0.194585   1.367   0.1919  \n## BodyWt      -0.021246   0.007974  -2.664   0.0177 *\n## LiverWt      0.014298   0.017217   0.830   0.4193  \n## Dose         4.178111   1.522625   2.744   0.0151 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.07729 on 15 degrees of freedom\n## Multiple R-squared:  0.3639, Adjusted R-squared:  0.2367 \n## F-statistic:  2.86 on 3 and 15 DF,  p-value: 0.07197\n#anova(fit1)\nfit1 |>  augment(rats.data) |> arrange(desc(.hat)) |> head()## # A tibble: 6 × 10\n##   BodyWt LiverWt  Dose EndDose .fitted  .resid  .hat .sigma .cooksd .std.resid\n##    <int>   <dbl> <dbl>   <dbl>   <dbl>   <dbl> <dbl>  <dbl>   <dbl>      <dbl>\n## 1    190     9    1       0.56   0.536  0.0241 0.851 0.0782 0.930        0.807\n## 2    200     7.2  1       0.23   0.298 -0.0677 0.392 0.0766 0.203       -1.12 \n## 3    149     5.2  0.75    0.21   0.308 -0.0982 0.316 0.0734 0.273       -1.54 \n## 4    195    10    0.98    0.41   0.360  0.0496 0.254 0.0785 0.0469       0.743\n## 5    186     6.8  0.94    0.28   0.339 -0.0588 0.217 0.0780 0.0510      -0.859\n## 6    146     7.3  0.73    0.3    0.318 -0.0184 0.195 0.0798 0.00425     -0.265\navPlots(fit1)\nfit1 <- lm(EndDose ~ BodyWt +  LiverWt + Dose, data = rats.data[-3,])\nsummary(fit1)## \n## Call:\n## lm(formula = EndDose ~ BodyWt + LiverWt + Dose, data = rats.data[-3, \n##     ])\n## \n## Residuals:\n##       Min        1Q    Median        3Q       Max \n## -0.102154 -0.056486  0.002838  0.046519  0.137059 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)\n## (Intercept)  0.311427   0.205094   1.518    0.151\n## BodyWt      -0.007783   0.018717  -0.416    0.684\n## LiverWt      0.008989   0.018659   0.482    0.637\n## Dose         1.484877   3.713064   0.400    0.695\n## \n## Residual standard error: 0.07825 on 14 degrees of freedom\n## Multiple R-squared:  0.02106,    Adjusted R-squared:  -0.1887 \n## F-statistic: 0.1004 on 3 and 14 DF,  p-value: 0.9585\n#anova(fit1)"},{"path":"diagnostic-methods-in-more-details.html","id":"should-unusual-data-be-discarded","chapter":"6 Diagnostic methods (in more details)","heading":"6.8.2.1 Should Unusual Data Be Discarded?","text":"Although problematic data ignored, also deleted automatically. important investigate observation unusual.Truly bad data (e.g.rats) can corrected thrown away. discrepant data-point correct, may able understand observation unusual.Species Brain data, makes sense humans enjoy brain size accounted variables. case like , may choose deal separately outlying observation.Outliers influential data may motivate model respecification e.g. introduction additional explanatory variables.However, must careful avoid overfitting data .e. permitting small portion data determine form model.extensive discussion can found Fox (2016) Chapter 11.7, pg 288-289.","code":""},{"path":"special-cases-of-multiple-regression.html","id":"special-cases-of-multiple-regression","chapter":"7 Special cases of multiple regression","heading":"7 Special cases of multiple regression","text":"","code":""},{"path":"special-cases-of-multiple-regression.html","id":"categorical-and-continuous-predictors-binary-categories","chapter":"7 Special cases of multiple regression","heading":"7.1 Categorical and continuous predictors (binary categories)","text":"Example (Ramsey Schafer (2002) pg 236, 245):\\(Y\\): average number flowers per plant (meadowfoam).\\(Y\\): average number flowers per plant (meadowfoam).Light intensity: 150, 300, 450, 600, 750, 900 (\\(\\mu\\) mol/\\(m^2\\)/sec)Light intensity: 150, 300, 450, 600, 750, 900 (\\(\\mu\\) mol/\\(m^2\\)/sec)Timing: Timing onset light treatment Early/Late. Coded 0/1.Timing: Timing onset light treatment Early/Late. Coded 0/1.Suppose data table (every 2nd row) consider following models:Parallel lines model (model ):\\[\\mathbb{E}(y)= \\beta_0+\\beta_1(timing)+ \\beta_2 (light)\\]Separate lines model (model B):\\[\\mathbb{E}(y)= \\beta_0+\\beta_1(timing)+ \\beta_2 (light) + \\beta_3 (timing \\times light)\\]Give design matrix parameter vector modelsTest \\(H_0: \\beta_3 = 0\\).Parallel lines model:\\[\\mathbb{E}(y)= \\beta_0+\\beta_1(timing)+ \\beta_2 (light)\\]\\[\\mathbf{X} = \\begin{bmatrix}\n1 & 0 & 150 \\\\\n1 & 0 & 300\\\\\n1 & 0 & 450 \\\\\n1 & 0 & 600\\\\\n1 & 0 & 750\\\\\n1 & 0 & 900\\\\\n1 & 1 & 150 \\\\\n1 & 1 & 300\\\\\n1 & 1 & 450 \\\\\n1 & 1 & 600 \\\\\n1 & 1 & 750\\\\\n1 & 1 & 900 \\\\\n\\end{bmatrix}\\]\\(\\boldsymbol{\\beta} = \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2\\\\\n\\end{bmatrix}\\)Separate lines model:\\[\\mathbb{E}(y)= \\beta_0+\\beta_1(timing)+ \\beta_2 (light) + \\beta_3 (timing \\times light)\\]\\[\\mathbf{X} = \\begin{bmatrix}\n1 & 0 & 150 &0 \\\\\n1 & 0 & 300 &0 \\\\\n1 & 0 & 450 &0 \\\\\n1 & 0 & 600 &0 \\\\\n1 & 0 & 750 &0 \\\\\n1 & 0 & 900 &0 \\\\\n1 & 1 & 150 &150 \\\\\n1 & 1 & 300 &300 \\\\\n1 & 1 & 450 &450 \\\\\n1 & 1 & 600 &600 \\\\\n1 & 1 & 750 &750 \\\\\n1 & 1 & 900 &900 \\\\\n\\end{bmatrix}\\]\\[\\boldsymbol{\\beta} = \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2\\\\\n\\beta_3\\\\\n\\end{bmatrix}\\]test \\(H_0: \\beta_3 = 0\\), P-value = 0.910, reject \\(H_0\\) (See table coefficients, output ).Model :Model B:","code":"\nflowers.data <- read.csv(\"data/flowers.csv\")\nflowers.data |> \n  glimpse()## Rows: 24\n## Columns: 4\n## $ Flowers   <dbl> 62.3, 77.4, 55.3, 54.2, 49.6, 61.9, 39.4, 45.7, 31.3, 44.9, …\n## $ Timing    <chr> \"Early\", \"Early\", \"Early\", \"Early\", \"Early\", \"Early\", \"Early…\n## $ Time      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, …\n## $ Intensity <int> 150, 150, 300, 300, 450, 450, 600, 600, 750, 750, 900, 900, …\nflowers.data <- flowers.data |> \n  mutate(Timing = as.factor(Timing))\nfit1 <- lm(Flowers ~ Intensity + Timing, flowers.data)\nsummary(fit1)## \n## Call:\n## lm(formula = Flowers ~ Intensity + Timing, data = flowers.data)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -9.652 -4.139 -1.558  5.632 12.165 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 71.305833   3.273772  21.781 6.77e-16 ***\n## Intensity   -0.040471   0.005132  -7.886 1.04e-07 ***\n## TimingLate  12.158333   2.629557   4.624 0.000146 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.441 on 21 degrees of freedom\n## Multiple R-squared:  0.7992, Adjusted R-squared:   0.78 \n## F-statistic: 41.78 on 2 and 21 DF,  p-value: 4.786e-08\nfit2 <- lm(Flowers ~ Intensity * Timing, flowers.data)\nsummary(fit2)## \n## Call:\n## lm(formula = Flowers ~ Intensity * Timing, data = flowers.data)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -9.516 -4.276 -1.422  5.473 11.938 \n## \n## Coefficients:\n##                       Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)          71.623333   4.343305  16.491 4.14e-13 ***\n## Intensity            -0.041076   0.007435  -5.525 2.08e-05 ***\n## TimingLate           11.523333   6.142360   1.876   0.0753 .  \n## Intensity:TimingLate  0.001210   0.010515   0.115   0.9096    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.598 on 20 degrees of freedom\n## Multiple R-squared:  0.7993, Adjusted R-squared:  0.7692 \n## F-statistic: 26.55 on 3 and 20 DF,  p-value: 3.549e-07\nmodel.matrix(fit2)##    (Intercept) Intensity TimingLate Intensity:TimingLate\n## 1            1       150          0                    0\n## 2            1       150          0                    0\n## 3            1       300          0                    0\n## 4            1       300          0                    0\n## 5            1       450          0                    0\n## 6            1       450          0                    0\n## 7            1       600          0                    0\n## 8            1       600          0                    0\n## 9            1       750          0                    0\n## 10           1       750          0                    0\n## 11           1       900          0                    0\n## 12           1       900          0                    0\n## 13           1       150          1                  150\n## 14           1       150          1                  150\n## 15           1       300          1                  300\n## 16           1       300          1                  300\n## 17           1       450          1                  450\n## 18           1       450          1                  450\n## 19           1       600          1                  600\n## 20           1       600          1                  600\n## 21           1       750          1                  750\n## 22           1       750          1                  750\n## 23           1       900          1                  900\n## 24           1       900          1                  900\n## attr(,\"assign\")\n## [1] 0 1 2 3\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$Timing\n## [1] \"contr.treatment\""},{"path":"special-cases-of-multiple-regression.html","id":"categorical-and-continuous-predictors-more-than-two-categories","chapter":"7 Special cases of multiple regression","heading":"7.2 Categorical and continuous predictors (more than two categories)","text":"Example: (Ramsey Schafer (2002)):\\(Y\\): Measure energy\\(Y\\): Measure energy\\(X_1\\): Measure weight\\(X_1\\): Measure weightGroup: Type flyer (1,2,3). Z1, Z2, Z3 (dummy variables).Group: Type flyer (1,2,3). Z1, Z2, Z3 (dummy variables).Parallel lines model (model ):\\[\\mathbb{E}(y)= \\beta_0+\\beta_1 z_2+ \\beta_2 z_3 + \\beta_3 x_1\\]Separate lines model (model B):\n\\[\\mathbb{E}(y)= \\beta_0+\\beta_1 z_2+ \\beta_2 z_3 + \\beta_3 x_1 + \\beta_4 x_1 z_2+ \\beta_5 x_1 z_3\\]Hypothesis testing:Test \\(H_0: \\beta_4 = \\beta_5 = 0\\) comparing two models using F-test.Test \\(H_0: \\beta_4 = \\beta_5 = 0\\) comparing two models using F-test.Test \\(H_0: \\beta_1 = \\beta_2 = 0\\) comparing parallel lines model model \\(\\mathbb{E}(y)= \\beta_0+\\beta_3 x_1\\) using F-test.Test \\(H_0: \\beta_1 = \\beta_2 = 0\\) comparing parallel lines model model \\(\\mathbb{E}(y)= \\beta_0+\\beta_3 x_1\\) using F-test.Give design matrix parameter vector models.Give design matrix parameter vector models.Test \\(H_0: \\beta_4 = \\beta_5 = 0\\), .e.Test \\(H_0: \\beta_4 = \\beta_5 = 0\\), .e.\\(H_0:\\) Model correct\\(H_A:\\) Model B preferable Model \\[\\begin{align*}\nF & =\\frac{(\\mbox{SSE}()-\\mbox{SSE}(B))/(k-q)}{\\mbox{SSE}(B)/(n-p)}\\\\\n& =\\frac{(0.5533- 0.5049)/(5-3)}{0.5049/(20-6)}\\\\\n& =\\frac{0.0242}{0.0361}\\\\\n& = 0.67.\\\\\n\\end{align*}\\]\\(F_{(2,14)}(0.95) = 3.73 > 0.67\\) reject \\(H_0\\), model OK.Test \\(H_0: \\beta_1 = \\beta_2 = 0\\), .e. let model C = one group model:\\[\\mathbb{E}(y)= \\beta_0+ \\beta_3 x_1 \\]\\(H_0:\\) Model C correct\\(H_A:\\) Model preferable Model C\\[\\begin{align*}\nF & =\\frac{(\\mbox{SSE}(C)-\\mbox{SSE}())/(k-q)}{\\mbox{SSE}()/(n-p)}\\\\\n& =\\frac{\\mbox{SSR}(|C)/(3-1)}{0.5533/(20-4)}\\\\\n& =\\frac{0.0296/2}{0.0346}\\\\\n& = 0.43\\\\\n\\end{align*}\\]don’t need see fit Model C, take Seq SS.\\(F_{(2,16)}(0.95) = 3.63 > 0.43\\) reject \\(H_0\\), model C adequate.","code":"\nflying.data <- read.csv(\"data/flying.csv\")\nflying.data  |> glimpse()## Rows: 20\n## Columns: 6\n## $ Group <int> 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3\n## $ x1    <dbl> 6.658011, 6.442540, 5.552960, 5.752573, 3.190476, 3.555348, 4.28…\n## $ y     <dbl> 3.77734812, 3.54961737, 3.14845333, 3.10906094, 0.90016136, 1.36…\n## $ G1    <int> 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n## $ G2    <int> 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0\n## $ G3    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1\nflying.data <- flying.data |> \n  mutate(Group = as.factor(Group))\nflying.data  |> \n  ggplot(aes(x = x1, y = y, col = Group)) +\n  geom_point()\nfitA <- lm(y ~ x1 + Group, data = flying.data)\nsummary(fitA)## \n## Call:\n## lm(formula = y ~ x1 + Group, data = flying.data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.23224 -0.12199 -0.03637  0.12574  0.34457 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -1.57636    0.28724  -5.488 4.96e-05 ***\n## x1           0.81496    0.04454  18.297 3.76e-12 ***\n## Group2       0.10226    0.11418   0.896    0.384    \n## Group3       0.07866    0.20268   0.388    0.703    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.186 on 16 degrees of freedom\n## Multiple R-squared:  0.9815, Adjusted R-squared:  0.9781 \n## F-statistic: 283.6 on 3 and 16 DF,  p-value: 4.464e-14\nanova(fitA)## Analysis of Variance Table\n## \n## Response: y\n##           Df  Sum Sq Mean Sq  F value    Pr(>F)    \n## x1         1 29.3919 29.3919 849.9108 2.691e-15 ***\n## Group      2  0.0296  0.0148   0.4276    0.6593    \n## Residuals 16  0.5533  0.0346                       \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nfitB <- lm(y ~ x1  * Group, data = flying.data)\nsummary(fitB)## \n## Call:\n## lm(formula = y ~ x1 * Group, data = flying.data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.25152 -0.12643 -0.00954  0.08124  0.32840 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)  \n## (Intercept)  -0.2024     1.2613  -0.161   0.8748  \n## x1            0.5898     0.2061   2.861   0.0126 *\n## Group2       -1.3784     1.2952  -1.064   0.3053  \n## Group3       -1.2681     1.2854  -0.987   0.3406  \n## x1:Group2     0.2456     0.2134   1.151   0.2691  \n## x1:Group3     0.2149     0.2236   0.961   0.3529  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1899 on 14 degrees of freedom\n## Multiple R-squared:  0.9832, Adjusted R-squared:  0.9771 \n## F-statistic: 163.4 on 5 and 14 DF,  p-value: 6.696e-12\nanova(fitB)## Analysis of Variance Table\n## \n## Response: y\n##           Df  Sum Sq Mean Sq  F value    Pr(>F)    \n## x1         1 29.3919 29.3919 815.0383 8.265e-14 ***\n## Group      2  0.0296  0.0148   0.4100    0.6713    \n## x1:Group   2  0.0484  0.0242   0.6718    0.5265    \n## Residuals 14  0.5049  0.0361                       \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(fitA, fitB)## Analysis of Variance Table\n## \n## Model 1: y ~ x1 + Group\n## Model 2: y ~ x1 * Group\n##   Res.Df     RSS Df Sum of Sq      F Pr(>F)\n## 1     16 0.55332                           \n## 2     14 0.50487  2   0.04845 0.6718 0.5265\nfitC <- lm(y ~ x1, data = flying.data)\nsummary(fitC)## \n## Call:\n## lm(formula = y ~ x1, data = flying.data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.21143 -0.14422 -0.04284  0.09681  0.37695 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -1.46826    0.13716  -10.71  3.1e-09 ***\n## x1           0.80861    0.02684   30.13  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.18 on 18 degrees of freedom\n## Multiple R-squared:  0.9806, Adjusted R-squared:  0.9795 \n## F-statistic: 907.6 on 1 and 18 DF,  p-value: < 2.2e-16\nanova(fitC)## Analysis of Variance Table\n## \n## Response: y\n##           Df  Sum Sq Mean Sq F value    Pr(>F)    \n## x1         1 29.3919 29.3919  907.64 < 2.2e-16 ***\n## Residuals 18  0.5829  0.0324                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"special-cases-of-multiple-regression.html","id":"two-way-anova-two-categorical-predictors","chapter":"7 Special cases of multiple regression","heading":"7.3 Two way anova: two categorical predictors","text":"Two way analysis variance (ANOVA) without interactions regression two categorical explanatory variables.Two way analysis variance (ANOVA) without interactions regression two categorical explanatory variables.Two way analysis variance (ANOVA) interactions regression two categorical explanatory variables plus third categorical explanatory variable interaction.Two way analysis variance (ANOVA) interactions regression two categorical explanatory variables plus third categorical explanatory variable interaction.Remember explanatory variable categorical, conceptually recoded using dummy indicator variables.Remember explanatory variable categorical, conceptually recoded using dummy indicator variables.Typically:Y = response.Two predictors B categorical, \\(\\) levels \\(b\\) levels B.","code":""},{"path":"special-cases-of-multiple-regression.html","id":"example-paint-adhesion","chapter":"7 Special cases of multiple regression","heading":"7.3.1 Example: paint adhesion","text":"experiment improve paint adhesion, factors examined := application method (\\(= 2\\)).B = paint type (\\(b = 3\\)),correct find best paint separately find best application method may best overall combination.example experimental data, \\(3 \\times 2\\) experimental conditions. example 3 replicates experimental condition, .e 18 observations total. dataset summarized 3 2 table means.example experimental data, \\(3 \\times 2\\) experimental conditions. example 3 replicates experimental condition, .e 18 observations total. dataset summarized 3 2 table means.experimental data, experimenter often set data equal number observations per cell. described balanced data.experimental data, experimenter often set data equal number observations per cell. described balanced data.Observational data unlikely balanced.Observational data unlikely balanced.","code":"\npaint <- read.csv(here(\"data\", \"paint.csv\"), header = TRUE)\npaint$Primer <- as.factor(paint$Primer)\npaint$Method <- as.factor(paint$Method)\npaint |> \n  ggplot(aes(x = Primer, y = Adhesion, colour = Method, group = Method)) +\n  geom_point(alpha = 0.5)"},{"path":"special-cases-of-multiple-regression.html","id":"notation","chapter":"7 Special cases of multiple regression","heading":"7.3.2 Notation","text":"Let \\(Y_{ijk}\\) \\(k\\)th response level \\(\\) \\(j\\) B.Let \\(Y_{ijk}\\) \\(k\\)th response level \\(\\) \\(j\\) B.Let \\(\\mu_{ij} = \\mathbb{E}(Y_{ijk})\\) mean response level \\(\\) \\(j\\) B.Let \\(\\mu_{ij} = \\mathbb{E}(Y_{ijk})\\) mean response level \\(\\) \\(j\\) B.model \\[Y_{ijk} = \\mu_{ij} + \\epsilon_{ijk}\\]\\(\\epsilon_{ijk} \\sim N(0, \\sigma^2)\\) independent.one way ANOVA model single categorical predictor \\(ab\\) levels.known means model.convenient decompose \\(\\mu_{ij}\\) \\[\\mu_{ij} = \\mu + \\alpha_i + \\beta_j + \\gamma_{ij}\\]\\[Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_{ij}  + \\epsilon_{ijk}\\]can think parameters :\\(\\mu\\) overall level, mean\\(\\mu\\) overall level, mean\\(\\alpha_i\\) additional effect level \\(\\) \\(\\alpha_i\\) additional effect level \\(\\) \\(\\beta_i\\) additional effect level \\(j\\) B\\(\\beta_i\\) additional effect level \\(j\\) B\\(\\gamma_{ij}\\) interaction effect level \\(\\) \\(j\\) B.\\(\\gamma_{ij}\\) interaction effect level \\(\\) \\(j\\) B.known effects model.However, effects model overparameterized.Means model \\(ab\\) \\(\\mu_{ij}\\) parameters, effects model parameterisation, express \\(\\mu_{ij}\\) terms \\(1++b+ ab\\) parameters.Thus need impose \\(1++b\\) identifability constraints.Standard identifiability constraints :\n\\(\\sum_i \\alpha_i = 0\\), \\(\\sum_j \\beta_j = 0\\), \\(\\sum_i \\gamma_{ij} = 0\\) \\(\\sum_j \\gamma_{ij} = 0\\).","code":""},{"path":"special-cases-of-multiple-regression.html","id":"two-way-anova-in-regression-setup","chapter":"7 Special cases of multiple regression","heading":"7.3.3 Two way ANOVA in regression setup","text":"Let \\(A_i\\) indicator level \\(\\) .Let \\(B_j\\) indicator level \\(j\\) B.\\(A_i B_j\\) indicator level \\(\\), B level \\(j\\).Supposing \\(=2\\), \\(b=3\\), can rewrite effects model \n\\[\\begin{align}\ny = \\mu &+  \\alpha_1A_1+  \\alpha_2A_2  \\notag \\\\\n&+ \\beta_1B_1+  \\beta_2B_2 +  \\beta_3B_3   \\notag \\\\\n&+ \\gamma_{11}A_1B_1 + \\gamma_{12}A_1B_2 + \\gamma_{13}A_1B_3 + \\gamma_{21}A_2B_1 + \\gamma_{22}A_2B_2 + \\gamma_{23}A_2B_3 \\\\\n& + \\epsilon\n\\end{align}\\]regression model -parameterized.construct \\(\\mathbf{X}\\) matrix, columns linearly independent.means \\(\\mathbf{X}^T \\mathbf{X}\\) matrix full rank inverse exist.context, obvious identifiability constraints set one \\(\\alpha\\)’s zero, .e. \\(\\alpha_1 =0\\). Also \\(\\beta_1 =0\\) \\(0 = \\gamma_{11}= \\gamma_{12}=\\gamma_{13}=\\gamma_{21}\\), basically parameter index 1.model becomes\n\\[\\begin{align}\ny = \\mu &+   \\alpha_2A_2  \\notag \\\\\n&+   \\beta_2B_2 +  \\beta_3B_3   \\notag \\\\\n& + \\gamma_{22}A_2B_2 + \\gamma_{23}A_2B_3 \\\\\n& + \\epsilon\n\\end{align}\\]required number parameters.Note parameter interpretation depends constraints chosen. example\ncell means ","code":""},{"path":"special-cases-of-multiple-regression.html","id":"interaction-plots","chapter":"7 Special cases of multiple regression","heading":"7.3.4 Interaction plots:","text":"plot means data, can assess whether lines connecting means parallel , indicating interaction .","code":"\npaint_means <- paint |> \n  group_by(Primer, Method) |> \n  summarise(Means = mean(Adhesion))\nrequire(gridExtra)\nplot1 <- paint_means |> \n  ggplot(aes(x = Primer, y = Means, colour = Method, group = Method)) +\n  geom_point(size = 4) + geom_line()\n\nplot2 <-paint_means |> \n  ggplot(aes(x = Method, y = Means, colour = Primer, group = Primer)) +\n  geom_point(size = 4) + geom_line()\ngrid.arrange(plot1, plot2, ncol=2) "},{"path":"special-cases-of-multiple-regression.html","id":"testing-hypotheses","chapter":"7 Special cases of multiple regression","heading":"7.3.5 Testing hypotheses","text":"Interactions: Test \\(H_0: \\gamma_{ij} = 0\\), \\(,j\\).Interactions: Test \\(H_0: \\gamma_{ij} = 0\\), \\(,j\\).Main effect : Test \\(H_0: \\alpha_{} = 0\\), \\(\\).Main effect : Test \\(H_0: \\alpha_{} = 0\\), \\(\\).Main effect B: Test \\(H_0: \\beta_{j} = 0\\), \\(j\\).Main effect B: Test \\(H_0: \\beta_{j} = 0\\), \\(j\\).Perform (1) first. perform (2) (3) evidence interaction. make sense test main effects \npresence interaction.","code":""},{"path":"special-cases-of-multiple-regression.html","id":"two-way-anova-in-r","chapter":"7 Special cases of multiple regression","heading":"7.3.6 Two way ANOVA in R","text":"evidence interaction primer method?Refit model:Based model, combination primer method best use?","code":"\nfit1 <- lm(Adhesion ~ Primer * Method, data = paint)\ncoef(fit1)##     (Intercept)         Primer2         Primer3         Method2 Primer2:Method2 \n##       4.2666667       1.0333333      -0.4333333       1.0333333      -0.2666667 \n## Primer3:Method2 \n##       0.3000000\nanova(fit1)## Analysis of Variance Table\n## \n## Response: Adhesion\n##               Df Sum Sq Mean Sq F value    Pr(>F)    \n## Primer         2 4.5811  2.2906 27.8581 3.097e-05 ***\n## Method         1 4.9089  4.9089 59.7027 5.357e-06 ***\n## Primer:Method  2 0.2411  0.1206  1.4662    0.2693    \n## Residuals     12 0.9867  0.0822                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nfit1 |> model.matrix() |> head()##   (Intercept) Primer2 Primer3 Method2 Primer2:Method2 Primer3:Method2\n## 1           1       0       0       0               0               0\n## 2           1       0       0       0               0               0\n## 3           1       0       0       0               0               0\n## 4           1       1       0       0               0               0\n## 5           1       1       0       0               0               0\n## 6           1       1       0       0               0               0\nfit2 <- lm(Adhesion ~ Primer + Method, data = paint)\ncoef(fit2)## (Intercept)     Primer2     Primer3     Method2 \n##   4.2611111   0.9000000  -0.2833333   1.0444444\nanova(fit2)## Analysis of Variance Table\n## \n## Response: Adhesion\n##           Df Sum Sq Mean Sq F value    Pr(>F)    \n## Primer     2 4.5811  2.2906  26.119 1.884e-05 ***\n## Method     1 4.9089  4.9089  55.975 2.960e-06 ***\n## Residuals 14 1.2278  0.0877                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nfit2 |> \n  broom::augment(paint) |>\n  ggplot(aes(x = Primer, y = Adhesion, col = Method, group = Method)) +\n  geom_point() +\n  geom_line(aes(x = Primer, y = .fitted))"},{"path":"special-cases-of-multiple-regression.html","id":"polynomial-regression","chapter":"7 Special cases of multiple regression","heading":"7.4 Polynomial regression","text":"Data paper strength percent hardwood pulp batch package (Arnholt Evans 2023).straight line fit data. regression significant tell goodness fit.","code":"\nload(here(\"data\", \"hardwood.rda\"))\ndr |> glimpse()## Rows: 19\n## Columns: 2\n## $ tensile  <dbl> 6.3, 11.1, 20.0, 24.0, 26.1, 30.0, 33.8, 34.0, 38.1, 39.9, 42…\n## $ hardwood <dbl> 1.0, 1.5, 2.0, 3.0, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 8.0, 9…\ndr |> \n  ggplot(aes(x = hardwood, y = tensile)) +\n  geom_point()\nfit1 <- lm(tensile ~ hardwood, data = dr)\nsummary(fit1)## \n## Call:\n## lm(formula = tensile ~ hardwood, data = dr)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -25.986  -3.749   2.938   7.675  15.840 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)   \n## (Intercept)  21.3213     5.4302   3.926  0.00109 **\n## hardwood      1.7710     0.6478   2.734  0.01414 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 11.82 on 17 degrees of freedom\n## Multiple R-squared:  0.3054, Adjusted R-squared:  0.2645 \n## F-statistic: 7.474 on 1 and 17 DF,  p-value: 0.01414"},{"path":"special-cases-of-multiple-regression.html","id":"model-2-quadratic-regression","chapter":"7 Special cases of multiple regression","heading":"7.4.1 Model 2: Quadratic regression","text":"Include \\(x^2\\) predictor.Comment: quadratic model better fit. quadratic term significant.Note: can specify model two ways: using poly \\((x,2)\\). However, \\(x\\), \\((x^2)\\) etc correlated correlated variables can cause problems. use poly() lets avoid producing orthogonal polynomials.","code":"\nfit2 <- lm(tensile ~ poly(hardwood,2), data = dr)\nsummary(fit2)## \n## Call:\n## lm(formula = tensile ~ poly(hardwood, 2), data = dr)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.8503 -3.2482 -0.7267  4.1350  6.5506 \n## \n## Coefficients:\n##                    Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)          34.184      1.014  33.709 2.73e-16 ***\n## poly(hardwood, 2)1   32.302      4.420   7.308 1.76e-06 ***\n## poly(hardwood, 2)2  -45.396      4.420 -10.270 1.89e-08 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.42 on 16 degrees of freedom\n## Multiple R-squared:  0.9085, Adjusted R-squared:  0.8971 \n## F-statistic: 79.43 on 2 and 16 DF,  p-value: 4.912e-09\nplot(fit2,1)\nfit2b <- lm(tensile ~ hardwood + I(hardwood ^2), data = dr)\nsummary(fit2b)## \n## Call:\n## lm(formula = tensile ~ hardwood + I(hardwood^2), data = dr)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.8503 -3.2482 -0.7267  4.1350  6.5506 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   -6.67419    3.39971  -1.963   0.0673 .  \n## hardwood      11.76401    1.00278  11.731 2.85e-09 ***\n## I(hardwood^2) -0.63455    0.06179 -10.270 1.89e-08 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.42 on 16 degrees of freedom\n## Multiple R-squared:  0.9085, Adjusted R-squared:  0.8971 \n## F-statistic: 79.43 on 2 and 16 DF,  p-value: 4.912e-09\ndr |> \n  ggplot(aes(x = hardwood, y = tensile)) +\n  geom_point() +\n  geom_line(aes(y=fitted(fit2)))+\n  geom_line(aes(y=fitted(fit2b)), col = 2)"},{"path":"special-cases-of-multiple-regression.html","id":"model-3-cubic-regression","chapter":"7 Special cases of multiple regression","heading":"7.4.2 Model 3: Cubic regression","text":"Comment: cubic model better fit. quadratic term significant.","code":"\nfit3 <- lm(tensile ~ poly(hardwood,3), data = dr)\nsummary(fit3)## \n## Call:\n## lm(formula = tensile ~ poly(hardwood, 3), data = dr)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -4.6250 -1.6109  0.0413  1.5892  5.0216 \n## \n## Coefficients:\n##                    Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)         34.1842     0.5931  57.641  < 2e-16 ***\n## poly(hardwood, 3)1  32.3021     2.5850  12.496 2.48e-09 ***\n## poly(hardwood, 3)2 -45.3963     2.5850 -17.561 2.06e-11 ***\n## poly(hardwood, 3)3 -14.5740     2.5850  -5.638 4.72e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.585 on 15 degrees of freedom\n## Multiple R-squared:  0.9707, Adjusted R-squared:  0.9648 \n## F-statistic: 165.4 on 3 and 15 DF,  p-value: 1.025e-11\nplot(fit3,1)\ndr |> \n  ggplot(aes(x = hardwood, y = tensile)) +\n  geom_point() +\n  geom_line(aes(y=fitted(fit3)), col = 2)"},{"path":"special-cases-of-multiple-regression.html","id":"model-4-quartic","chapter":"7 Special cases of multiple regression","heading":"7.4.3 Model 4: Quartic","text":"Comment: quartic term significant.","code":"\nfit4 <- lm(tensile ~ poly(hardwood,4), data = dr)\nsummary(fit4)## \n## Call:\n## lm(formula = tensile ~ poly(hardwood, 4), data = dr)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.1384 -1.0550 -0.3203  1.0779  4.5030 \n## \n## Coefficients:\n##                    Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)         34.1842     0.5824  58.696  < 2e-16 ***\n## poly(hardwood, 4)1  32.3021     2.5386  12.724 4.39e-09 ***\n## poly(hardwood, 4)2 -45.3963     2.5386 -17.883 4.87e-11 ***\n## poly(hardwood, 4)3 -14.5740     2.5386  -5.741 5.10e-05 ***\n## poly(hardwood, 4)4  -3.1647     2.5386  -1.247    0.233    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.539 on 14 degrees of freedom\n## Multiple R-squared:  0.9736, Adjusted R-squared:  0.9661 \n## F-statistic: 129.1 on 4 and 14 DF,  p-value: 6.994e-11\nplot(fit4,1)\ndr |> \n  ggplot(aes(x = hardwood, y = tensile)) +\n  geom_point() +\n  geom_line(aes(y=fitted(fit4)), col = 2)"},{"path":"special-cases-of-multiple-regression.html","id":"confounding-example","chapter":"7 Special cases of multiple regression","heading":"7.5 Confounding example","text":"autumn, small winged fruit called samara fall maple trees, spinning go. forest scientist studied relationship fast fell “disk loading” (quantity based size weight).\ndata give loadings fall velocities fruit three trees. Ryan, Joiner, Rogosa (1994).Y = VelocityX = LoadA = Tree: 3 levelsSome models fit:Different intercepts, slope\\[y = \\beta_0 +  \\beta_1z_2 +   \\beta_2z_3  +  \\epsilon\\]Common slope intercept\\[y = \\beta_0 +  \\beta_1x + \\epsilon\\]Common slope\\[y = \\beta_0 +  \\beta_1x +  \\beta_2z_2 +   \\beta_3z_3  +  \\epsilon\\]Separate lines\\[y = \\beta_0 +  \\beta_1x +  \\beta_2z_2 +   \\beta_3z_3 + \\\\\n\\beta_4 x \\times z_2  + \\beta_5 x \\times z_3+ \\epsilon\\]\\(z_i\\) indicator variable \\(\\)th level factor .plots can see velocity different trees. Particularly, tree 3 seems lowest velocity. can also see tree 2 wide range velocities.difference velocity size samara fruits tree also affects ? expect samara specific load fall velocity tree 1 fallen tree 3?can answer questions comparing models .\\(\\beta_5\\) difference change mean circumference associated 1 unit increase Load tree 3 opposed tree 1.Write 95% confidence interval \\(\\beta_5\\).estimated change mean circumference associated 1 unit increase Load tree 3?","code":"\ntree <- read.table(here(\"data\", \"samara.txt\"), header=TRUE)\ntree |> glimpse()## Rows: 36\n## Columns: 3\n## $ Tree     <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n## $ Load     <dbl> 0.239, 0.208, 0.223, 0.224, 0.246, 0.213, 0.198, 0.219, 0.241…\n## $ Velocity <dbl> 1.34, 1.06, 1.14, 1.13, 1.35, 1.23, 1.23, 1.15, 1.25, 1.24, 1…\ntree$Tree <- as.factor(tree$Tree)\ntree <- tree[complete.cases(tree),]\nfit0 <-lm(Velocity~Tree, data=tree)\ntree |> ggplot(aes(x = Tree, y = Velocity, color = Tree)) + \n  geom_point() + \n  geom_point(aes(y=fitted(fit0)), pch = 3, col= \"black\")\nsummary(fit0)## \n## Call:\n## lm(formula = Velocity ~ Tree, data = tree)\n## \n## Residuals:\n##       Min        1Q    Median        3Q       Max \n## -0.300000 -0.082500  0.005833  0.087500  0.250000 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  1.23417    0.03676  33.571  < 2e-16 ***\n## Tree2       -0.05417    0.05316  -1.019    0.316    \n## Tree3       -0.28333    0.05199  -5.450 5.37e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1274 on 32 degrees of freedom\n## Multiple R-squared:  0.5097, Adjusted R-squared:  0.479 \n## F-statistic: 16.63 on 2 and 32 DF,  p-value: 1.117e-05\nfit1 <- lm(Velocity~Load, data=tree)\ntree |> ggplot(aes(x = Load, y = Velocity, color = Tree)) + \n  geom_point() + \n  geom_line(aes(y=fitted(fit1)))\nfit2 <- lm(Velocity~Load+Tree, data=tree)\ntree |> ggplot(aes(x = Load, y = Velocity, color = Tree)) + \n  geom_point() + \n  geom_line(aes(y=fitted(fit2)))\nfit3 <- lm(Velocity~Tree * Load, data=tree)\ntree |> ggplot(aes(x = Load, y = Velocity, color = Tree)) + \n  geom_point() + \n  geom_line(aes(y=fitted(fit3)))\nsummary(fit3)## \n## Call:\n## lm(formula = Velocity ~ Tree * Load, data = tree)\n## \n## Residuals:\n##       Min        1Q    Median        3Q       Max \n## -0.120023 -0.049465 -0.001298  0.049938  0.145571 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)  \n## (Intercept)   0.5414     0.2632   2.057   0.0488 *\n## Tree2        -0.8408     0.3356  -2.505   0.0181 *\n## Tree3        -0.2987     0.4454  -0.671   0.5078  \n## Load          3.0629     1.1599   2.641   0.0132 *\n## Tree2:Load    3.7343     1.5000   2.490   0.0188 *\n## Tree3:Load    0.8205     2.2837   0.359   0.7220  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.07554 on 29 degrees of freedom\n## Multiple R-squared:  0.8436, Adjusted R-squared:  0.8167 \n## F-statistic: 31.29 on 5 and 29 DF,  p-value: 7.656e-11"},{"path":"special-cases-of-multiple-regression.html","id":"comparing-models-2-and-3","chapter":"7 Special cases of multiple regression","heading":"7.5.1 Comparing models 2 and 3:","text":"Test \\(H_0: \\beta_4= \\beta_5 = 0\\) versus \\(H_a: \\beta_4, \\beta_5\\) zero.reject \\(H_0\\), use model 2 describe association Velocity, Load tree.","code":"\nanova(fit2, fit3)## Analysis of Variance Table\n## \n## Model 1: Velocity ~ Load + Tree\n## Model 2: Velocity ~ Tree * Load\n##   Res.Df     RSS Df Sum of Sq     F  Pr(>F)  \n## 1     31 0.20344                             \n## 2     29 0.16549  2  0.037949 3.325 0.05011 .\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"special-cases-of-multiple-regression.html","id":"comparing-models-1-and-2","chapter":"7 Special cases of multiple regression","heading":"7.5.2 Comparing models 1 and 2:","text":"Starting model 2 full model, can see model 1 appropriate simplification:Test \\(H_0: \\beta_2= \\beta_3 = 0\\) versus \\(H_a: \\beta_2, \\beta_3\\) zero.conclusion, found Tree doesn’t affect velocity, Load . samara fruits Load, velocity every tree. reason velocity Tree 3 smaller smaller fruits.","code":"\nanova(fit1, fit2)## Analysis of Variance Table\n## \n## Model 1: Velocity ~ Load\n## Model 2: Velocity ~ Load + Tree\n##   Res.Df     RSS Df Sum of Sq      F Pr(>F)\n## 1     33 0.21476                           \n## 2     31 0.20344  2  0.011322 0.8626 0.4319"},{"path":"special-cases-of-multiple-regression.html","id":"type-1-and-type-2-sums-of-squares","chapter":"7 Special cases of multiple regression","heading":"7.5.3 Type 1 and type 2 sums of squares","text":"","code":"\nanova(fit3)## Analysis of Variance Table\n## \n## Response: Velocity\n##           Df  Sum Sq  Mean Sq F value    Pr(>F)    \n## Tree       2 0.53942 0.269708  47.262 7.488e-10 ***\n## Load       1 0.31554 0.315542  55.294 3.406e-08 ***\n## Tree:Load  2 0.03795 0.018975   3.325   0.05011 .  \n## Residuals 29 0.16549 0.005707                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\ncar::Anova(fit3)## Anova Table (Type II tests)\n## \n## Response: Velocity\n##             Sum Sq Df F value    Pr(>F)    \n## Tree      0.011322  2   0.992   0.38306    \n## Load      0.315542  1  55.294 3.406e-08 ***\n## Tree:Load 0.037949  2   3.325   0.05011 .  \n## Residuals 0.165492 29                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"special-cases-of-multiple-regression.html","id":"another-factor","chapter":"7 Special cases of multiple regression","heading":"7.5.4 Another factor","text":"Suppose data collected 2 Days, suspected day might also impact relationship Load Velocity.model allows 3-way interaction Load, Tree Day.model allows 3-way interaction Load, Tree Day.can say interaction Tree Load varies Day, equivalently, interaction Load Day varies Tree.can say interaction Tree Load varies Day, equivalently, interaction Load Day varies Tree.assess whether Day may omitted, compare fit3:assess whether Day may omitted, compare fit3:","code":"\ntree$Day <- factor(rep(1:2, length=35))\nfit4 <- lm(Velocity ~  Tree*Load*Day, data=tree)\ntree$fit4 <- fitted(fit4)\nggplot(tree, aes(x = Load, y = Velocity, color = Tree)) + \n  geom_point()+  facet_wrap(~Day) + \n  stat_smooth(method = \"lm\", se = FALSE)\n# ggplot(tree, aes(x = Load, y = Velocity, color = Tree)) + geom_point()+  facet_wrap(~Day)+ geom_line(aes(y=fit4 )) # the same\nanova(fit3, fit4)## Analysis of Variance Table\n## \n## Model 1: Velocity ~ Tree * Load\n## Model 2: Velocity ~ Tree * Load * Day\n##   Res.Df     RSS Df Sum of Sq      F Pr(>F)\n## 1     29 0.16549                           \n## 2     23 0.14061  6  0.024887 0.6785 0.6684"},{"path":"special-cases-of-multiple-regression.html","id":"quadratic-terms-and-interactions","chapter":"7 Special cases of multiple regression","heading":"7.6 Quadratic terms and interactions","text":"Example Ramsey Schafer (2002) pg 252. data corn yields rainfall `RainfallData.csv’, library(Sleuth3) ‘ex0915’.\nVariables:Yield: corn yield (bushels/acre)Rainfall: rainfall (inches/year)Year: year.Link: http://www.rpubs.com/kdomijan/332466","code":""},{"path":"special-cases-of-multiple-regression.html","id":"an-example-with-two-continuous-and-two-categorical-predictors","chapter":"7 Special cases of multiple regression","heading":"7.7 An example with two continuous and two categorical predictors","text":"FEV data - full description see Kahn (2005).Response variable: fev (forced expiratory volume) measures respiratory function.Predictors: age, height, gender smoke.dataset library(covreg).smoking affect pulmonary function?plot appers smoking improves lung function! However, plot, smoking adjusted age body size.Let’s visualize effect smoking fev accounting age height:Smokers (blue) tend lower fev compared individuals age/height.Smokers (blue) tend lower fev compared individuals age/height.Younger children less likely smokers, likely lower fev older (bigger) ones.Younger children less likely smokers, likely lower fev older (bigger) ones.example Simpson’s paradox.example Simpson’s paradox.Height age independent - older children tend taller, trend tapers 14.Height age independent - older children tend taller, trend tapers 14.gender? children older 10, boys taller girls. Males (blue) higher fev, confounded height.gender? children older 10, boys taller girls. Males (blue) higher fev, confounded height.Let’s fit models.Age positive effect fev smoking negative. plot age fev suggested include smoking age interaction.second model includes interaction, shown significant:Display fitted model:model fit well? Let’s take look fits vs residuals diagnostic plot (coloured gender).see:Increasing variance residuals.Increasing variance residuals.Gender height might account .Gender height might account .Remember relationship age height, well height gender.Remember relationship age height, well height gender.scatterplots indicated quadratic fit might good option height.see :Quadratic term significantQuadratic term significantSmoking longer significantSmoking longer significantInteraction height?Interaction height?Display fitted model:Let’s consider another, complex model:model fit well? points coloured gender.see even small dataset, many models possible. One can consider adding interactions last model . can helpful think relationships variables structured way. sometimes set directed graph (causal diagram).One argue height consequence smoke (unlike confounders age sex). including height model (adjusting height) might get downward biased (incorrect) estimate effect smoking.\nindepth discussion causal analysis datatset see Cummiskey et al. (2020).Without height:","code":"\nload(here(\"data\", \"fev.rda\"))\nglimpse(fev)## Rows: 654\n## Columns: 5\n## $ age    <int> 9, 8, 7, 9, 9, 8, 6, 6, 8, 9, 6, 8, 8, 8, 8, 7, 5, 6, 9, 9, 5, …\n## $ fev    <dbl> 1.708, 1.724, 1.720, 1.558, 1.895, 2.336, 1.919, 1.415, 1.987, …\n## $ height <dbl> 57.0, 67.5, 54.5, 53.0, 57.0, 61.0, 58.0, 56.0, 58.5, 60.0, 53.…\n## $ male   <int> 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, …\n## $ smoke  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\nfev <- fev |> \n  mutate(male = as.factor(male), \n                      smoke = as.factor(smoke))\nfit1 <- lm(fev~ age + smoke, data = fev)\nsummary(fit1)## \n## Call:\n## lm(formula = fev ~ age + smoke, data = fev)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.68070 -0.35220 -0.04599  0.35034  2.08515 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.349135   0.081787   4.269 2.26e-05 ***\n## age          0.232476   0.008223  28.272  < 2e-16 ***\n## smoke1      -0.208911   0.080573  -2.593  0.00973 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.564 on 651 degrees of freedom\n## Multiple R-squared:  0.5782, Adjusted R-squared:  0.5769 \n## F-statistic: 446.1 on 2 and 651 DF,  p-value: < 2.2e-16\nfit2 <- lm(fev~ age * smoke, data = fev)\nsummary(fit2)## \n## Call:\n## lm(formula = fev ~ age * smoke, data = fev)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.77573 -0.34712 -0.03269  0.33471  2.05749 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.242056   0.083021   2.916  0.00367 ** \n## age          0.243704   0.008371  29.114  < 2e-16 ***\n## smoke1       1.904894   0.424926   4.483 8.70e-06 ***\n## age:smoke1  -0.159959   0.031594  -5.063 5.38e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.5536 on 650 degrees of freedom\n## Multiple R-squared:  0.5942, Adjusted R-squared:  0.5923 \n## F-statistic: 317.2 on 3 and 650 DF,  p-value: < 2.2e-16\nfit3 <- lm(fev~ height + I(height^2)+ smoke , data = fev)\nsummary(fit3)## \n## Call:\n## lm(formula = fev ~ height + I(height^2) + smoke, data = fev)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.80474 -0.23068 -0.00234  0.21778  1.99269 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  6.0404522  1.5047762   4.014 6.66e-05 ***\n## height      -0.2507239  0.0499314  -5.021 6.63e-07 ***\n## I(height^2)  0.0031632  0.0004121   7.676 6.04e-14 ***\n## smoke1      -0.0185929  0.0563310  -0.330    0.741    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.413 on 650 degrees of freedom\n## Multiple R-squared:  0.7741, Adjusted R-squared:  0.7731 \n## F-statistic: 742.6 on 3 and 650 DF,  p-value: < 2.2e-16\nfit4 <- lm(fev~  male+ height + I(height^2)  + age + smoke, data = fev)\nsummary(fit4)## \n## Call:\n## lm(formula = fev ~ male + height + I(height^2) + age + smoke, \n##     data = fev)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.61264 -0.22793  0.00617  0.22435  1.80390 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  6.8630488  1.4990763   4.578 5.62e-06 ***\n## male1        0.0945539  0.0328676   2.877  0.00415 ** \n## height      -0.2732761  0.0496785  -5.501 5.44e-08 ***\n## I(height^2)  0.0031165  0.0004086   7.628 8.54e-14 ***\n## age          0.0699792  0.0091943   7.611 9.62e-14 ***\n## smoke1      -0.1325347  0.0570996  -2.321  0.02059 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.3951 on 648 degrees of freedom\n## Multiple R-squared:  0.7939, Adjusted R-squared:  0.7923 \n## F-statistic: 499.2 on 5 and 648 DF,  p-value: < 2.2e-16\nfit5 <- lm(fev~ male*smoke+age*smoke, data = fev)\nsummary(fit5)## \n## Call:\n## lm(formula = fev ~ male * smoke + age * smoke, data = fev)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.87731 -0.33574 -0.02565  0.30803  1.93283 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   0.123763   0.081090   1.526  0.12744    \n## male1         0.278636   0.043524   6.402 2.95e-10 ***\n## smoke1        2.065510   0.404618   5.105 4.36e-07 ***\n## age           0.240731   0.007974  30.188  < 2e-16 ***\n## male1:smoke1  0.456419   0.141805   3.219  0.00135 ** \n## smoke1:age   -0.181915   0.030397  -5.985 3.59e-09 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.5265 on 648 degrees of freedom\n## Multiple R-squared:  0.6341, Adjusted R-squared:  0.6312 \n## F-statistic: 224.6 on 5 and 648 DF,  p-value: < 2.2e-16"},{"path":"about.html","id":"about","chapter":"8 About","heading":"8 About","text":"","code":""},{"path":"about.html","id":"license","chapter":"8 About","heading":"8.1 License","text":"book licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License https://creativecommons.org/licenses/-nc-sa/4.0/.","code":""},{"path":"about.html","id":"citation","chapter":"8 About","heading":"8.2 Citation","text":"BibTeX entry LaTeX users :following packages used creating notes:","code":"@Manual{,\n  title = {Notes for ST463/ST683 Linear Models 1},\n  author = {Katarina Domijan, Catherine Hurley},\n  year = {2018},\n  organization = {Department of Mathematics and Statistics, Maynooth University},\n  address = {Maynooth, Ireland},\n  url = {https://bookdown.org/domijank/ST463/},\n  doi = {10.5281/zenodo.17945213},\n}##  [1] \"base (4.5.1)\"           \"bookdown (0.46)\"        \"broom (1.0.11)\"        \n##  [4] \"car (3.1.3)\"            \"carData (3.0.5)\"        \"condvis2 (0.1.2)\"      \n##  [7] \"datasauRus (0.1.9)\"     \"datasets (4.5.1)\"       \"distributional (0.5.0)\"\n## [10] \"dplyr (1.1.4)\"          \"forcats (1.0.1)\"        \"GGally (2.4.0)\"        \n## [13] \"ggdist (3.3.3)\"         \"ggplot2 (4.0.1)\"        \"graphics (4.5.1)\"      \n## [16] \"grDevices (4.5.1)\"      \"gridExtra (2.3)\"        \"here (1.0.2)\"          \n## [19] \"knitr (1.50)\"           \"lubridate (1.9.4)\"      \"MASS (7.3.65)\"         \n## [22] \"methods (4.5.1)\"        \"purrr (1.2.0)\"          \"readr (2.1.6)\"         \n## [25] \"rosdata (0.1.1)\"        \"scatterplot3d (0.3.44)\" \"Sleuth3 (1.0.6)\"       \n## [28] \"stats (4.5.1)\"          \"stringr (1.6.0)\"        \"tibble (3.3.0)\"        \n## [31] \"tidyr (1.3.1)\"          \"tidyverse (2.0.0)\"      \"utils (4.5.1)\"         \n## [34] \"xtable (1.8.4)\""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
