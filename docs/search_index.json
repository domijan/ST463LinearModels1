[["index.html", "Linear Models Lecture Notes Chapter 1 Module Preliminaries 1.1 Recommended texts 1.2 Software", " Linear Models Lecture Notes Katarina Domijan 2025-12-16 Chapter 1 Module Preliminaries This book contains lecture notes for module ST303 Linear Models offered by the Mathematics and Statistics Department at Maynooth University. As such it is continually revised to teach the course, the content may change from year to year. A base link is https://bookdown.org/domijank/ST463/ Note that the module code has changed since. Feel free to adopt this book or parts of it to your own purposes, but please acknowledge its use in your own work. To cite this book, please use: Katarina Domijan, Catherine Hurley (2018). Notes for ST463/ST683 Linear Models 1. Department of Mathematics and Statistics, Maynooth University, Maynooth Ireland. Online textbook (version 1.0, November 12, 2018). Retrieved from https://bookdown.org/domijank/ST463/, DOI:10.5281/zenodo.17945213 1.1 Recommended texts There are many good resources. These lecture notes are based on: Weisberg (2005), Fox (2005), Fox (2016), Ramsey and Schafer (2002), Draper and Smith (1966). 1.2 Software We will use R. I will have a lecture on R for beginners to bring people to speed in week 2. All code needed to complete assignments will be similar to examples in the notes. To create this document, I am using the bookdown package (Xie 2018) which was built on top of R Markdown (Allaire et al. 2025), (Xie, Allaire, and Grolemund 2018), (Xie, Dervieux, and Riederer 2020) and knitr (Xie 2015). We will use: tidyverse (Wickham et al. 2019), in particular dplyr (Wickham et al. 2022) and ggplot2 (Wickham 2016). Other useful packages used are: broom(Robinson, Hayes, and Couch 2022), MASS (Venables and Ripley 2002), car (Fox and Weisberg 2011), Sleuth3 (F. L. Ramsey et al. 2016). References Allaire, JJ, Yihui Xie, Christophe Dervieux, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, et al. 2025. Rmarkdown: Dynamic Documents for r. https://github.com/rstudio/rmarkdown. Draper, Norman Richard, and Harry Smith. 1966. Applied Regression Analysis. Wiley Series in Probability and Mathematical Statistics. Wiley. F. L. Ramsey, Original by, D. W. Schafer; modifications by Daniel W. Schafer, Jeannie Sifneos, Berwin A. Turlach; vignettes contributed by Nicholas Horton, Linda Loi, Kate Aloisio, Ruobing Zhang, and with corrections by Randall Pruim. 2016. Sleuth3: Data Sets from Ramsey and Schafer’s \"Statistical Sleuth (3rd Ed)\". https://CRAN.R-project.org/package=Sleuth3. Fox, John. 2005. Regression diagnostics. Sage Publishers. ———. 2016. Applied Regression Analysis and Generalized Linear Models. 3rd ed. SAGE Publications. Fox, John, and Sanford Weisberg. 2011. An R Companion to Applied Regression. Second. Thousand Oaks CA: Sage. http://socserv.socsci.mcmaster.ca/jfox/Books/Companion. Ramsey, Fred, and Daniel Schafer. 2002. The Statistical Sleuth: A Course in Methods of Data Analysis. 2nd ed. Duxbury Press. Robinson, David, Alex Hayes, and Simon Couch. 2022. Broom: Convert Statistical Objects into Tidy Tibbles. https://CRAN.R-project.org/package=broom. Venables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with s. Fourth. New York: Springer. http://www.stats.ox.ac.uk/pub/MASS4. Weisberg, Sanford. 2005. Applied linear regression. Wiley-Blackwell. Wickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. http://ggplot2.org. Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686. Wickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2022. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr. Xie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.name/knitr/. ———. 2018. Bookdown: Authoring Books and Technical Documents with r Markdown. https://CRAN.R-project.org/package=bookdown. Xie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. R Markdown: The Definitive Guide. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown. Xie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. R Markdown Cookbook. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown-cookbook. "],["intro.html", "Chapter 2 Introduction 2.1 Data and Models 2.2 Introductory Examples", " Chapter 2 Introduction 2.1 Data and Models Modeling: development of mathematical expressions that describe the behavior of a random variable of interest. The variable of interest is called the response (or dependent) variable and denoted with \\(Y\\). Predictor or explanatory variables (also called the independent variables) are thought to provide information on the behavior of \\(Y\\) and are incorporated into the model. Predictor variables will be denoted as \\(X\\). Data consist of information taken from \\(n\\) units. Subscripts \\(i = 1,..., n\\) identify the particular unit from which the observations were taken. Additional subscripts can be used to identify different predictors. All models involve unknown constants, called parameters, which control the behavior of the model. These parameters are denoted by Greek letters (e.g. \\(\\beta\\)) and are to be estimated from the data. We denote estimates using hat notation, e.g. \\(\\hat{\\beta}\\). In this module we will study linear models. Here the parameters enter the model as simple coefficients on the \\(X\\)s or functions of \\(X\\)s. 2.2 Introductory Examples A first look at how \\(Y\\) changes as \\(X\\) is varied is seen in a scatterplot. 2.2.1 Mother and daughter heights Data from Pearson and Lee (1903). heights &lt;- read.table(here(&quot;data&quot;, &quot;heights.txt&quot;), header = TRUE) head(heights) ## Mheight Dheight ## 1 59.7 55.1 ## 2 58.2 56.5 ## 3 60.6 56.0 ## 4 60.7 56.8 ## 5 61.8 56.0 ## 6 55.5 57.9 \\(\\bar{y}=\\) 63.75, sd(\\(y\\)) = 2.6 heights |&gt; ggplot(aes(x = Dheight)) + geom_histogram() heights |&gt; ggplot(aes(x = Mheight, y = Dheight)) + geom_point() + geom_abline() + xlab(&quot;mother height (inches)&quot;) + ylab(&quot;daughter height (inches)&quot;) Taller mothers have taller daughters. Since most points fall above line \\(y=x\\) most daughters are taller. heights |&gt; ggplot(aes(x = Mheight, y = Dheight)) + geom_point() + geom_abline() + geom_smooth(method = &quot;lm&quot;, col = &quot;red&quot;) + xlab(&quot;mother height (inches)&quot;) + ylab(&quot;daughter height (inches)&quot;) -Does the data follow a linear pattern? If so we can use the linear regression line to summarise the data. -We can use the regression line to predict a daughters height based on her mother’s height. This is: \\(\\hat{y}=\\) 29.92 +0.54 \\(x\\) 2.2.2 Bacterial count and storage temperature bacteria &lt;- read.csv(here(&quot;data&quot;, &quot;bacteria.csv&quot;)) bacteria |&gt; ggplot(aes(x = temp, y = count)) + geom_jitter(width = 0.1, height = .1) + geom_smooth(method = lm, se = FALSE) Points are jittered to avoid overprinting. It does not appear to be a linear relationship. Consider a transformation? bacteria |&gt; ggplot(aes(x = temp, y = log(count))) + geom_jitter(width = 0.1, height = .1) + geom_smooth(method = lm, se = FALSE) Log transformed bacteria counts appear to have a linear relationship with temperature. 2.2.3 Yield and Rainfall The dataset is from Ramsey and Schafer (2002). The data on corn yields and rainfall are in `ex0915’ in library(Sleuth3) (F. L. Ramsey et al. 2016). Variables: Yield: corn yield (bushels/acre) Rainfall: rainfall (inches/year) Year: year. library(GGally) library(Sleuth3) ggpairs(ex0915[, c(2, 3, 1)], upper = list( continuous = &quot;points&quot;, combo = &quot;facethist&quot;, discrete = &quot;facetbar&quot;, na = &quot;na&quot; ), lower = list( continuous = &quot;cor&quot;, combo = &quot;box_no_facet&quot;, discrete = &quot;count&quot;, na = &quot;na&quot; )) 2.2.4 Driving Example from: Weisberg (2005). Study how fuel consumption varies over 50 US states and the District of Columbia and the effect of state gasoline tax on the consuption. Variable: FuelC: Gasoline sold for road use, thousands of gallons Drivers:Number of licensed drivers in the state Income: Per person personal income for the year 2000, in thousands of dollars Miles: Miles of Federal-aid highway miles in the state Pop: 2001 population age 16 and over Tax: Gasoline state tax rate, cents per gallon State: State name We will use a scatterplot matrix. driving &lt;- read.table(here(&quot;data&quot;, &quot;fuel2001.txt&quot;), header = TRUE) driving &lt;- driving[, -c(5, 8)] pairs(driving[, c(2, 1, 3, 4, 5, 6)]) Both Drivers and FuelC are state totals so will be larger in more populous states. Income is per person, we want to make variables comparable. Transform variables: FuelC2:FuelC/Pop Drivers2: Drivers/Pop Miles2:log\\(_2\\)(Miles) driving2 &lt;- driving |&gt; mutate(Drivers = Drivers / Pop, FuelC = FuelC / Pop, Miles = log(Miles, 2)) pairs(driving2[, c(2, 1, 3, 4, 5, 6)]) FuelC decreases as tax increases but there is a lot of variation. Fuel is weakly related to a number of other variables. Other graphical representations of the dataset: library(MASS) parcoord(driving2[, c(2, 6, 1, 3, 4, 5)]) Parallel coordinates in package MASS (Venables and Ripley 2002). We can fit a linear model predicting FuelC using all other variables. We would be particularly interested in the relationship between Tax and FuelC but we need to take into account (adjust for) the other predictors. We can also look at the data and the model that we will fit using conditional visualisation (C. B. Hurley, O’Connell, and Domijan 2022) in R package condvis2 (C. Hurley, OConnell, and Domijan 2022). If you run the code below, it will open an interactive plot in a separate window. It shows a low-dimensional visualisation, constructed showing the relationship between the response FuelC and the predictor Tax, conditional on the remaining predictors. The conditioning values can be selected within the shiny app. fit1 &lt;- lm(FuelC~., driving2) library(condvis2) condvis(driving2, fit1, sectionvars = &quot;Tax&quot;) 2.2.5 Fuel Consumption Information was recorded on fuel usage and average temperature (\\(^oF\\)) over the course of one week for eight office complexes of similar size. Data from Bowerman and Schafer (1990). We expect fuel use to be driven by weather conditions. Fuel use: response or dependent variable. Denoted by \\(Y\\). Temperature: Explanatory or predictor variable. Denoted by \\(X\\). We observe n=8 pairs: \\((x_{i}, y_{i}), i =1,...,8\\). Temp &lt;- c(28, 28, 32.5, 39, 45.9, 57.8, 58.1, 62.5) Fuel &lt;- c(12.4, 11.7, 12.4, 10.8, 9.4, 9.5, 8, 7.5) FuelTempData &lt;- data.frame(cbind(Temp, Fuel)) Temp Fuel 28.0 12.4 28.0 11.7 32.5 12.4 39.0 10.8 45.9 9.4 57.8 9.5 58.1 8.0 62.5 7.5 FuelTempData |&gt; ggplot(aes(x = Temp, y = Fuel)) + geom_point() + geom_smooth(method = lm, se = FALSE) The scatterplot shows that fuel use decreases roughly linearly as temperature increases. We assume there’s an underlying true line: \\[\\mbox{Fuel} =\\beta_{0} + \\beta_{1}\\mbox{Temp} + \\epsilon\\] or, more generally: \\(y =\\beta_{0} + \\beta_{1}x + \\epsilon.\\) The intercept (\\(\\beta_0\\)) and slope (\\(\\beta_1\\)), are unknown parameters and \\(\\epsilon\\) is the random error component. For each observation we have:\\(y_i =\\beta_{0} + \\beta_{1}x_i + \\epsilon_i\\). We can estimate \\(\\beta_0\\) and \\(\\beta_1\\) from the available data. One method that can be used to do this is the method of ordinary least squares. NOTE: other models are possible: FuelTempData |&gt; ggplot(aes(x = Temp, y = Fuel)) + geom_point() + geom_smooth(method = loess, se = FALSE) 2.2.6 Elections and Economy library(rosdata) hibbs |&gt; ggplot(aes(x = growth, y = vote, label = year)) + geom_text() Variables: year: 1952 to 2012 growth: inflation-adjusted growth in average personal income vote: percentage of votes for the incumbent party’s candidate in US presidential elections The better the economy was performing, the better the incumbent party’s candidate did, with the biggest exceptions being 1952 (Korean War) and 1968 (Vietnam War). Example from (Gelman, Hill, and Vehtari 2020) available in package (Gelman, Hill, and Vehtari 2025). References Bowerman, Bruce L., and Daniel Schafer. 1990. Linear Statistical Models. 2nd ed. Thomson Wadsworth. F. L. Ramsey, Original by, D. W. Schafer; modifications by Daniel W. Schafer, Jeannie Sifneos, Berwin A. Turlach; vignettes contributed by Nicholas Horton, Linda Loi, Kate Aloisio, Ruobing Zhang, and with corrections by Randall Pruim. 2016. Sleuth3: Data Sets from Ramsey and Schafer’s \"Statistical Sleuth (3rd Ed)\". https://CRAN.R-project.org/package=Sleuth3. Gelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Analytical Methods for Social Research. Cambridge University Press. https://doi.org/10.1017/9781139161879. ———. 2025. Rosdata: Data for Regression and Other Stories. https://github.com/avehtari/ROS-Examples. Hurley, Catherine B., Mark O’Connell, and Katarina Domijan. 2022. “Interactive Slice Visualization for Exploring Machine Learning Models.” Journal of Computational and Graphical Statistics 31 (1): 1–13. https://doi.org/10.1080/10618600.2021.1983439. Hurley, Catherine, Mark OConnell, and Katarina Domijan. 2022. Condvis2: Interactive Conditional Visualization for Supervised and Unsupervised Models in Shiny. https://doi.org/10.32614/CRAN.package.condvis2. Pearson, K, and A Lee. 1903. “On the Laws of Inheritance in Man: I. Inheritance of Physical Characters.” Biometrika 2: 357–462. Ramsey, Fred, and Daniel Schafer. 2002. The Statistical Sleuth: A Course in Methods of Data Analysis. 2nd ed. Duxbury Press. Venables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with s. Fourth. New York: Springer. http://www.stats.ox.ac.uk/pub/MASS4. Weisberg, Sanford. 2005. Applied linear regression. Wiley-Blackwell. "],["SLR.html", "Chapter 3 Simple Linear regression 3.1 Ordinary least squares 3.2 The formal simple linear regression model 3.3 Simple linear regression models in R 3.4 Statistical inference 3.5 Analysis of variance (for s.l.r.) 3.6 Sample correlation 3.7 Assessing the simple linear regression model assumptions 3.8 A note on the Galton paradox", " Chapter 3 Simple Linear regression 3.1 Ordinary least squares We have seen some introductory examples in Section 2.2.5. Fuel consumption example, what is the `best fitting line’ to summarise the linear trend? \\[y_i =\\beta_{0} + \\beta_{1}x_i + \\epsilon_i.\\] The method of ordinary least squares chooses \\(\\beta_{0}\\), \\(\\beta_{1}\\) to minimise: \\[\\begin{align*} S(\\beta_{0}, \\beta_{1}) &amp; =\\sum_{i=1}^{n}\\epsilon_i^2 \\\\ &amp; = \\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2\\\\ \\end{align*}\\] The least squares estimators \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) must satisfy: \\(\\frac{\\delta S}{\\delta \\beta_0} = 0\\) and \\(\\frac{\\delta S}{\\delta \\beta_1} = 0\\). \\[\\begin{align*} \\frac{\\delta S}{\\delta \\beta_0} &amp; = - 2\\sum_{i=1}^{n} (y_i-\\beta_0-\\beta_1x_i) \\\\ \\frac{\\delta S}{\\delta \\beta_1} &amp; = - 2\\sum_{i=1}^{n} x_i(y_i-\\beta_0-\\beta_1x_i). \\end{align*}\\] Setting these to 0 at \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) gives: \\[\\begin{align} \\sum_{i=1}^{n} (y_i-\\hat{\\beta}_0-\\hat{\\beta}_1x_i) &amp; =0 \\tag{3.1}\\\\ \\sum_{i=1}^{n} x_i(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1x_i)&amp; =0. \\tag{3.2} \\end{align}\\] These equations ((3.1) and (3.2)) are called the normal equations. From (3.1): \\[\\begin{align*} \\sum_{i=1}^{n} y_i-n\\hat{\\beta}_0-\\hat{\\beta}_1\\sum_{i=1}^{n}x_i &amp; =0\\\\ \\hat{\\beta}_0&amp; =\\bar{y}-\\hat{\\beta}_1\\bar{x}. \\end{align*}\\] Substitute into (3.2): \\[\\begin{align*} \\sum_{i=1}^{n}x_i( y_i-\\bar{y}+\\hat{\\beta}_1\\bar{x}-\\hat{\\beta}_1x_i) &amp; =0\\\\ \\sum_{i=1}^{n}x_i( y_i-\\bar{y}) &amp; =\\hat{\\beta}_1\\sum_{i=1}^{n}x_i(x_i-\\bar{x})\\\\ \\hat{\\beta}_1&amp; = \\frac{\\sum_{i=1}^{n}x_i( y_i-\\bar{y})}{\\sum_{i=1}^{n}x_i(x_i-\\bar{x})}\\\\ &amp; =\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})( y_i-\\bar{y})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\\\\ &amp; =\\frac{S_{xy}}{S_{xx}}. \\end{align*}\\] Some notation: \\[\\begin{align*} S_{xx} &amp; =\\sum_{i=1}^{n}(x_{i} - \\bar{x})^{2} =\\sum_{i=1}^{n}x_{i}^{2} - n\\bar{x}^2 \\\\ S_{yy} &amp; =\\sum_{i=1}^{n}(y_{i} - \\bar{y})^{2} =\\sum_{i=1}^{n}y_{i}^{2} - n\\bar{y}^2 \\\\ S_{xy} &amp; =\\sum_{i=1}^{n}(x_{i} - \\bar{x})(y_{i} - \\bar{y}) = \\sum_{i=1}^{n}x_{i}y_{i} - n\\bar{x}\\bar{y} \\end{align*}\\] So, the equation of the OLS fitted line is given by: \\[\\hat{y} =\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x,\\] where \\[\\hat{\\beta}_{1} = \\frac{S_{xy}}{S_{xx}}\\] and \\[\\hat{\\beta}_{0} = \\bar{y}-\\hat{\\beta}_1\\bar{x}.\\] 3.1.1 Residuals The fitted value at each observation is: \\[\\hat{y}_i =\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_i\\] The residuals are computed as: \\[e_i = y_i-\\hat{y}_i\\] 3.1.2 Some algebraic implications of the OLS fit \\(\\sum_{i=1}^n e_i = \\sum_{i=1}^n (y_i - \\hat{y}_i) = 0\\) (residuals sum to 0) \\(\\sum_{i=1}^n x_i e_i = \\sum_{i=1}^n x_i(y_i - \\hat{y}_i) = 0\\) \\(\\sum_{i=1}^n y_i = \\sum_{i=1}^n \\hat{y}_i\\) (from (3.1)) \\(\\bar{y} = \\hat{\\beta}_0+\\hat{\\beta}_1\\bar{x}\\) (OLS line always goes through the mean of the sample) \\(\\sum_{i=1}^n \\hat{y}_ie_i = 0\\) (from (3.1) and (3.2)). 3.1.3 OLS Estimates for the Fuel Consumption Example x &lt;- Temp y &lt;- Fuel n &lt;- 8 cbind(x, y, xsq = x^2, ysq = y^2, xy = x * y) ## x y xsq ysq xy ## [1,] 28.0 12.4 784.00 153.76 347.20 ## [2,] 28.0 11.7 784.00 136.89 327.60 ## [3,] 32.5 12.4 1056.25 153.76 403.00 ## [4,] 39.0 10.8 1521.00 116.64 421.20 ## [5,] 45.9 9.4 2106.81 88.36 431.46 ## [6,] 57.8 9.5 3340.84 90.25 549.10 ## [7,] 58.1 8.0 3375.61 64.00 464.80 ## [8,] 62.5 7.5 3906.25 56.25 468.75 sum(x) ## [1] 351.8 sum(y) ## [1] 81.7 mean(x) ## [1] 43.975 mean(y) ## [1] 10.2125 sum(x^2) ## [1] 16874.76 sum(y^2) ## [1] 859.91 sum(x * y) ## [1] 3413.11 Sxx &lt;- sum(x^2) - n * mean(x)^2 Sxx ## [1] 1404.355 Syy &lt;- sum(y^2) - n * mean(y)^2 Syy ## [1] 25.54875 Sxy &lt;- sum(x * y) - n * mean(x) * mean(y) Sxy ## [1] -179.6475 Calculate \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\): \\[\\begin{align*} \\hat{\\beta}_{1} &amp; = \\frac{S_{xy}}{S_{xx}} = \\frac{\\sum_{i=1}^{n}x_{i}y_{i} - n\\bar{x}\\bar{y}}{\\sum_{i=1}^{n}x_{i}^{2} - n\\bar{x}^{2}} \\\\ &amp; =\\frac{-179.65}{1404.355} = -0.128 \\end{align*}\\] \\(\\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1}\\bar{x} = 10.212 - ( - 0.128)(43.98) =15.84\\) The equation of the fitted line is \\(\\hat{y}= 15.84 - 0.128 x\\). 3.1.4 Interpretation of the fitted simple linear regression line: Parameter estimates -0.128 is the estimated change in mean fuel use for a 1\\(^oF\\) increase in temperature. In theory, 15.84 is the estimated mean fuel use at a temperature of 0\\(^oF\\). However, we have no reason to believe this is a good estimate because our data contains no information about the fuel-temperature relationship below 28\\(^oF\\). 3.1.5 Predicting The fitted line allows us to predict fuel use at any temperature within the range of the data. For example, at \\(x=30^oF\\): \\[\\hat{y}_i = 15.84 - 0.128 \\times 30 = 12.\\] 12 units of fuel is the estimated fuel use at \\(30^oF\\). E.g. at \\(x=40\\); \\(\\hat{y} = 10.721\\), at \\(x=50\\); \\(\\hat{y} = 9.442\\). 3.2 The formal simple linear regression model The SLR model tries to capture two features: a linear trend and fluctuations (scatter about that trend). Because of random variations in experimental conditions we do not expect to get the same value of \\(y\\) even if we keep repeating the experiment at various fixed \\(x\\) values. SLR model tries to model the scatter about the regression line. We will have to make some assumptions about the behaviour of these chance fluctuations. 3.2.1 Model The SLR model is of the form: \\[y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i}, \\hspace{0.5cm} \\epsilon_{i} \\sim N(0, \\sigma^{2}), \\hspace{0.5cm} i=1,...,n. \\] \\(\\beta_0\\) and \\(\\beta_1\\) are unknown parameters \\(y\\) and \\(\\epsilon\\) are random \\(x\\) is assumed non-random We use errors \\(\\epsilon_{i}\\) to model the chance fluctuations about the regression line (i.e. the underlying true line). So the SLR model assumes that these errors, i.e. vertical distances from the observed point to the regression line, are, on average, equal to zero. It also assumes that they are normally distributed. Another assumption is that the \\(\\epsilon_{i}\\) values are independent and identically distributed (IID). 3.2.2 Assumptions \\(\\mathbb{E}[\\epsilon_{i}] = 0\\), so \\(\\mathbb{E}[y_{i}] = \\beta_0 + \\beta_1x_i+ \\mathbb{E}[\\epsilon_i] = \\beta_0 + \\beta_1x_i\\). Var(\\(\\epsilon_i\\)) = \\(\\sigma^2\\). Equivalently Var(\\(y_{i}\\)) = \\(\\sigma^2\\). \\(\\epsilon_i\\) are independent (therefore \\(y_{i}\\) also are). \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). Equivalently \\(y_i \\sim N(\\beta_0 + \\beta_1x_i, \\sigma^2)\\). NOTE: if \\(x_i\\) are random then the model says that \\(\\mathbb{E}[y_{i}|x_i] = \\beta_0 + \\beta_1x_i\\) and Var(\\(y_{i}|x_i\\)) = \\(\\sigma^2\\). 3.2.3 Estimation of \\(\\sigma^2\\) NOTE: \\(\\sigma^2\\) = Var(\\(\\epsilon_i\\)) The errors \\(\\epsilon_i\\) are not observable, but the residuals, \\(e_i\\) should have similar properties. We estimate \\(\\sigma^2\\) by \\[\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n e_i^2}{n-2}.\\] \\(n-2\\) is the degrees of freedom and \\(\\sum_{i=1}^n e_i^2\\) is called the residual sum of squares, denoted \\(\\mbox{SSE}\\). 3.2.4 Review of some probability results Let \\(U\\), \\(W\\) and \\(Z\\) be three random variables: \\(\\mathbb{E}[U]\\) = mean of the distribution of \\(U\\) Var \\((U) = \\mathbb{E}[U^2] - (\\mathbb{E}[U])^2\\) Cov(\\(U,U\\)) = Var(\\(U\\)) Cov(\\(U, W\\)) = \\(\\mathbb{E}[UW] - \\mathbb{E}[U]\\mathbb{E}[W]\\) If \\(U\\) and \\(W\\) are uncorrelated or independent then Cov(\\(U,W\\)) = 0 \\(\\mbox{Corr}(U,W) = \\frac{\\mbox{Cov}(U,W)}{\\sqrt{\\mbox{Var}(U)\\mbox{Var}(W)}}\\) For constants \\(a\\) and \\(b\\): \\(\\mathbb{E}[aU+bW] = a\\mathbb{E}[U] + b\\mathbb{E}[W]\\) Var(\\(aU \\pm bW\\)) = \\(a^2\\)Var[\\(U\\)] + \\(b^2\\)Var[\\(W\\)] \\(\\pm\\) \\(2ab\\)Cov(\\(U\\),\\(W\\)) Cov(\\(aU+bW, cZ\\)) = \\(ac\\)Cov(\\(U\\),\\(Z\\)) + \\(bc\\)Cov(\\(W\\),\\(Z\\)) 3.2.5 Properties of the estimates When the model holds: \\(\\hat{\\beta}_1 \\sim N\\left(\\beta_1,\\frac{\\sigma^2}{S_{xx}}\\right)\\) \\(\\hat{\\beta}_0 \\sim N\\left(\\beta_0, \\sigma^2\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right)\\right)\\) Cov\\((\\hat{\\beta}_0, \\hat{\\beta}_1) = -\\sigma^2\\frac{\\bar{x}}{S_{xx}}\\) \\(\\hat{y} \\sim N\\left(\\beta_0 + \\beta_1x, \\sigma^2\\left(\\frac{1}{n}+ \\frac{(x-\\bar{x})^2}{S_{xx}}\\right)\\right)\\) \\((n-2)\\frac{\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi^2_{(n-2)}\\) \\(\\mathbb{E}[\\hat{\\sigma}^2] = \\sigma^2\\) Proof of (1): First show that \\(\\mathbb{E}[\\hat{\\beta}_1] = \\beta_1\\). \\[\\begin{align*} \\hat{\\beta}_1 &amp; = \\frac{S_{xy}}{S_{xx}} \\\\ &amp; = \\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})( y_i-\\bar{y})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2} \\\\ &amp; = \\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})y_i}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2} \\\\ &amp; = \\sum_{i = 1}^n a_iy_i \\end{align*}\\] where \\(a_i\\) depend only on \\(x\\) and are NOT random. By linearity of expectation: \\[\\begin{align*} \\mathbb{E}[\\hat{\\beta}_1] &amp; = \\sum_{i = 1}^n a_i\\mathbb{E}[y_i]\\\\ &amp; = \\sum_{i = 1}^n a_i (\\beta_0+\\beta_1x_i) \\mbox{ (from the model assumptions)}\\\\ &amp; = \\beta_0\\sum_{i = 1}^n a_i+\\beta_1\\sum_{i = 1}^n a_i x_i \\end{align*}\\] But \\[\\sum_{i = 1}^n a_i = \\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2} = 0,\\] And \\[\\begin{align*} \\sum_{i = 1}^n a_i x_i &amp; = \\frac{\\sum_{i=1}^{n}x_i(x_i-\\bar{x})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2} \\\\ &amp; = \\frac{\\sum_{i=1}^{n}x_i^2-n\\bar{x}^2}{S_{xx}} \\\\ &amp; = \\frac{S_{xx}}{S_{xx}} = 1. \\end{align*}\\] So \\(\\mathbb{E}[\\hat{\\beta}_1] = \\beta_1\\) as required. Second, show that Var(\\(\\hat{\\beta}_1\\)) = \\(\\frac{\\sigma^2}{S_{xx}}\\) \\[\\begin{align*} \\mbox{Var}(\\hat{\\beta}_1) &amp; = \\mbox{Var} \\left( \\sum_{i = 1}^n a_i y_i \\right)\\\\ &amp; = \\sum_{i = 1}^n a_i^2 \\mbox{Var}(y_i) \\mbox{(since $y_i$s are independent)}\\\\ &amp; = \\sigma^2 \\sum_{i = 1}^n a_i^2\\\\ &amp; = \\sigma^2 \\sum_{i = 1}^n \\left( \\frac{x_i-\\bar{x}}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2} \\right)^2\\\\ &amp; = \\sigma^2 \\frac{\\sum_{i = 1}^n (x_i-\\bar{x})^2}{(\\sum_{i=1}^{n}(x_i-\\bar{x})^2)^2} \\\\ &amp; = \\sigma^2 \\frac{S_{xx}}{(S_{xx})^2} \\\\ &amp; = \\frac{ \\sigma^2}{S_{xx}} \\mbox{ (as required)} \\end{align*}\\] Finally, the normality assumption follows as \\(\\hat{\\beta}_1\\) is a linear combination of normal random variables (\\(y_i\\)s). Proof of (2): First show that \\(\\mathbb{E}[\\hat{\\beta}_0] = \\beta_0\\). \\[\\begin{align*} \\mathbb{E}[\\hat{\\beta}_0] &amp; = \\mathbb{E}[\\bar{y} - \\hat{\\beta}_1\\bar{x}]\\\\ &amp; = \\mathbb{E}[\\bar{y}] - \\beta_1 \\bar{x}\\\\ &amp; = \\frac{1}{n}\\sum_{i = 1}^n\\mathbb{E}[y_i] - \\beta_1 \\bar{x}\\\\ &amp; = \\frac{1}{n}\\sum_{i = 1}^n (\\beta_0+ \\beta_1 x_i) - \\beta_1 \\bar{x}\\\\ &amp; = \\frac{1}{n}( n\\beta_0 + \\beta_1 \\sum_{i = 1}^n x_i) - \\beta_1 \\bar{x}\\\\ &amp; = \\beta_0 + \\beta_1 \\bar{x} - \\beta_1 \\bar{x}\\\\ &amp; = \\beta_0 \\mbox{ (as required)} \\end{align*}\\] Second, show that Var(\\(\\hat{\\beta}_0\\)) = \\(\\sigma^2\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right)\\) \\[\\begin{align*} \\mbox{Var}(\\hat{\\beta}_0) &amp; =\\mbox{Var}(\\bar{y} - \\hat{\\beta}_1\\bar{x}) \\\\ &amp; = \\mbox{Var}(\\bar{y}) + \\bar{x}^2 \\mbox{Var}(\\hat{\\beta}_1) - 2\\bar{x}\\mbox{Cov}(\\bar{y}, \\hat{\\beta}_1) \\end{align*}\\] \\[\\begin{align*} \\mbox{Cov}(\\bar{y}, \\hat{\\beta}_1) &amp; =\\mbox{Cov}\\left( \\frac{1}{n}\\sum_{i = 1}^n y_i, \\sum_{i = 1}^n a_iy_i \\right)\\\\ &amp; =\\sum_{i = 1}^n \\sum_{j = 1}^n \\frac{1}{n} a_i\\mbox{Cov}(y_i, y_j)\\\\ &amp; =\\frac{1}{n} \\sum_{i = 1}^n \\sum_{j = 1}^n a_i\\mbox{Cov}(y_i, y_j)\\\\ &amp; =\\frac{1}{n} \\sum_{i = 1}^n a_i\\mbox{Cov}(y_i, y_i) \\mbox{ (since $y_i$ are indep.)} \\\\ &amp; =\\frac{\\sigma^2}{n} \\sum_{i = 1}^n a_i\\\\ &amp; = 0 \\end{align*}\\] \\[\\begin{align*} \\mbox{Var}(\\hat{\\beta}_0) &amp; = \\mbox{Var}(\\bar{y}) + \\bar{x}^2 \\mbox{Var}(\\hat{\\beta}_1) \\\\ &amp; = \\frac{1}{n^2} \\sum_{i = 1}^n \\mbox{Var}(y_i) + \\bar{x}^2 \\frac{\\sigma^2}{S_{xx}}\\\\ &amp; = \\frac{1}{n^2} n\\sigma^2 + \\bar{x}^2 \\frac{\\sigma^2}{S_{xx}}\\\\ &amp; = \\sigma^2\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right) \\mbox{ (as required)} \\end{align*}\\] Finally, the normality assumption follows as \\(\\hat{\\beta}_0\\) is a linear combination of normal random variables (\\(y_i\\)s and \\(\\hat{\\beta}_1\\)). Proof of (3): \\[\\begin{align*} \\mbox{Cov}(\\hat{\\beta}_0, \\hat{\\beta}_1) &amp; =\\mbox{Cov}(\\bar{y} - \\hat{\\beta}_1\\bar{x}, \\hat{\\beta}_1)\\\\ &amp; =\\mbox{Cov}(\\bar{y}, \\hat{\\beta}_1) - \\mbox{Cov}(\\hat{\\beta}_1\\bar{x}, \\hat{\\beta}_1) \\\\ &amp; = 0 - \\bar{x}\\mbox{Cov}(\\hat{\\beta}_1, \\hat{\\beta}_1) \\\\ &amp; = -\\bar{x} \\frac{\\sigma^2}{S_{xx}} \\end{align*}\\] Proof of (4): First show that \\(\\mathbb{E}[\\hat{y}] = \\beta_0 + \\beta_1 x\\). \\[\\begin{align*} \\mathbb{E}[\\hat{y}] &amp; = \\mathbb{E}[\\hat{\\beta}_0 + \\hat{\\beta}_1x]\\\\ &amp; = \\mathbb{E}[\\hat{\\beta}_0] + \\mathbb{E}[\\hat{\\beta}_1]x\\\\ &amp; = \\beta_0 + \\beta_1 x \\mbox{ (as required)} \\end{align*}\\] Second, show that Var(\\(\\hat{y}\\)) = \\(\\sigma^2\\left(\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{S_{xx}}\\right)\\) \\[\\begin{align*} \\mbox{Var}(\\hat{y}) &amp; =\\mbox{Var}(\\hat{\\beta}_0 + \\hat{\\beta}_1 x) \\\\ &amp; =\\mbox{Var}(\\bar{y} - \\hat{\\beta}_1\\bar{x} + \\hat{\\beta}_1 x) \\\\ &amp; =\\mbox{Var}(\\bar{y} + \\hat{\\beta}_1(x - \\bar{x})) \\\\ &amp; =\\mbox{Var}(\\bar{y}) + (x - \\bar{x})^2 \\mbox{Var}(\\hat{\\beta}_1) + 2(x - \\bar{x})\\mbox{Cov}(\\bar{y}, \\hat{\\beta}_1)\\\\ &amp; =\\frac{\\sigma^2}{n} + (x - \\bar{x})^2 \\frac{\\sigma^2}{S_{xx}}\\\\ &amp; =\\sigma^2\\left(\\frac{1}{n} + \\frac{(x - \\bar{x})^2}{S_{xx}}\\right) \\quad \\mbox{ (as required)}. \\end{align*}\\] Finally, the normality assumption follows as \\(\\hat{y}\\) is a linear combination of \\(y_i\\)s. 3.2.6 Special cases At \\(x = 0\\), \\(\\hat{y} = \\hat{\\beta}_0\\). At \\(x = x_i\\), \\(\\hat{y} = \\hat{y}_i\\). \\[\\begin{align*} \\mbox{Var}(\\hat{y}_i) &amp; =\\sigma^2\\left(\\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{S_{xx}}\\right)\\\\ &amp; =\\sigma^2h_{ii} \\end{align*}\\] NOTE: \\(h_{ii} = \\left(\\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{S_{xx}}\\right)\\) is a distance value (see later!) 3.3 Simple linear regression models in R 3.3.1 R Some useful code is below: mylm &lt;- lm(Fuel ~ Temp, data = FuelTempData) summary(mylm) ## ## Call: ## lm(formula = Fuel ~ Temp, data = FuelTempData) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.5663 -0.4432 -0.1958 0.2879 1.0560 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15.83786 0.80177 19.754 1.09e-06 *** ## Temp -0.12792 0.01746 -7.328 0.00033 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6542 on 6 degrees of freedom ## Multiple R-squared: 0.8995, Adjusted R-squared: 0.8827 ## F-statistic: 53.69 on 1 and 6 DF, p-value: 0.0003301 anova(mylm) ## Analysis of Variance Table ## ## Response: Fuel ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Temp 1 22.9808 22.981 53.695 0.0003301 *** ## Residuals 6 2.5679 0.428 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 mylm |&gt; broom::augment(FuelTempData) |&gt; head() ## # A tibble: 6 × 8 ## Temp Fuel .fitted .resid .hat .sigma .cooksd .std.resid ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 28 12.4 12.3 0.144 0.307 0.712 0.0154 0.264 ## 2 28 11.7 12.3 -0.556 0.307 0.651 0.231 -1.02 ## 3 32.5 12.4 11.7 0.720 0.219 0.617 0.217 1.24 ## 4 39 10.8 10.8 -0.0489 0.143 0.716 0.000542 -0.0807 ## 5 45.9 9.4 9.97 -0.566 0.128 0.663 0.0628 -0.927 ## 6 57.8 9.5 8.44 1.06 0.261 0.460 0.623 1.88 mylm |&gt; broom::augment(FuelTempData) |&gt; ggplot(aes(x = Temp, y = Fuel)) + geom_point() + geom_line(aes(x = Temp, y = .fitted)) 3.4 Statistical inference 3.4.1 R simulation: Reminder: the linear relationship \\(\\mathbb{E}[y_{i}] = \\beta_{0} + \\beta_{1}x_{i}\\) is the underlying true line and the values of its parameters (intercept \\(\\beta_0\\) and slope \\(\\beta_1\\)) are unknown. We estimate the parameters with \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). The parameter estimates have sampling distributions. To simulate a dataset, choose true model parameters \\(\\beta_0\\) \\(\\beta_1\\) and \\(\\sigma^2\\). beta_0 &lt;- 1 beta_1 &lt;- 2 sigma_sq &lt;- 10 Choose sample size \\(n\\): n &lt;- 20 # number of observations in the sample Simulate \\(x_i\\) and \\(\\epsilon_i\\) for \\(i = 1,..., n\\): set.seed(12) x &lt;- runif(n, -2, 2) eps &lt;- rnorm(n, 0, sqrt(sigma_sq)) Calculate \\(y_i\\): y &lt;- beta_0 + x * beta_1 + eps Let’s have a look at our made up data: plot(x, y) abline(beta_0, beta_1, col = 2) # true model points(mean(x), mean(y), col = 3) fit &lt;- lm(y ~ x) # fit the model abline(coef(fit)[1], coef(fit)[2], col = 3) Red is the true model and green line is estimated from the simmulated dataset. Let’s compare the estimated parameters with \\(\\beta_0\\) \\(\\beta_1\\) and \\(\\sigma^2\\): hat_beta_0 &lt;- coef(fit)[1] hat_beta_1 &lt;- coef(fit)[2] s &lt;- sqrt(sum(residuals(fit)^2) / (n - 2)) hat_beta_0 ## (Intercept) ## 1.073254 hat_beta_1 ## x ## 2.107893 s^2 ## [1] 6.541065 We can think of these as single realisations from their sampling distribution of the estimators Some useful calculations for sampling distributions of estimators: x_bar &lt;- mean(x) Sxx &lt;- sum(x^2) - n * (x_bar^2) var_beta_0 &lt;- sigma_sq * (1 / n + x_bar^2 / Sxx) var_beta_1 &lt;- sigma_sq / Sxx est_cov &lt;- -sigma_sq * x_bar / Sxx # estimated covariance from one sample se_fit &lt;- sqrt(sigma_sq * (1 / n + (x - x_bar)^2 / Sxx)) Let’s plot the sampling dists for these estimators: \\(\\hat{\\beta}_1 \\sim N\\left(\\beta_1,\\frac{\\sigma^2}{S_{xx}}\\right)\\) data.frame(hat_beta_1)|&gt; ggplot(aes(xdist = dist_normal(beta_1, sqrt(var_beta_1)))) + stat_slab() + geom_point(aes(x = hat_beta_1, y = 0)) + ggtitle(&quot;sampling distribution&quot;)+ xlab(expression(hat(beta)[1]))+ ylab(&quot;density&quot;) \\(\\hat{\\beta}_0 \\sim N\\left(\\beta_0, \\sigma^2\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right)\\right)\\) data.frame(hat_beta_0)|&gt; ggplot(aes(xdist = dist_normal(beta_0, sqrt(var_beta_0)))) + stat_slab() + geom_point(aes(x = hat_beta_0, y = 0)) + ggtitle(&quot;sampling distribution&quot;)+ xlab(expression(hat(beta)[0]))+ ylab(&quot;density&quot;) \\((n-2)\\frac{\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi^2_{(n-2)}\\) data.frame(s^2)|&gt; ggplot(aes(xdist = dist_chisq(n-2))) + stat_slab() + geom_point(aes(x = s^2 *(n - 2) / sigma_sq, y = 0)) + ggtitle(&quot;sampling distribution&quot;)+ xlab(expression((n - 2)*hat(sigma)^2 / sigma^2 ))+ ylab(&quot;density&quot;) Now let’s repeat this simulation process 500 times: N &lt;- 500 # number of simulations estimates &lt;- matrix(0, N, 3) # object to save parameter estimates for (i in 1:N) { # x &lt;- runif(n, -5, 5) eps &lt;- rnorm(n, 0, sqrt(sigma_sq)) y &lt;- beta_0 + x * beta_1 + eps fit &lt;- lm(y ~ x) estimates[i, 1] &lt;- coef(fit)[1] estimates[i, 2] &lt;- coef(fit)[2] estimates[i, 3] &lt;- anova(fit)[[&quot;Mean Sq&quot;]][2] # sigamsq } Plot the \\(N = 500\\) fitted lines: Plot the estimates and compare to sampling distribution curves: estimates |&gt; ggplot(aes(x = beta_hat_0)) + geom_dots() + stat_slab(aes(xdist = dist_normal(beta_0, sqrt(var_beta_0))), col = 1, fill = NA) + ggtitle(&quot;sampling distribution and simulation estimates&quot;)+ xlab(expression(hat(beta)[0])) + ylab(&quot;Density&quot;) estimates |&gt; ggplot(aes(x = beta_hat_1)) + geom_dots() + stat_slab(aes(xdist = dist_normal(beta_1, sqrt(var_beta_1))), col = 1, fill = NA, height = 0.75) + ggtitle(&quot;sampling distribution and simulation estimates&quot;)+ xlab(expression(hat(beta)[1])) + ylab(&quot;Density&quot;) estimates |&gt; ggplot(aes(x = sigma_sq_hat*(n - 2) / sigma_sq)) + geom_dots()+ stat_slab(aes(xdist = dist_chisq(n-2)), col = 1, fill = NA) + ggtitle(&quot;sampling distribution and simulation estimates&quot;)+ xlab(expression(expression((n - 2)*hat(sigma)^2 / sigma^2 ))) + ylab(&quot;Density&quot;) 3.4.2 Inference for \\(\\beta_1\\) 3.4.2.1 Confidence Interval \\(\\hat{\\beta}_1\\) estimates \\(\\beta_1\\), for example the change in fuel use for a 1\\(^oF\\) increase in temperature. We would like to construct a confidence interval for \\(\\beta_1\\). This will give us an interval where we are confident the true \\(\\beta_1\\) lies. The key to obtaining a C.I. is the fact that: \\[\\hat{\\beta}_1 \\sim N(\\beta_1, \\frac{\\sigma^2}{S_{xx}}).\\] Equivalently \\[\\frac{\\hat{\\beta}_{1} - \\beta_{1}}{\\sigma/\\sqrt{S_{xx}}} \\sim N(0,1).\\] And, when we replace \\(\\sigma\\) by \\(\\hat{\\sigma}\\) we have: \\[\\frac{\\hat{\\beta}_{1} - \\beta_{1}}{\\hat{\\sigma}/\\sqrt{S_{xx}}} \\sim t_{n-2}.\\] The df is \\(n-2\\) because this is the df associated with the estimate of \\(\\sigma\\). In general, when: \\[\\frac{\\mbox{Est - parameter}}{\\mbox{S.E.(Est)}} \\sim \\mbox{distribution}.\\] A C.I. for the parameter is given by: \\[\\mbox{Est} \\pm \\mbox{(quantile from distribution)} \\times \\mbox{S.E.(Est)}.\\] A \\((1-\\alpha)\\times 100\\%\\) confidence interval for \\(\\beta_{1}\\): \\[\\hat{\\beta}_{1} \\pm t_{n-2}(\\alpha/2) \\times \\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}}.\\] For the fuel use data, a \\(95\\%\\) C.I. for \\(\\beta_1\\) is: \\[\\begin{align*} \\hat{\\beta}_1 &amp;\\pm t_6(0.025) \\times \\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}}\\\\ &amp; = -0.128 \\pm 2.45 \\times 0.018\\\\ &amp; = (-0.171, -0.085) \\end{align*}\\] We are \\(95\\%\\) confident that the average fuel use drop is between 0.085 and 0.171. 3.4.2.2 Hypothesis test In some settings we may wish to test: \\(H_{0}: \\beta_{1} = 0\\) versus \\(H_{A}: \\beta_{1} \\ne 0\\). The null hypothesis here is that \\(\\mathbb{E}[y] = \\beta_0\\) i.e. \\(\\mathbb{E}[y]\\) is not linearly related to \\(x\\). Under \\(H_0\\): \\[t_{obs} = \\frac{\\hat{\\beta}_{1} - 0}{\\hat{\\sigma}/\\sqrt{S_{xx}}} \\sim t_{n-2}.\\] P-value = \\(P[T_{n-2} \\geq |t_{obs}|]\\) Reject \\(H_0\\) for small p-values, typically \\(p&lt; 0.05\\). In the fuel use example: \\[t_{obs} = \\frac{-0.128-0}{0.018} = -7.33\\] and p-value \\(&lt; 0.001\\), so we reject \\(H_0\\) and conclude that \\(\\beta_{1} \\ne 0\\). We could also test \\(H_0: \\beta_1=b\\) by computing: \\[\\frac{\\hat{\\beta}_{1} - b}{\\mbox{S.E}.(\\hat{\\beta}_{1})}.\\] 3.4.3 Inference for \\(\\beta_0\\) A \\(95\\%\\) C.I. for \\(\\beta_0\\) is: \\[\\hat{\\beta}_{0} \\pm t_{n-2}(\\alpha/2) \\times \\mbox{S.E.}(\\hat{\\beta}_{0}).\\] Note: \\(\\mbox{S.E.}(\\hat{\\beta}_0) = \\sqrt{\\hat{\\sigma}^2\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right)}\\) For the fuel use data: \\[\\begin{align*} &amp; = 15.84 \\pm 2.45 \\times 0.8018\\\\ &amp; = (13.88, 17.80) \\end{align*}\\] We can also test for a particular value of \\(\\beta_0\\), e.g. \\(H_0: \\beta_0 = 0\\) vs. \\(H_A: \\beta_0 \\neq 0\\) The null hypothesis here is that \\(\\mathbb{E}[y] = \\beta_1 x\\) i.e. the line passes through the origin. The test statistic is \\[\\frac{\\hat{\\beta}_{0} - \\beta_0}{\\mbox{S.E.}(\\hat{\\beta}_{0})} = 19.75.\\] for the fuel data. P-value \\(= 2P[T_6 \\geq 19.75] &lt;0.001.\\) Note: This is for illustration, in practice with this particular data we would not do this. Why? 3.4.4 Inference for mean response Suppose we want to estimate \\(\\mu=\\mathbb{E}[y]\\) at a particular value of \\(x\\). At \\(x_0\\) let: \\[\\mu_0 = \\mathbb{E}[y_0] = \\beta_0 + \\beta_1 x_0\\] We can estimate \\(\\mu_0\\) by: \\[\\hat{y}_0 = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0.\\] e.g. we estimate the mean fuel use at temperature \\(50^oF\\) by \\[\\hat{y}_0 = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times 50 =15.84-0.128 \\times 50 =9.44.\\] A \\(95\\%\\) C.I. for \\(\\mu_0\\) is given by \\[\\begin{align*} \\mbox{Est} &amp;\\pm t_{n-2}(\\alpha/2) \\times \\mbox{S.E.(Est)}\\\\ \\hat{y}_0 &amp;\\pm t_{n-2}(\\alpha/2) \\times \\mbox{S.E.}_{\\mbox{fit}}(\\hat{y}_0) \\end{align*}\\] where \\[\\begin{align*} \\mbox{S.E.}_{\\mbox{fit}}(\\hat{y}_0)&amp; =\\hat{\\sigma}\\sqrt{h_{00}}\\\\ &amp; = \\hat{\\sigma}\\sqrt{\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}}\\\\ &amp; = 0.65 \\times \\sqrt{0.15}=0.254 \\end{align*}\\] The \\(95\\%\\) C.I. is \\[\\begin{align*} &amp; = 9.44 \\pm 2.45 \\times 0.254\\\\ &amp; = (8.8, 10.1) \\end{align*}\\] This interval contains the true mean fuel use at \\(50^oF\\) with \\(95\\%\\) confidence. 3.4.5 Inference for prediction Suppose we want to predict the response at a particular value of \\(x\\). At \\(x_0\\), let \\(y_0\\) be the unobserved response. From our model: \\[y_0 =\\mu_0 + \\epsilon= \\beta_0 + \\beta_1 x_0 + \\epsilon\\] and we estimate (predict) it by: \\[\\hat{y}_0 = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0.\\] Note: estimation of a random variable is called prediction. In our example, we predict that the fuel use at temp \\(50^oF\\) as: \\[\\hat{y}_0 = 15.84-0.128\\times 50=9.44\\] Note that the prediction of future response equals the estimate of the mean response, however the associated standard errors (and hence confidence intervals) are different. Confidence intervals for random variables are called prediction intervals (PIs). In our prediction of \\(y_0\\) by \\(\\hat{y}_0\\) the prediction error \\(y_0-\\hat{y}_0\\) is with variance: \\[\\begin{align*} \\mbox{Var}(y_0-\\hat{y}_0)&amp; = \\mbox{Var}(y_0)+ \\mbox{Var}(\\hat{y}_0)\\mbox{ (indep as $y_0$ is out of sample)}\\\\ &amp; =\\sigma^2 + \\sigma^2\\times h_{00} \\\\ &amp; = \\sigma^2(1+h_{00})\\\\ &amp; = \\sigma^2\\left(1+\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}\\right) \\end{align*}\\] The \\(\\mbox{S.E.}\\) of the prediction is then: \\[\\mbox{S.E.}_{\\mbox{pred}} (\\hat{y}_0) = \\hat{\\sigma}\\sqrt{1+\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}}\\] The \\(95\\%\\) prediction interval is \\[\\hat{y}_0 \\pm t_{n-2}(\\alpha/2) \\times \\mbox{S.E.}_{\\mbox{pred}} (\\hat{y}_0)\\] For \\(x_0\\) = 50, \\(\\hat{y}_0 =9.442\\): \\[\\mbox{S.E.}_{\\mbox{pred}}= \\sqrt{1+0.15} \\times 0.654 = 0.702\\] So the \\(95\\%\\) P.I. is: \\[\\begin{align*} &amp; = 9.44 \\pm 2.45 \\times 0.702\\\\ &amp; = (7.72, 11.16) \\end{align*}\\] We are \\(95\\%\\) sure that the interval contains the actual fuel use on a week with temp = \\(50^oF\\). 3.4.6 Plotting CIs and PIs in R mylm |&gt; broom::augment(FuelTempData, interval = c(&quot;confidence&quot;)) |&gt; head(4) ## # A tibble: 4 × 10 ## Temp Fuel .fitted .lower .upper .resid .hat .sigma .cooksd .std.resid ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 28 12.4 12.3 11.4 13.1 0.144 0.307 0.712 0.0154 0.264 ## 2 28 11.7 12.3 11.4 13.1 -0.556 0.307 0.651 0.231 -1.02 ## 3 32.5 12.4 11.7 10.9 12.4 0.720 0.219 0.617 0.217 1.24 ## 4 39 10.8 10.8 10.2 11.5 -0.0489 0.143 0.716 0.000542 -0.0807 FuelTempData |&gt; ggplot(aes(x = Temp, y = Fuel)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = TRUE) mylm |&gt; broom::augment(FuelTempData, interval = c(&quot;prediction&quot;)) |&gt; ggplot(aes(x = Temp, y = Fuel))+ geom_point() + geom_line(aes(y = .fitted)) + geom_ribbon(aes(ymin = .lower, ymax = .upper), fill = &quot;pink&quot;, alpha = 0.4) newx &lt;- seq(30, 60) CI &lt;- predict(mylm, newdata = data.frame(Temp = newx), interval = c(&quot;confidence&quot;), level = 0.95, type = &quot;response&quot;) CI |&gt; head(4) ## fit lwr upr ## 1 12.00021 11.17760 12.82281 ## 2 11.87228 11.08013 12.66444 ## 3 11.74436 10.98149 12.50724 ## 4 11.61644 10.88152 12.35136 PI &lt;- predict(mylm, newdata = data.frame(Temp = newx), interval = c(&quot;prediction&quot;), level = 0.95, type = &quot;response&quot;) PI |&gt; head(4) ## fit lwr upr ## 1 12.00021 10.200424 13.79999 ## 2 11.87228 10.086217 13.65835 ## 3 11.74436 9.971085 13.51764 ## 4 11.61644 9.855011 13.37787 3.5 Analysis of variance (for s.l.r.) The analysis of variance is a method for comparing the fit of two or more models to the same dataset. It is particularly useful in multiple regression. 3.5.1 Vote Data: does regression on growth explain vote? In simple linear regression, this is equivalent to testing hypotheses about the slope parameter. In other words: comparing the fit of the simple linear regression model to the fit of the null (intercept only) model. lin_mod &lt;- lm(vote ~ growth, data = hibbs) anova(lin_mod) ## Analysis of Variance Table ## ## Response: vote ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## growth 1 273.63 273.632 19.321 0.00061 *** ## Residuals 14 198.27 14.162 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 3.5.2 ANOVA decomposition \\[\\begin{align*} \\mbox{data} &amp; = \\mbox{fit} + \\mbox{residual} \\\\ y_i &amp; = \\hat{y}_i +e_i\\\\ y_i - \\bar{y} &amp; = \\hat{y}_i - \\bar{y} +e_i \\\\ \\sum_{i = 1}^n ( y_i - \\bar{y})^2 &amp; =\\sum_{i = 1}^n(\\hat{y}_i - \\bar{y} +e_i)^2\\\\ &amp; =\\sum_{i = 1}^n(\\hat{y}_i - \\bar{y})^2 +\\sum_{i = 1}^ne_i^2 + 2\\sum_{i = 1}^n (\\hat{y}_i - \\bar{y})e_i \\end{align*}\\] The last term is zero because (from normal equations): \\(\\sum_{i = 1}^n\\hat{y}_ie_i = 0\\) and \\(\\sum_{i = 1}^ne_i = 0\\). The decomposition: \\[\\sum_{i = 1}^n ( y_i - \\bar{y})^2 =\\sum_{i = 1}^n(\\hat{y}_i - \\bar{y})^2 +\\sum_{i = 1}^ne_i^2\\] is called the ANOVA decomposition. These calculations are summarised in the ANOVA table. 3.5.3 For the voting data SST &lt;- sum((hibbs$vote - mean(hibbs$vote))^2) SSR &lt;- sum((predict(lin_mod) - mean(hibbs$vote))^2) SSE &lt;- sum(residuals(lin_mod)^2) SST ## [1] 471.905 SSR ## [1] 273.6323 SSE ## [1] 198.2727 SSR/SST ## [1] 0.5798462 3.5.4 ANOVA table In general, an ANOVA table is a method for partitioning variability in a response variable into what is explained by the model fitted and what is left over. The exact form of the ANOVA table will depend on the model that has been fitted. The hypothesis being tested by the model will also depend on the model that has been fitted. An ANOVA table for the simple linear regression model: SOURCE df SS MS F Regression 1 SSR MSR = SSR/1 MSR/MSE Error n-2 SSE MSE = SSE/(n-2) Total n-1 SST NOTE: The total sum of squares, \\(\\mbox{SST} = \\sum_{i=1}^{n}(y_{i} - \\bar{y})^{2}\\) is the sum of squares of \\(y\\) about the mean. The total sum of squares does not depend on \\(x\\). (NB: this is \\(S_{yy}\\)) The regression sum of squares, \\(\\mbox{SSR} = \\sum_{i=1}^{n}(\\hat{y}_{i} - \\bar{y})^{2}\\). Note that \\(\\hat{y}_{i}\\) depends on \\(x\\). The residual/error sum of squares, \\(\\mbox{SSE} = \\sum_{i=1}^{n}e_{i}^{2} = \\sum_{i=1}^{n}(y_{i} - \\hat{y}_{i})^{2}\\). \\(\\mbox{SST} = \\mbox{SSR} + \\mbox{SSE}\\). The sums of squares have associated degrees of freedom (df). MS = SS/df The mean squared error \\(\\mbox{MSE}\\) estimates \\(\\sigma^{2}\\). The coefficient of determination is: \\[R^{2} = \\frac{\\mbox{SSR}}{\\mbox{SST}} = 1 - \\frac{\\mbox{SSE}}{\\mbox{SST}}.\\] \\(R^{2}\\) is always between 0 and 1 and it measures the proportion of variation in \\(y\\) that is explained by regression with \\(x\\). summary(lin_mod) ## ## Call: ## lm(formula = vote ~ growth, data = hibbs) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.9929 -0.6674 0.2556 2.3225 5.3094 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 46.2476 1.6219 28.514 8.41e-14 *** ## growth 3.0605 0.6963 4.396 0.00061 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.763 on 14 degrees of freedom ## Multiple R-squared: 0.5798, Adjusted R-squared: 0.5498 ## F-statistic: 19.32 on 1 and 14 DF, p-value: 0.00061 3.5.5 Special cases \\(R^{2}=1\\), \\(\\mbox{SSR} = \\mbox{SST}\\), \\(\\mbox{SSE}\\) = 0. \\(e_{i}=0\\), \\(i = 1, \\dots,n\\), data fall on a straight line. \\(R^{2}=0\\), \\(\\mbox{SSR} = 0\\), \\(\\mbox{SSE} = \\mbox{SST}\\). \\(\\hat{y}_{i}=\\bar{y}\\), \\(\\hat{\\beta}_1=0\\). 3.5.6 Does regression on x explain y? In simple linear regression this amounts to testing: \\[H_0: \\beta_1 = 0\\] \\[H_A: \\beta_1 \\ne 0\\] We can use a t-test for this, but there is an equivalent test based on the F distribution. As we will see later, F-tests have a wide range of applications. If \\(H_0\\) holds, then \\(\\mbox{SSR}\\) is small and \\(\\mbox{SSE}\\) large. Therefore large values of \\(\\mbox{SSR}\\) relative to \\(\\mbox{SSE}\\) provide evidence against \\(H_0\\). The F-statistic is: \\[F=\\frac{\\mbox{SSR}/df_R}{\\mbox{SSE}/df_E}.\\] where \\(df_R=1\\) is the degrees of freedom of \\(\\mbox{SSR}\\) and \\(df_E=n-2\\) is the degrees of freedom of \\(\\mbox{SSE}\\). Under \\(H_0\\),\\(F \\sim F_{1,n-2}\\). By dividing each SS by the \\(df\\) we put them on a common scale, so that if \\(H_0\\) is true: \\[\\mbox{SSR}/1 \\approx \\mbox{SSE}/(n-2)\\] and \\[F_{obs}\\approx 1.\\] Large values of \\(F_{obs}\\) provide evidence against \\(H_0\\). P-value: \\(P( F_{1,n-2} \\geq F_{obs})\\). 3.5.7 Notes on the ANOVA table (not examinable) \\(\\mathbb{E}[\\mbox{MSR}] = \\sigma^2 + \\beta_1^2 S_{xx}\\). Proof: \\[\\begin{align*} \\mbox{MSR} &amp; = \\sum_{i = 1}^n (\\hat{y}_i - \\bar{y})^2/1\\\\ &amp; = \\sum_{i = 1}^n (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i - \\bar{y})^2\\\\ &amp; = \\sum_{i = 1}^n (\\bar{y} - \\hat{\\beta}_1 \\bar{x} + \\hat{\\beta}_1x_i - \\bar{y})^2\\\\ &amp; =\\hat{\\beta}_1^2\\sum_{i = 1}^n (x_i - \\bar{x})^2\\\\ &amp; =\\hat{\\beta}_1^2 S_{xx} \\end{align*}\\] \\[\\begin{align*} \\mathbb{E}[\\mbox{MSR}] &amp; = \\mathbb{E}[\\hat{\\beta}_1^2 S_{xx}]\\\\ &amp; =S_{xx} \\mathbb{E}[\\hat{\\beta}_1^2]\\\\ &amp; = S_{xx} \\left(\\mbox{Var}(\\hat{\\beta}_1) + \\mathbb{E}[\\hat{\\beta}_1]^2 \\right)\\\\ &amp; = S_{xx} \\left(\\frac{\\sigma^2}{S_{xx}} + \\beta_1^2 \\right)\\\\ &amp; = \\sigma^2 + \\beta_1^2 S_{xx} \\end{align*}\\] \\(\\mathbb{E}[\\mbox{MSE}] = \\sigma^2\\). Proof: \\[\\begin{align*} \\mbox{MSE} &amp; = \\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n-2}\\\\ &amp; = \\frac{\\sum_{i = 1}^ne_i^2}{n-2} \\end{align*}\\] \\[\\begin{align*} \\mathbb{E}[\\mbox{MSE}] &amp; = \\frac{1}{n-2} \\mathbb{E}\\left[\\sum_{i = 1}^ne_i^2 \\right]\\\\ &amp; = \\frac{1}{n-2} \\sum_{i = 1}^n\\mathbb{E}[e_i^2]\\\\ &amp; = \\frac{1}{n-2} \\sum_{i = 1}^n\\left( \\mbox{Var}(e_i) + \\mathbb{E}[e_i]^2 \\right) \\end{align*}\\] NOTE: \\(\\mathbb{E}[\\epsilon_i] = 0\\), \\(\\mbox{Var}(\\epsilon_i) = \\sigma^2\\), but \\(\\mathbb{E}[e_i] = 0\\), \\(\\mbox{Var}(e_i)= \\sigma^2(1- h_{ii})\\). We will revisit this later in the course. \\[\\begin{align*} \\mathbb{E}[\\mbox{MSE}] &amp; = \\frac{1}{n-2} \\sum_{i = 1}^n \\left( \\sigma^2( 1 - h_{ii}) + 0 \\right)\\\\ &amp; =\\frac{1}{n-2} \\sum_{i = 1}^n \\sigma^2 \\left(1-\\left(\\frac{1}{n} +\\frac{(x_i - \\bar{x})^2}{S_{xx}}\\right)\\right)\\\\ &amp; =\\frac{1}{n-2} \\sum_{i = 1}^n \\left(\\sigma^2 - \\frac{\\sigma^2}{n} -\\frac{\\sigma^2 (x_i - \\bar{x})^2}{S_{xx}}\\right)\\\\ &amp; =\\frac{1}{n-2} \\left(\\sigma^2 n- \\sigma^2 -\\frac{\\sigma^2 }{S_{xx}}\\sum_{i = 1}^n(x_i - \\bar{x})^2\\right)\\\\ &amp; = \\frac{1}{n-2} \\left((n-2) \\sigma^2\\right)\\\\ &amp; =\\sigma^2 \\end{align*}\\] Under the \\(H_0\\), \\(\\beta_1 = 0\\) and then \\(\\mathbb{E}[\\mbox{MSE}] = \\mathbb{E}[\\mbox{MSR}]\\). \\(\\mbox{MSE} = \\hat{\\sigma}^2\\) can be computed using the formula: \\[\\hat{\\sigma}^2 = \\frac{S_{yy} - \\hat{\\beta}_1^2S_{xx}}{n-2}.\\] 3.6 Sample correlation The relationship between \\(X\\) and \\(Y\\) can be examined using a scatterplot \\((x_i, y_i)\\). Sample correlation measures the strength and direction of the linear association between \\(X\\) and \\(Y\\). It is defined as: \\[r = \\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}}\\] \\(r\\) is the estimate of the population correlation (\\(\\rho\\)) between \\(X\\) and \\(Y\\). cor(hibbs$vote, hibbs$growth) ## [1] 0.7614763 cor(hibbs$vote, hibbs$growth)^2 ## [1] 0.5798462 3.6.0.1 Connection between correlation and regression: \\(\\hat{\\beta}_1=\\sqrt{\\frac{\\mbox{SST}}{S_{xx}}}r=\\frac{s_y}{s_x}r\\) where \\(s_y\\) and \\(s_x\\) are the standard deviations of the \\(y_i\\)’s and \\(x_i\\)’s. Note that if you change the role of \\(X\\) and \\(Y\\) the resulting regression line would have slope \\(\\frac{s_x}{s_y}r\\). \\(r^2 =R^2\\) the coefficient of determination. In the SLR model variable \\(x\\) is treated as fixed and \\(y\\) and \\(\\epsilon\\) are random. It is convenient to think of the predictor variable as fixed even if it israndom as we are interested in the behaviour of \\(y\\) at various fixed \\(x\\) values. One can calculate \\(r\\) for any pair of variables (see next page), but correlation assumes that variables are bivariately normally distributed. Whereas correlation treats both variables symmetrically, in regression, the exploratory variable is used to explain or predict the response variable. Father and son heights (Galton data) 3.6.1 Examples of correlation Anscombe data. Can be found in Fox, Weisberg, and Price (2022). In all graphs below, correlation is \\(r = -0.06\\). 3.6.2 Comparison of the correlation and coefficient of determination for two data sets. cor(X1, Y1)^2 ## [1] 0.6699889 summary(lm(Y1 ~ X1))[8] ## $r.squared ## [1] 0.6699889 cor(X2, Y2)^2 ## [1] 0.6895371 summary(lm(Y2 ~ X2))[8] ## $r.squared ## [1] 0.6895371 3.7 Assessing the simple linear regression model assumptions 3.7.1 Assumptions In the SLR model, we assume that \\(y_{i} \\sim\\) N(\\(\\beta_{0} + \\beta_{1}x_{i}, \\sigma^{2}\\)) and that the \\(y_{i}\\)’s are independent. Equivalently, since \\(\\epsilon_{i} = y_{i} - \\beta_{0} - \\beta_{1}x_{i}\\), the SLR model assumes that \\(\\epsilon_{i} \\sim\\) N(0, \\(\\sigma^{2}\\)) and the \\(\\epsilon_{i}\\)’s are independent and identically distributed. We want to check the following: There is a linear relationship, i.e. \\(\\mathbb{E}\\)[\\(y_{i}\\)] = \\(\\beta_{0} + \\beta_{1}x_{i}\\). If the data do not follow a linear relationship then the simple linear regression model is not appropriate. The \\(\\epsilon_{i}\\)’s have a constant variance, i.e. Var(\\(\\epsilon_{i}\\)) = \\(\\sigma^{2}\\) for all \\(i\\). If there is not constant variance, the line will summarise the data okay but the parameter estimate standard errors, estimates of \\(\\sigma\\) etc, are all based on incorrect assumptions. The \\(\\epsilon_{i}\\)’s are independent. The \\(\\epsilon_{i}\\)’s are normally distributed (with mean 0). 3.7.2 Violations and consequences Linearity: A straight linear relationship may be inadequate. A straight linear relationship may only be appropriate for most of the data. When linearity is violated least squares estimates can be biased and standard errors may be inaccurate. Constant variance: When the variance is not constant least squares estimate are unbiased but standard errors are inaccurate. Independence: When there is a lack of independence least squares estimates are unbiased but standard errors are seriously affected. Normality: Violations of normality do not have much impact on estimates and standard errors. Tests and C.I.’s are not usually seriously affected because of the C.L.T. 3.7.3 Graphical tools for assessment Plot of \\(y_i\\) versus \\(x_i\\). If satisfactory use simple linear regression. Sometimes the patterns in the plot of \\(y_i\\) versus \\(x_i\\) are difficult to detect because of the total variability of the response variable is much larger than the variability around the regression line. Scatterplots of residuals vs fits are better at finding patterns because the linear component of the variation in the responses has been removed, leaving a clearer picture about curvature and spread. The plot alerts the user of nonlinearity, non-constant variance and the presence of outliers. Plot of \\(e_i\\) versus \\(\\hat{y}_i\\) (or \\(x_i\\)). If satisfactory use simple linear regression. If linearity is violated but \\(\\mathbb{E}[y]\\) is monotonic in \\(x\\) and \\(\\mbox{Var}(y)\\) is constant, try transforming \\(x\\) and then use simple linear regression. If linearity is violated and \\(\\mathbb{E}[y]\\) is not monotonic, try quadratic regression \\(\\mathbb{E}[y] = \\beta_0+\\beta_1 x+\\beta_2 x^2\\) (we will look at this later). If linearity is violated and \\(\\mbox{Var}(y)\\) increases with \\(\\mathbb{E}[y]\\), try transforming y and then use simple linear regression. If the distribution of \\(y\\) about \\(\\mathbb{E}[y]\\) is skewed, i.e. non-normal, then use simple linear regression but report skewness. If linearity is not violated but \\(\\mbox{Var}(y)\\) increases with \\(\\mathbb{E}[y]\\), use weighted regression (we will look at this later). plot(lin_mod,1) Normal probability plot The model assumes normality of \\(y\\) about \\(\\mathbb{E}[y]\\), or, equivalently, normality of \\(\\epsilon\\). The residuals \\(e_i\\) approximate \\(\\epsilon\\) and should therefore have a normal distribution. The normal probability (quantile) plot is a plot of \\(z_i\\) versus \\(e_i\\), where \\(z_i\\) are quantiles from the standard normal distribution. This plot should roughly follow a straight line pattern. plot(lin_mod,2) Residuals vs. time order If the data are collected over time, serial correlation or a general time trend may occur. A plot of \\(e_i\\) vs. \\(t_i\\) (time of the i\\(^{th}\\) observation) may be examined for patterns. Everytime you use SLR you should also draw graphs 1) to 3). Also plot 4) when appropriate. 3.7.4 Cigarette Data FDA data on cigarettes, response is carbon monoxide, predictor is nicotine. Data from McIntyre (1994). summary(fit) ## ## Call: ## lm(formula = carbon.monoxide ~ nicotine) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.3273 -1.2228 0.2304 1.2700 3.9357 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.6647 0.9936 1.675 0.107 ## nicotine 12.3954 1.0542 11.759 3.31e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.828 on 23 degrees of freedom ## Multiple R-squared: 0.8574, Adjusted R-squared: 0.8512 ## F-statistic: 138.3 on 1 and 23 DF, p-value: 3.312e-11 # studres(fit) To assess the fit construct residual plots Plot 1: residuals increasing with the fit, non constant variance. Plot 2: no indication that the assumption of Normality is unreasonable. There are 3 unusual observations: 3, 19, 25. Obs 3 has a large negative residual. It is the point on the upper right of the fitted line plot. It is a high influence point, meaning it has a big effect on the fitted line obtained. Obs 19 and 25 have large positive residuals. We could attempt to improve the fit, refit the model without observation 3. Diagnostic plots: Plot 1: no linear pattern, but some hint of non-constant variance. 3.7.4.1 Transformations: How can we pick the best transformation? Examine the fitted line plot: linearity, constant variance. Examine the residual vs fit plot: no relationship, constant variance, no outliers. Check the normality of residuals. Check for sensitivity: whether fit would change substantially if extreme points are removed. One can also compare \\(R^2\\) as long as the \\(y\\) values are on the same scale. Furthermore, \\(R^2\\) doesn’t follow a distribution, so we can’t compare \\(R^2\\) in two models and know that one is meaningfully better. Note: interpretation changes after transformations. Row 1: delete outlier Row 2: use a square root transformation for the predictor. This diminishes the influence of the outlier. The residual plot hints at a small amount of bias. Row 3: take square root transformations of both the response and the predictor. Row 4: take log transformations of both the response and the predictor. 3.7.5 Another Example: Diamonds Dataset from Wickham (2016). diamonds |&gt; glimpse() ## Rows: 53,940 ## Columns: 10 ## $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.… ## $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver… ## $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,… ## $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, … ## $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64… ## $ table &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58… ## $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34… ## $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.… ## $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.… ## $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.… diamonds |&gt; ggplot(aes(x = carat, y = price)) + geom_point() SLR model does not seem to be a good option! Let’s fit it anyway and explore the model diagnostics: diamond_mod &lt;- lm(price~carat, diamonds) summary(diamond_mod) ## ## Call: ## lm(formula = price ~ carat, data = diamonds) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18585.3 -804.8 -18.9 537.4 12731.7 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2256.36 13.06 -172.8 &lt;2e-16 *** ## carat 7756.43 14.07 551.4 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1549 on 53938 degrees of freedom ## Multiple R-squared: 0.8493, Adjusted R-squared: 0.8493 ## F-statistic: 3.041e+05 on 1 and 53938 DF, p-value: &lt; 2.2e-16 plot(diamond_mod, 1) plot(diamond_mod, 2) Try a transformation: diamond_mod2 &lt;- lm(log(price)~log(carat), diamonds) plot(diamond_mod2, 1) plot(diamond_mod2, 2) 3.8 A note on the Galton paradox 3.8.1 The Galton paradox Sons are on average taller than their fathers (by 1 inch approx) apply(father.son, 2, mean) ## father son ## 67.68683 68.68423 Taller than average fathers have taller than average sons. Regression towards the mean: although the above is true, for these tall people, the son’s height was on average less than the father’s. The suggestion is that each generation would have offspring more near average than the previous generation and that over many generations the offspring would be of uniform heigth. However, the observations showed the sons as variable as the fathers. apply(father.son, 2, sd) ## father son ## 2.745827 2.816194 An apparent paradox? 3.8.2 Two regressions Regressing \\(y\\) on \\(x\\), treats \\(x\\) variable as fixed and only vertical distances are minimized. Howevever, regressing \\(x\\) on \\(y\\), i.e. trying to predict the fathers’ heights from their sons’ treats the sons’ heights \\(y\\) as fixed and the least squares criterion minimizes the horizontal distances. 3.8.3 Regression vs orthogonal regression References Fox, John, Sanford Weisberg, and Brad Price. 2022. carData: Companion to Applied Regression Data Sets. https://doi.org/10.32614/CRAN.package.carData. McIntyre, Lauren. 1994. “Using Cigarette Data for an Introduction to Multiple Regression.” Journal of Statistics Education 2 (1): null. https://doi.org/10.1080/10691898.1994.11910468. Wickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. http://ggplot2.org. "],["multiple-regression.html", "Chapter 4 Multiple regression 4.1 Introductory examples 4.2 Least squares estimation for multiple regression 4.3 Prediction from multiple linear regression model 4.4 Regression models in matrix notation: examples 4.5 The formal multiple regression model and properties 4.6 The hat matrix 4.7 ANOVA for multiple regression 4.8 1-way ANOVA model 4.9 One way ANOVA in regression notation 4.10 Confidence intervals and hypothesis tests for linear combinations of \\(\\boldsymbol{\\beta}\\)", " Chapter 4 Multiple regression 4.1 Introductory examples Setup: response variable \\(y\\), predictors \\(x_1\\), \\(x_2\\), …, \\(x_k\\). 4.1.1 Example 1: Fuel Use Example from Section 2. Information was recorded on fuel usage and average temperature (\\(^oF\\)) over the course of one week for eight office complexes of similar size. Data are from Bowerman and Schafer (1990). \\(y\\) = fuel use, \\(x_1\\) = temperature, \\(x_2\\) = chill index. Data: Temp Fuel Chill 28.0 12.4 18 28.0 11.7 14 32.5 12.4 24 39.0 10.8 22 45.9 9.4 8 57.8 9.5 16 58.1 8.0 1 62.5 7.5 0 We wish to use \\(x_1\\) and \\(x_2\\) to predict \\(y\\). This should give more accurate predictions than either \\(x_1\\) or \\(x_2\\) alone. A multiple linear regression model is: fuel use \\(\\approx\\) a linear function of temperature and chill index. More precisely: \\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon.\\] As before, \\(\\epsilon\\) is the unobserved error, \\(\\beta_0, \\beta_1, \\beta_2\\) are the unknown parameters. When \\(\\mathbb{E}[\\epsilon ] = 0\\) we have \\[\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2.\\] In SLR we can check model appropriateness by plotting \\(y\\) vs \\(x\\) and observing whether the points fall close to a line. Here we could construct a 3-d plot of \\(y\\), \\(x_1\\), \\(x_2\\) and points should fall close to a plane. For a given set of values of \\(x_1\\) and \\(x_2\\), say \\(x_1 = 45.9\\) and \\(x_2 = 8\\), the model says that the mean fuel use is: \\[\\mathbb{E}[y] = \\beta_0 + \\beta_1 \\times 45.9 + \\beta_2 \\times 8.\\] If \\(x_1 = x_2 = 0\\) then \\(\\mathbb{E}[y] = \\beta_0\\), the model intercept. To interpret \\(\\beta_1\\) suppose \\(x_1 = t\\) and \\(x_2 = c\\). Then \\[\\mathbb{E}[y]=\\beta_0 + \\beta_1 \\times t + \\beta_2 \\times c.\\] Now suppose \\(x_1\\) increases by \\(1\\) and \\(x_2\\) stays fixed: \\[\\mathbb{E}[y]=\\beta_0 + \\beta_1 \\times (t + 1) + \\beta_2 \\times c.\\] Substracting these we find that \\(\\beta_1\\) is the increase in \\(\\mathbb{E}[y]\\) associated with 1 unit increase in \\(x_1\\) for a fixed \\(x_2\\). I.e. two weeks having the same chill index but whose temperature differed by \\(1^o\\) would have a mean fuel use difference of \\(\\beta_1\\). 4.1.2 Example 2: Categorical predictors Suppose we wish to predict the fuel efficiency of different car types. Data are from Cryer and Miller (1991). We have data on: \\(y\\) = gallons per mile (gpm), \\(x_1\\) = car weight (w), \\(x_2\\) = transmission type (ttype): 1 = automatic or 0 = manual. We use the model \\[\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2.\\] \\(\\beta_0\\) = the mean gpm for cars of weight \\(w = 0\\) and ttype = manual. \\(\\beta_1\\) = change in mean gpm when weight increases by 1 for the same ttype. \\(\\beta_2\\) = change in mean gpm when the car of the same weight is changed from manual to automatic. The model says that: \\[\\begin{align*} \\mathbb{E}[y] &amp; = \\beta_0 + \\beta_1 x_1 \\quad \\mbox{ for manual}\\\\ &amp; = \\beta_0 + \\beta_2 + \\beta_1 x_1 \\quad \\mbox{ for automatic.} \\end{align*}\\] Therefore we are fitting two lines with different intercepts but the same slope. The data should look like: Suppose the data look like this: This suggests we should fit two lines with different intercepts and different slopes. We introduce a third predictor: \\[\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1x_2,\\] giving: \\[\\begin{align*} \\mathbb{E}[y] &amp; = \\beta_0 + \\beta_1 x_1 \\quad \\mbox{ for manual}\\\\ &amp; = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) x_1 \\quad \\mbox{ for automatic.} \\end{align*}\\] The term \\(x_1x_2\\) is called an interaction term. Here: \\(\\beta_2\\) = difference in intercept \\(\\beta_3\\) = difference in slope. 4.1.3 Example 3: Polynomials We have one predictor \\(x\\) but the plot of \\(y\\) vs \\(x\\) exhibits a quadratic pattern. Then we can fit a multiple regression model: \\[\\mathbb{E}[y] = \\beta_0 + \\beta_1 x + \\beta_2 x^2.\\] This is also called a quadratic regression model or, more generally, a polynomial regression model. Higher order polynomial regression models can also be used if needed. Link: http://www.rpubs.com/kdomijan/333155 4.1.4 Example 4: Nonlinear relationships For example, \\[y = \\alpha x_1 ^{\\beta x_2} \\epsilon.\\] Nonlinear models can sometimes be linearized, for example: \\[log(y) = log(\\alpha) + \\beta x_2 log(x_1) + log(\\epsilon).\\] Here: \\(x = x_2 log(x_1)\\). NOTE: the term linear refers to the linearity of regression parameters. A general form for multiple linear regression model (with two explanatory variables): \\[y = \\beta_0 f_0(x_1, x_2) + \\beta_1 f_1(x_1, x_2) + \\beta_2 f_2(x_1, x_2) + \\dots\\] where \\(f_j(x_1, x_2)\\) are known functions of explanatory variables. The extension to more than two explanatory variables is straightforward. 4.1.5 Diamonds data diamonds |&gt; ggplot(aes(x = carat, y = log(price), col = color)) + geom_point() + facet_wrap(~clarity) # model.orig &lt;- lm(log(price) ~ poly(carat,2) + color + clarity +cut, diamonds) # summary(model.orig) 4.1.6 Cigarette Data continued Data from 3.7.4. Consider a second predictor (weight): Regression (nicotine only) summary(fit) ## ## Call: ## lm(formula = carbon.monoxide ~ nicotine) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.3273 -1.2228 0.2304 1.2700 3.9357 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.6647 0.9936 1.675 0.107 ## nicotine 12.3954 1.0542 11.759 3.31e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.828 on 23 degrees of freedom ## Multiple R-squared: 0.8574, Adjusted R-squared: 0.8512 ## F-statistic: 138.3 on 1 and 23 DF, p-value: 3.312e-11 Regression (weight only) summary(fit2) ## ## Call: ## lm(formula = carbon.monoxide ~ weight) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.524 -2.533 0.622 2.842 7.268 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -11.795 9.722 -1.213 0.2373 ## weight 25.068 9.980 2.512 0.0195 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.289 on 23 degrees of freedom ## Multiple R-squared: 0.2153, Adjusted R-squared: 0.1811 ## F-statistic: 6.309 on 1 and 23 DF, p-value: 0.01948 Regression (both predictors) fit3 &lt;- lm(carbon.monoxide ~ weight + nicotine) summary(fit3) ## ## Call: ## lm(formula = carbon.monoxide ~ weight + nicotine) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.3304 -1.2249 0.2314 1.2677 3.9371 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.61398 4.44663 0.363 0.720 ## weight 0.05883 5.02395 0.012 0.991 ## nicotine 12.38812 1.24473 9.952 1.32e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.87 on 22 degrees of freedom ## Multiple R-squared: 0.8574, Adjusted R-squared: 0.8444 ## F-statistic: 66.13 on 2 and 22 DF, p-value: 4.966e-10 Regression (quadratic) nicotine.sq &lt;- nicotine^2 fit4 &lt;- lm(carbon.monoxide ~ nicotine + nicotine.sq) summary(fit4) ## ## Call: ## lm(formula = carbon.monoxide ~ nicotine + nicotine.sq) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.9857 -1.1052 0.1834 0.8654 3.4145 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.784 1.453 -1.227 0.23264 ## nicotine 20.111 2.775 7.248 2.92e-07 *** ## nicotine.sq -3.730 1.267 -2.945 0.00749 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.583 on 22 degrees of freedom ## Multiple R-squared: 0.8977, Adjusted R-squared: 0.8884 ## F-statistic: 96.53 on 2 and 22 DF, p-value: 1.284e-11 4.2 Least squares estimation for multiple regression Our model states that: \\[y = \\beta_0 + \\beta_1x_{1} + \\beta_2x_{2} + ... + \\beta_kx_k + \\epsilon,\\] where \\(k&lt;n\\). For each observation we have: \\[y_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + ... + \\beta_kx_{ik} + \\epsilon_i.\\] We can write this more compactly using matrix notation. Let \\(\\mathbf{Y}\\) be the response vector: \\[\\mathbf{Y} =\\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots\\\\ y_{n} \\end{bmatrix}\\] Let \\(\\mathbf{X}\\) be the \\(n \\times p\\) matrix, where \\(p = k+1\\): \\[\\mathbf{X} = \\begin{bmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; x_{13} &amp; \\dots &amp; x_{1k} \\\\ 1 &amp; x_{21} &amp; x_{22} &amp; x_{23} &amp; \\dots &amp; x_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; x_{n3} &amp; \\dots &amp; x_{nk} \\end{bmatrix}\\] Let \\(\\boldsymbol{\\beta}\\) be the \\(p\\)-dim parameter vector: \\[\\boldsymbol{\\beta} =\\begin{bmatrix} \\beta_{0} \\\\ \\beta_{1} \\\\ \\vdots\\\\ \\beta_{k} \\end{bmatrix}\\] Let \\(\\boldsymbol{\\epsilon}\\) be the \\(n\\)-dim error vector: \\[\\boldsymbol{\\epsilon} =\\begin{bmatrix} \\epsilon_{1} \\\\ \\epsilon_{2} \\\\ \\vdots\\\\ \\epsilon_{n} \\end{bmatrix}\\] The model states that: \\[\\mathbf{Y}=\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}.\\] The vector of fitted values is: \\[\\hat{\\mathbf{Y}}=\\mathbf{X}\\hat{\\boldsymbol{\\beta}}.\\] The corresponding residual values are: \\[\\mathbf{e}=\\mathbf{Y}-\\hat{\\mathbf{Y}}.\\] The OLS estimates minimise: \\[S(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_{i1}- ... - \\beta_kx_{ik})^2\\] over \\(\\boldsymbol{\\beta}\\). Therefore the OLS estimates satisfy: \\[\\frac{\\delta S(\\boldsymbol{\\beta})}{\\delta \\beta_j} = 0, \\quad \\forall j\\] and as before we evaluate at \\(\\hat{\\boldsymbol{\\beta}}\\) \\[\\frac{\\delta S(\\boldsymbol{\\beta})}{\\delta \\beta_0} = -2 \\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_{i1}- ... - \\beta_kx_{ik})\\] \\[\\frac{\\delta S(\\boldsymbol{\\beta})}{\\delta \\beta_j} = -2 \\sum_{i=1}^{n} x_{ij}(y_i-\\beta_0-\\beta_1x_{i1}- ... - \\beta_kx_{ik}), \\quad \\forall j = 1,...,k.\\] The OLS estimates of \\(\\boldsymbol{\\beta}\\) satisfy: \\[\\sum_{i=1}^{n}(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1x_{i1}- ... - \\hat{\\beta}_kx_{ik}) = 0\\] and \\[\\sum_{i=1}^{n}(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1x_{i1}- ... - \\hat{\\beta}_kx_{ik})x_{ij} = 0, \\quad \\forall j = 1,...,k.\\] These normal equations (see (3.1) and (3.2)) can be written as: \\[\\sum_{i=1}^{n}e_i = 0\\] and \\[\\sum_{i=1}^{n}x_{ij}e_i = 0, \\quad \\forall j = 1,...,k.\\] We can combine this into one matrix equation: \\[\\mathbf{X}^T\\mathbf{e}= \\mathbf{0}\\] or equivalently: \\[\\mathbf{X}^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})= \\mathbf{0}\\] Therefore the OLS estimator \\(\\hat{\\boldsymbol{\\beta}}\\) satisfies: \\[\\begin{align} \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} &amp;= \\mathbf{X}^T\\mathbf{Y}\\\\ \\hat{\\boldsymbol{\\beta}} &amp;= (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{Y}.\\tag{4.1} \\end{align}\\] Matrix \\(\\mathbf{X}^T\\mathbf{X}\\) is non-singular (i.e has an inverse) iff \\(rank(\\mathbf{X}) =p\\), i.e. \\(\\mathbf{X}\\) has full rank and the columns of \\(\\mathbf{X}\\) are linearly independent. 4.2.1 Estimation of \\(\\sigma^2\\) = Var\\((\\epsilon)\\) A point estimate of \\(\\sigma^2\\) is the mean squared error: \\[\\hat{\\sigma}^2 = \\mbox{MSE} = \\frac{\\mbox{SSE}}{n-p} = \\frac{\\sum_{i=1}^n e_i^2}{n-p}.\\] 4.2.2 Estimation of Var\\((\\hat{\\beta})\\) \\[\\mbox{Var}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^T\\mathbf{X})^{-1} \\sigma^2.\\] \\[\\widehat{\\mbox{Var}(\\hat{\\boldsymbol{\\beta}})} = (\\mathbf{X}^T\\mathbf{X})^{-1} \\hat{\\sigma}^2.\\] 4.3 Prediction from multiple linear regression model As we have seen already, to predict from a multiple regression model we use: \\[\\hat{y}_i = \\hat{\\beta}_0+ \\hat{\\beta}_1x_{i1}+ \\cdots+\\hat{\\beta}_kx_{ik}\\] or \\[\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\] At a particular set of \\(x_0\\) values we predict the response \\(y_0\\) by: \\[\\hat{y}_0 = \\mathbf{x}_0^T\\hat{\\boldsymbol{\\beta}}\\] where \\(\\mathbf{x}_0^T = ( 1, x_{01},x_{02},..., x_{0k})\\). We also use \\(\\hat{y}_0\\) to estimate \\(\\mathbb{E}(y_0)\\), the mean of \\(y_0\\) at a given set of \\(x_0\\) values. The \\(\\mbox{S.E.}\\) for the estimate of the mean \\(\\mathbb{E}(y_0)\\) is: \\[\\mbox{S.E.}_{\\mbox{fit}} (\\hat{y}_0)= \\hat{\\sigma}\\sqrt{\\mathbf{x}_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_0}.\\] A \\(1-\\alpha\\) confidence interval for the expected response at \\(\\mathbf{x}_0\\) is given by: \\[\\hat{y}_0 \\pm t_{n-p}(\\alpha/2) \\times \\mbox{S.E.}_{\\mbox{fit}} (\\hat{y}_0).\\] The \\(\\mbox{S.E.}\\) for the predicted \\(y_0\\): \\[\\mbox{S.E.}_{\\mbox{pred}}(\\hat{y}_0) = \\hat{\\sigma}\\sqrt{1+ \\mathbf{x}_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_0}.\\] Note: \\[\\mbox{S.E.}_{\\mbox{pred}}(\\hat{y}_0)= \\sqrt{\\hat{\\sigma}^2+\\mbox{S.E.}_{\\mbox{fit}}(\\hat{y}_0)^2}\\] 4.4 Regression models in matrix notation: examples 4.4.1 Example 1: SLR The \\(\\mathbf{X}\\) matrix is: \\[\\mathbf{X} = \\begin{bmatrix} 1 &amp; x_{1}\\\\ \\vdots &amp; \\vdots\\\\ 1 &amp;x_{n} \\end{bmatrix}\\] To estimate the coefficients \\(\\hat{\\boldsymbol{\\beta}}\\): \\[\\begin{align*} \\mathbf{X}^T\\mathbf{X} &amp;= \\begin{bmatrix} n &amp; \\sum x_{i}\\\\ \\sum x_{i}&amp; \\sum x_{i}^2 \\end{bmatrix}\\\\ (\\mathbf{X}^T\\mathbf{X})^{-1} &amp; = \\frac{1}{n \\sum x_{i}^2 - (\\sum x_{i})^2}\\begin{bmatrix} \\sum x_{i}^2&amp; -\\sum x_{i}\\\\ -\\sum x_{i} &amp; n \\end{bmatrix} \\\\ &amp; = \\frac{1}{n (\\sum x_{i}^2 - n\\bar{x}^2)}\\begin{bmatrix} \\sum x_{i}^2 &amp; -n\\bar{x} \\\\ -n\\bar{x} &amp; n \\end{bmatrix} \\\\ &amp; = \\frac{1}{S_{xx}}\\begin{bmatrix} \\sum x_{i}^2/n &amp; -\\bar{x} \\\\ -\\bar{x} &amp; 1 \\end{bmatrix} \\\\ \\mathbf{X}^T\\mathbf{Y} &amp;= \\begin{bmatrix} \\sum y_{i} \\\\ \\sum x_{i}y_{i} \\end{bmatrix} = \\begin{bmatrix} n\\bar{y} \\\\ \\sum x_{i}y_{i} \\end{bmatrix}\\\\ \\hat{\\boldsymbol{\\beta}} &amp; = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\\\ &amp; = \\frac{1}{S_{xx}}\\begin{bmatrix} \\bar{y}\\sum x_{i}^2 -\\bar{x} \\sum x_{i}y_i \\\\ -n \\bar{x} \\bar{y} + \\sum x_{i}y_i \\end{bmatrix} \\end{align*}\\] With some algebra, this gives: \\[\\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}}\\] and \\[\\hat{\\beta}_0= \\bar{y} - \\hat{\\beta}_1\\bar{x}\\] as before, and \\[\\begin{align*} \\mbox{Var}(\\hat{\\boldsymbol{\\beta}})&amp; = (\\mathbf{X}^T\\mathbf{X})^{-1}\\sigma^2\\\\ &amp; = \\frac{\\sigma^2}{S_{xx}} \\begin{bmatrix} \\sum x_{i}^2/n&amp; -\\bar{x} \\\\ -\\bar{x}&amp; 1 \\end{bmatrix} \\end{align*}\\] which gives \\[\\mbox{Var}(\\hat{\\beta}_0) = \\sigma^2\\left(\\frac{1}{n}+ \\frac{\\bar{x}^2}{S_{xx}}\\right),\\] \\[\\mbox{Var}(\\hat{\\beta}_1) = \\frac{\\sigma^2}{S_{xx}},\\] \\[\\mbox{Cov}(\\hat{\\beta}_0, \\hat{\\beta}_1) = -\\bar{x}\\frac{\\sigma^2}{S_{xx}}.\\] 4.4.2 Example 2 Example from Stapleton (2009), Problem 3.1.1, pg 81. A scale has 2 pans. The measurement given by the scale is the difference between the weight in pan 1 and pan 2, plus a random error \\(\\epsilon\\). Suppose that \\(\\mathbb{E}[\\epsilon] = 0\\) and \\(\\mbox{Var}(\\epsilon) = \\sigma^2\\) and the \\(\\epsilon_i\\) are independent. Suppose also that two objects have weight \\(\\beta_1\\) and \\(\\beta_2\\) and that 4 measurements are taken: Pan 1: object 1, Pan 2: empty Pan 1: empty, Pan 2: object 2 Pan 1: object 1, Pan 2: object 2 Pan 1: object 1 and 2, Pan 2: empty Let \\(y_1\\), \\(y_2\\), \\(y_3\\) and \\(y_4\\) be the four observations. Then: \\[\\begin{align*} y_1 &amp; = \\beta_1 + \\epsilon_1\\\\ y_2 &amp; =- \\beta_2 + \\epsilon_2\\\\ y_3 &amp; = \\beta_1 - \\beta_2 + \\epsilon_3\\\\ y_4 &amp; = \\beta_1 + \\beta_2 + \\epsilon_4\\\\ \\end{align*}\\] \\(\\mathbf{X} = \\begin{bmatrix} 1 &amp;0 \\\\ 0 &amp; -1 \\\\ 1 &amp; -1 \\\\ 1 &amp; 1 \\end{bmatrix}\\) \\(\\mathbf{Y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\end{bmatrix}\\) \\(\\boldsymbol{\\beta} = \\begin{bmatrix} \\beta_1 \\\\ \\beta_2 \\end{bmatrix}\\) \\(\\boldsymbol{\\epsilon} = \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\end{bmatrix}\\) The model is: \\[\\mathbf{Y} = \\begin{bmatrix} 1 &amp;0 \\\\ 0 &amp; -1 \\\\ 1 &amp; -1 \\\\ 1 &amp; 1 \\end{bmatrix} \\times\\begin{bmatrix} \\beta_1 \\\\ \\beta_2 \\end{bmatrix} + \\boldsymbol{\\epsilon} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\] The OLS estimates of \\(\\boldsymbol{\\beta}\\) are given by: \\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{Y}.\\] \\[\\begin{align*} \\mathbf{X}^T\\mathbf{X} &amp; = \\begin{bmatrix} 1 &amp; 0 &amp; 1 &amp; 1\\\\ 0 &amp; -1 &amp; -1 &amp; 1\\\\ \\end{bmatrix} \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\\\ 1 &amp; -1 \\\\ 1 &amp; 1\\\\ \\end{bmatrix}\\\\ &amp; = \\begin{bmatrix} 3 &amp; 0 \\\\ 0 &amp; 3 \\\\ \\end{bmatrix}\\\\ &amp; = 3\\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{bmatrix}\\\\ \\mathbf{X}^T\\mathbf{Y} &amp;= \\begin{bmatrix} y_1 + y_3 + y_4\\\\ -y_2 - y_3 + y_4\\\\ \\end{bmatrix}\\\\ \\hat{\\boldsymbol{\\beta}} &amp; = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{Y}\\\\ &amp; = \\frac{1}{3}\\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} y_1 + y_3 + y_4\\\\ -y_2 - y_3 + y_4\\\\ \\end{bmatrix}\\\\ &amp; = \\frac{1}{3}\\begin{bmatrix} y_1 + y_3 + y_4\\\\ -y_2 - y_3 + y_4\\\\ \\end{bmatrix}\\\\ \\mbox{Var}(\\hat{\\boldsymbol{\\beta}}) &amp;= (\\mathbf{X}^T\\mathbf{X})^{-1} \\sigma^2 = \\frac{1}{3}\\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{bmatrix} \\sigma^2.\\\\ \\end{align*}\\] Can we improve the experiment so that the 4 measurements yield estimates of \\(\\boldsymbol{\\beta}\\) with smaller variance? present design: \\(\\mbox{Var}(\\hat{\\beta}_i) = \\frac{\\sigma^2}{3}\\) Let \\(\\mathbf{X} = \\begin{bmatrix} 1 &amp; -1 \\\\ 1 &amp; -1 \\\\ 1 &amp; 1 \\\\ 1 &amp; 1 \\end{bmatrix}\\), \\(\\mbox{Var}(\\hat{\\beta}_i) = \\frac{\\sigma^2}{4}\\) Let \\(\\mathbf{X} = \\begin{bmatrix} 1 &amp; 0 \\\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; 1 \\end{bmatrix}\\), \\(\\mbox{Var}(\\hat{\\beta}_i) = \\frac{\\sigma^2}{2}\\). 4.5 The formal multiple regression model and properties 4.5.1 Concepts: random vectors, covariance matrix, multivariate normal distribution (MVN). Let \\(Y_1,...,Y_n\\) be r.v.s defined on a common probability space. Then \\(\\mathbf{Y}\\) is a random vector. Let \\(\\mu_i = \\mathbb{E}[y_i]\\) and \\(\\boldsymbol{\\mu} = \\begin{bmatrix} \\mu_1 \\\\ \\vdots \\\\ \\mu_n \\end{bmatrix}\\). Then \\(\\boldsymbol{\\mu}\\) is a mean vector and we write: \\[\\mathbb{E}[\\mathbf{Y}] = \\boldsymbol{\\mu}.\\] Let \\(\\sigma_{ij} = Cov(y_i, y_j)\\). Then \\(\\boldsymbol{\\Sigma}\\) is the covariance matrix of \\(\\mathbf{Y}\\), where \\(\\boldsymbol{\\Sigma}_{ij} = [\\sigma_{ij}].\\) We write: \\[\\mbox{Var}(\\mathbf{Y}) = \\boldsymbol{\\Sigma}.\\] Aside: \\(\\mbox{Cov}(Y_i,Y_j) = \\mathbb{E}[(Y_i-\\mathbb{E}[Y_i])(Y_j-\\mathbb{E}[Y_j])]\\) \\(\\mbox{Cov}(Y_i,Y_i) = \\mbox{Var}(Y_i)\\) If \\(Y_i,Y_j\\) are independent then \\(\\mbox{Cov}(Y_i,Y_j) = 0\\). When \\(Y_i,Y_j\\) have bivariate normal distribution, if \\(\\mbox{Cov}(Y_i,Y_j) = 0\\), then \\(Y_i,Y_j\\) are independent. Fact: Suppose \\(\\mathbf{Y}\\) has mean \\(\\boldsymbol{\\mu}\\) and variance \\(\\boldsymbol{\\Sigma}\\). Then for a vector of constants \\(\\mathbf{b}\\) and matrix of constants \\(\\mathbf{C}\\): \\[\\mathbb{E}[\\mathbf{C}\\mathbf{Y} + \\mathbf{b}] = \\mathbf{C}\\boldsymbol{\\mu} + \\mathbf{b}\\] and \\[\\mbox{Var}( \\mathbf{C}\\mathbf{Y} + \\mathbf{b} ) = \\mathbf{C}\\boldsymbol{\\Sigma} \\mathbf{C}^T.\\] Defn: A random \\(n\\) - dim vector \\(\\mathbf{Y}\\) is said to have a MVN distribution if \\(\\mathbf{Y}\\) can be written as \\[\\mathbf{Y} = \\mathbf{A}\\mathbf{Z} + \\boldsymbol{\\mu}\\] where: \\(Z_1, Z_2, ..., Z_p\\) are iid N(0,1), \\(\\mathbf{A}\\) is an \\(n \\times p\\) matrix of constants and \\(\\boldsymbol{\\mu}\\) is an \\(n\\) vector of constants. Notes: Random vector \\(\\mathbf{Z}\\) is multivariate normal with mean \\(\\mathbf{0}\\) and covariance \\(\\mathbf{I}_p\\) since \\(Z_i\\)s are independent so their covariances are 0. We write: \\[\\mathbf{Z} \\sim N_p(\\mathbf{0}, \\mathbf{I}_p).\\] \\(\\mathbb{E}[\\mathbf{Y}] = \\mathbb{E}[\\mathbf{A}\\mathbf{Z} + \\boldsymbol{\\mu}] = \\boldsymbol{\\mu}\\), \\(\\mbox{Var}(\\mathbf{Y}) = \\mathbf{A}\\mathbf{A}^T\\), \\(\\mathbf{Y} \\sim N_n (\\boldsymbol{\\mu}, \\mathbf{A}\\mathbf{A}^T)\\). 4.5.2 Multiple regression model \\[\\mathbf{Y}=\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}.\\] \\(\\mathbf{Y}=\\) \\(n\\) - dimensional response random vector. \\(\\boldsymbol{\\beta}=\\) unknown \\(p\\) - dimensional parameter vector. \\(\\mathbf{X}=\\) an \\(n \\times p\\) matrix of constants. \\(\\boldsymbol{\\epsilon}=\\) \\(n\\) - dimensional error vector. Assumptions: Linearity: \\(\\mathbb{E}[\\boldsymbol{\\epsilon} ] =\\mathbf{0}\\), hence \\(\\mathbb{E}[\\mathbf{Y}] = \\mathbf{X}\\boldsymbol{\\beta}\\). Constant variance and 0 covariances \\(\\mbox{Var}(\\boldsymbol{\\epsilon}) = \\sigma^2 I_n\\) and \\(\\mbox{Var}(\\mathbf{Y}) = \\sigma^2 I_n\\). MVN distribution: \\(\\boldsymbol{\\epsilon} \\sim N_n(\\mathbf{0},\\sigma^2 I_n )\\) and \\(\\mathbf{Y} \\sim N_n(\\mathbf{X}\\boldsymbol{\\beta},\\sigma^2 I_n )\\) Notes: When the off diagonal entries of the covariance matrix of a MVN distribution are 0, the \\(Y_1, ..., Y_n\\) are independent. Theorem 4.1 Let \\(\\hat{\\boldsymbol{\\beta}}\\) be the OLS estimator of \\(\\boldsymbol{\\beta}\\). When the model assumptions hold: \\[\\hat{\\boldsymbol{\\beta}} \\sim N_p(\\boldsymbol{\\beta}, (\\mathbf{X}^T\\mathbf{X})^{-1}\\sigma^2)\\] Corollary: \\(\\hat{\\beta}_j \\sim N(\\beta_j, c_{jj}\\sigma^2)\\), where \\(c_{jj}\\) is the \\(jj\\) entry of \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\) for \\(j = 0, ..., k\\). Theorem 4.2 Let \\(\\hat{\\sigma}^2 = \\frac{\\mbox{SSE}}{n-p}\\). When the model assumptions hold: \\[(n-p)\\frac{\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi ^2_{(n-p)}\\] and \\[\\mathbb{E}[\\hat{\\sigma}^2] =\\sigma^2\\] The distribution of \\(\\hat{\\sigma}^2\\) is independent of \\(\\hat{\\boldsymbol{\\beta}}\\). Corollary: \\[\\frac{\\hat{\\beta}_j - \\beta_j }{\\hat{\\sigma} \\sqrt{c_{jj}}} \\sim t_{n-p}\\] So we can do tests and obtain CIs for \\(\\beta_j\\) 4.6 The hat matrix The vector of fitted values: \\[\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{Y} = \\mathbf{H}\\mathbf{Y}.\\] The hat matrix (also known as the projection matrix): \\[\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\] has dimension \\(n \\times n\\) is symmetric (\\(\\mathbf{H}^T = \\mathbf{H}\\)) and is idempotent (\\(\\mathbf{H}^2 = \\mathbf{H}\\mathbf{H} = \\mathbf{H}\\)). We have \\[\\hat{\\mathbf{Y}}= \\mathbf{H}\\mathbf{Y}\\] \\[\\mathbf{e}= \\mathbf{Y} - \\hat{\\mathbf{Y}} =\\mathbf{Y} - \\mathbf{H}\\mathbf{Y} = (\\mathbf{I} - \\mathbf{H})\\mathbf{Y}\\] \\[\\mbox{SSE} = \\mathbf{e}^T\\mathbf{e} = \\mathbf{Y}^T (\\mathbf{I} - \\mathbf{H})\\mathbf{Y}\\] 4.6.1 The QR Decomposition of a matrix We have seen that OLS estimates for \\(\\boldsymbol{\\beta}\\) can be found by using: \\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{Y}.\\] Inverting the \\(\\mathbf{X}^T\\mathbf{X}\\) matrix can sometimes introduce significant rounding errors into the calculations and most software packages use QR decomposition of the design matrix \\(\\mathbf{X}\\) to compute the parameter estimates. E.g. take a look at the documentation for the lm method in R. How does this work? We need to find an \\(n \\times p\\) matrix \\(\\mathbf{Q}\\) and a \\(p \\times p\\) matrix \\(\\mathbf{R}\\) such that: \\[\\mathbf{X}=\\mathbf{Q}\\mathbf{R}\\] and \\(\\mathbf{Q}\\) has orthonormal columns, i.e. \\(\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{I}_p\\) \\(\\mathbf{R}\\) is an upper triangular matrix. There are several methods for computing the \\(\\mathbf{Q}\\mathbf{R}\\) factorization (we won’t study them, but high quality code for the computation exists in publicly available Lapack package {http://www.netlib.org/lapack/lug/}). We can show that: \\[\\begin{align*} \\mathbf{X} &amp;=\\mathbf{Q}\\mathbf{R} \\\\ \\mathbf{X}^T\\mathbf{X} &amp;=(\\mathbf{Q}\\mathbf{R})^T(\\mathbf{Q}\\mathbf{R}) = \\mathbf{R}^T\\mathbf{R}\\\\ \\end{align*}\\] Then: \\[\\begin{align*} (\\mathbf{X}^T\\mathbf{X})\\hat{\\boldsymbol{\\beta}} &amp; =\\mathbf{X}^T\\mathbf{Y}\\\\ (\\mathbf{R}^T\\mathbf{R})\\hat{\\boldsymbol{\\beta}} &amp; =\\mathbf{R}^T\\mathbf{Q}^T\\mathbf{Y}\\\\ \\mathbf{R}\\hat{\\boldsymbol{\\beta}} &amp; = \\mathbf{Q}^T\\mathbf{Y}\\\\ \\end{align*}\\] Since \\(\\mathbf{R}\\) is a triangular matrix we can use backsolving and this is an easy equation to solve. We can also show that the hat matrix becomes: \\[\\mathbf{H} = \\mathbf{Q}\\mathbf{Q}^T\\] 4.7 ANOVA for multiple regression Recap: ANOVA decomposition \\[\\begin{align*} \\mbox{SSR} &amp; = \\sum_{i=1}^n(\\hat{y}_i - \\bar{y})^2 = \\sum_{i=1}^n \\hat{y}_i ^2- n\\bar{y}^2 \\\\ \\mbox{SSE} &amp; = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n e_i^2\\\\ \\mbox{SST} &amp; = \\sum_{i=1}^n(y_i - \\bar{y})^2 = \\sum_{i=1}^n y_i ^2- n\\bar{y}^2 \\\\ \\end{align*}\\] Theorem 4.3 \\(\\mbox{SST} = \\mbox{SSR} + \\mbox{SSE}\\) Proof: this follows from the decomposition of response = fit + residual. \\[\\begin{align*} \\mathbf{Y} &amp; = \\hat{\\mathbf{Y}} + \\mathbf{e}\\\\ \\mathbf{Y}^T\\mathbf{Y} &amp; = (\\hat{\\mathbf{Y}} + \\mathbf{e})^T (\\hat{\\mathbf{Y}} + \\mathbf{e})\\\\ &amp; = \\hat{\\mathbf{Y}}^T\\hat{\\mathbf{Y}} + \\mathbf{e}^T\\mathbf{e}+ 2\\hat{\\mathbf{Y}}^T\\mathbf{e} \\\\ \\end{align*}\\] But \\(\\hat{\\mathbf{Y}}^T = \\hat{\\boldsymbol{\\beta}}^T\\mathbf{X}^T\\) and \\(\\mathbf{X}^T\\mathbf{e} = 0\\), from normal equations, so \\(\\hat{\\mathbf{Y}}^T\\mathbf{e} = 0\\). Alternatively: \\(\\hat{\\mathbf{Y}} = \\mathbf{H}\\mathbf{Y}\\), \\(\\mathbf{e} = (\\mathbf{I} - \\mathbf{H})\\mathbf{Y}\\), so \\(\\hat{\\mathbf{Y}}^T\\mathbf{e} = \\mathbf{Y}^T\\mathbf{H}(\\mathbf{I} - \\mathbf{H})\\mathbf{Y} = 0\\), since \\(\\mathbf{H}^2=\\mathbf{H}\\). Therefore, \\[\\mathbf{Y}^T\\mathbf{Y} = \\hat{\\mathbf{Y}}^T\\hat{\\mathbf{Y}} + \\mathbf{e}^T\\mathbf{e}\\] \\[\\sum_{i=1}^n y_i^2 =\\sum_{i=1}^n \\hat{y}_i^2 + \\sum_{i=1}^n e_i^2\\] and substracting \\(n\\bar{y}^2\\) from both sides completes the proof. ANOVA table: \\[\\begin{align*} \\mbox{SSR} &amp; = \\sum_{i=1}^n(\\hat{y}_i - \\bar{y})^2, \\;\\;\\;\\; df =p-1 \\\\ \\mbox{SSE} &amp; = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2, \\;\\;\\;\\; df = n-p \\\\ \\mbox{SST} &amp; = \\sum_{i=1}^n(y_i - \\bar{y})^2, \\;\\;\\;\\; df =n-1. \\end{align*}\\] SOURCE df SS MS F Regression p-1 SSR MSR = SSR/(p-1) MSR/MSE Error n-p SSE MSE = SSE/(n-p) Total n-1 SST If \\(\\beta_1 = \\beta_2 = ... = \\beta_k = 0\\) then \\(\\hat{\\beta}_j \\approx 0\\) for \\(j = 1,...,k\\) and \\(\\hat{y}_i \\approx \\bar{y}\\). Then, \\(\\mbox{SSE} \\approx \\mbox{SST}\\) and \\(\\mbox{SSR} \\approx 0\\). Small values of \\(\\mbox{SSR}\\) relative to \\(\\mbox{SSE}\\) provide indication that \\(\\beta_1 = \\beta_2 = ... = \\beta_k = 0\\). \\[\\begin{align*} &amp;H_0: \\beta_1 = \\beta_2 = ... = \\beta_k = 0\\\\ &amp;H_A: \\mbox{ not all }\\beta_j = 0 \\mbox{ for }j = 1,...,k \\end{align*}\\] Under \\(H_0\\): \\[F= \\frac{\\mbox{SSR}/(p-1)}{\\mbox{SSE}/(n-p)} \\sim F_{(p-1, n-p)}\\] P-value is \\(P( F_{(p-1, n-p)} \\geq F_{obs})\\), where \\(F_{obs}\\) is the observed \\(F\\)-value. Coefficient of determination \\(R^2 = \\frac{\\mbox{SSR}}{\\mbox{SST}}\\), \\(0 \\leq R^2 \\leq 1\\). \\(R^2\\) is the proportion of variability in \\(Y\\) explained by regression on \\(X_1,...,X_k\\). Adjusted \\(R^2\\) is the modified version of \\(R^2\\) adjusted for the number of predictors in the model. R uses: \\[R^2_{Adj} = 1-(1- R^2)\\frac{n-1}{n-p-1}.\\] 4.8 1-way ANOVA model 4.8.1 Example: A study was carried out to examine the effects of caffeine. Thirty students were randomly assigned to one of: control, no caffeine low dose caffeine low dose caffeine plus sugar The response \\(y\\) is an index measuring unrest 2 hrs later. (Example from Draper and Smith (1966).) Let \\(y_{ij}\\) be the response for the \\(j^{th}\\) person in the \\(i^{th}\\) group, \\(j=1,...,10\\), \\(i=1,2,3\\). Let \\(n_i\\) be the number assigned to group \\(i\\). Model: \\(y_{ij} = \\mu_{i} + \\epsilon_{ij}\\), \\(\\quad\\) \\(\\epsilon_{ij}\\) iid \\(N(0, \\sigma^2)\\), where \\(\\mu_i\\) is the population mean for those at dose \\(i\\). Or equivalently: \\(y_{ij} = \\mu + \\alpha_{i} + \\epsilon_{ij}\\), \\(\\quad\\) \\(\\epsilon_{ij}\\) iid \\(N(0, \\sigma^2)\\), where \\(\\mu\\) is the overall population mean and \\(\\alpha_i\\) is the effect of receiving treatment \\(i\\). O.L.S estimates for Model 1 are: \\[\\begin{align*} S(\\mu_1, ..., \\mu_g)&amp; =\\sum_{i=1}^g\\sum_{j=1}^{n_i}\\epsilon_{ij}^2 = \\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\mu_i)^2,\\\\ \\frac{\\delta S(\\mu_1, ..., \\mu_g)}{\\delta \\mu_i}&amp; = -2\\sum_{j=1}^{n_i}(y_{ij}-\\mu_i), \\quad \\forall i = 1,...,g \\\\ \\end{align*}\\] Setting these equal to 0 and evaluating at \\(\\hat{\\mu}_i\\) gives: \\[\\begin{align*} \\sum_{j=1}^{n_i}(y_{ij}-\\hat{\\mu}_i) &amp; =0.\\\\ \\sum_{j=1}^{n_i}y_{ij}-n_i\\hat{\\mu}_i &amp; =0.\\\\ \\hat{\\mu}_i =\\sum_{j=1}^{n_i}y_{ij}/n_i &amp; =\\bar{y}_{i.}\\\\ \\end{align*}\\] NOTE: \\(\\bar{y}_{i.}\\) is the average of responses at level \\(i\\) of \\(X\\). Model 1 has \\(g=3\\) parameters but model 2 has 4 parameters and is over-parameterised (\\(\\mu_i = \\mu + \\alpha_{i}\\)). Usually the constraint \\(\\sum \\alpha_i = 0\\) or \\(\\alpha_3 = 0\\) is imposed. The hypothesis of interest in this model is: \\[\\begin{align*} &amp; H_0: \\mu_1=\\mu_2 = ...= \\mu_g\\\\ &amp; H_A: \\mbox{not all } \\mu_i \\mbox{ are the same.}\\\\ \\end{align*}\\] Equivalently: \\[\\begin{align*} &amp; H_0: \\alpha_i=0, \\hspace{1cm}\\forall i = 1,...,g\\\\ &amp; H_A: \\mbox{not all } \\alpha_i = 0.\\\\ \\end{align*}\\] Calculations can be summarised in the ANOVA table: SOURCE df SS MS F Group g-1 SSG MSG = SSG/(g-1) MSG/MSE Error n-g SSE MSE = SSE/(n-g) Total n-1 SST where: \\(\\mbox{SSG} = \\sum_{i = 1}^gn_{i}(\\bar{y}_{i.} - \\bar{y}_{..})^{2}\\) \\(\\mbox{SSE} = \\sum_{i=1}^g\\sum_{j = 1}^{n_i}(y_{ij} - \\bar{y}_{i.})^{2}\\) \\(\\mbox{SST} = \\sum_{i=1}^g\\sum_{j = 1}^{n_i}(y_{ij} - \\bar{y}_{..})^{2}\\) Under \\(H_{0}\\): \\[F_{obs} = \\frac{\\mbox{MSG}}{\\mbox{MSE}} \\sim F_{g-1, n-g}.\\] We reject \\(H_{0}\\) for large values of \\(F_{obs}\\). 4.9 One way ANOVA in regression notation First we have to set up dummy variables: \\[X_i= \\{ \\begin{array}{ll} 1 &amp; \\quad\\mbox{if observation in gp }i\\\\ 0 &amp; \\quad\\mbox{ow}\\end{array}\\] Model: (effects model \\(y_{ij} = \\mu + \\alpha_{i} + \\epsilon_{ij}\\)) \\[Y = \\mu + \\alpha_1X_1 + \\alpha_2X_2 + \\alpha_3X_3 + \\epsilon\\] \\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\] where \\(\\mathbf{Y}\\) is a \\(30 \\times 1\\) vector of responses and \\[\\boldsymbol{\\beta} = \\begin{bmatrix} \\mu \\\\ \\alpha_{1} \\\\ \\alpha_{2} \\\\ \\alpha_{3} \\end{bmatrix} \\quad \\quad \\quad \\mathbf{X} = \\begin{bmatrix} 1 &amp; 1 &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}\\] Note that in the \\(\\mathbf{X}\\) matrix the first column equals the sum of the second, third and fourth columns, therefore it is not of full rank so \\(\\mathbf{X}^T\\mathbf{X}\\) does not have an inverse and there is not unique \\(\\hat{\\boldsymbol{\\beta}}\\). We could require \\(\\sum\\alpha_i = 0\\) and then the solution would be unique. Or, we could require that \\(\\alpha_3 = 0\\) and drop the last column of \\(\\mathbf{X}\\). We could also derive a solution where the first column of \\(\\mathbf{X}\\) is omitted. Then the model becomes: \\[Y = \\mu_1X_1 + \\mu_2X_2 + \\mu_3X_3 + \\epsilon\\] \\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\] where \\[\\boldsymbol{\\beta} =\\begin{bmatrix} \\mu_{1} \\\\ \\mu_{2}\\\\ \\mu_{3} \\end{bmatrix}\\quad \\quad \\quad \\mathbf{X} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\] This is the means model \\(y_{ij} = \\mu_i + \\epsilon_{ij}\\). \\[\\begin{align*} \\hat{\\boldsymbol{\\beta}} &amp; = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\\\ &amp; = \\begin{bmatrix} 10 &amp; 0 &amp; 0 \\\\ 0 &amp; 10 &amp; 0 \\\\ 0 &amp; 0 &amp; 10 \\end{bmatrix}^{-1}\\begin{bmatrix} \\sum_{j=1}^{10} y_{1j} \\\\ \\sum_{j=1}^{10} y_{2j}\\\\ \\sum_{j=1}^{10} y_{3j} \\end{bmatrix}\\\\ &amp; = \\frac{1}{10}\\mathbf{I}_3\\begin{bmatrix} y_{1.} \\\\ y_{2.}\\\\ y_{3.} \\end{bmatrix}\\\\ &amp; = \\begin{bmatrix} \\bar{y}_{1.} \\\\ \\bar{y}_{2.}\\\\ \\bar{y}_{3.} \\end{bmatrix}\\\\ &amp; = \\begin{bmatrix} \\hat{\\mu}_{1} \\\\ \\hat{\\mu}_{2}\\\\ \\hat{\\mu}_{3} \\end{bmatrix} \\end{align*}\\] The fitted values are then: \\[\\hat{Y} = \\hat{\\mu}_1X_1 + \\hat{\\mu}_2X_2 + \\hat{\\mu}_3X_3\\] or \\[\\hat{Y} = \\hat{\\mu}_i = \\bar{y}_{i.}\\] if \\(Y\\) comes from group \\(i\\). 4.9.1 Fitting the model in R 4.9.1.1 Example: Caffeine in R coffee.data &lt;- read.csv(&quot;data/coffee.csv&quot;) coffee.data |&gt; glimpse() #Tell R this is a categorical variable coffee.data$x &lt;- as.factor(coffee.data$x) coffee.data |&gt; ggplot(aes(x = x, y = y)) + geom_boxplot() Using aov, \\(Y\\) is response, \\(X\\) is group. fit.oneway.anova &lt;- aov(y~x, data = coffee.data) summary(fit.oneway.anova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 2 61.4 30.700 6.181 0.00616 ** ## Residuals 27 134.1 4.967 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(fit.oneway.anova) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 2 61.4 30.7000 6.1812 0.006163 ** ## Residuals 27 134.1 4.9667 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 model.tables(fit.oneway.anova, &quot;means&quot;) ## Tables of means ## Grand mean ## ## 246.5 ## ## x ## x ## 1 2 3 ## 244.8 246.4 248.3 # plot(fit.oneway.anova) #diagnostic plots coefficients(fit.oneway.anova) ## (Intercept) x2 x3 ## 244.8 1.6 3.5 TukeyHSD(fit.oneway.anova, &quot;x&quot;, ordered = TRUE) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## factor levels have been ordered ## ## Fit: aov(formula = y ~ x, data = coffee.data) ## ## $x ## diff lwr upr p adj ## 2-1 1.6 -0.8711391 4.071139 0.2606999 ## 3-1 3.5 1.0288609 5.971139 0.0043753 ## 3-2 1.9 -0.5711391 4.371139 0.1562593 plot(TukeyHSD(fit.oneway.anova, &quot;x&quot;)) Using lm fit.coffee &lt;- lm(y~x, data = coffee.data) summary(fit.coffee) ## ## Call: ## lm(formula = y ~ x, data = coffee.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.400 -2.075 -0.300 1.675 3.700 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 244.8000 0.7047 347.359 &lt; 2e-16 *** ## x2 1.6000 0.9967 1.605 0.12005 ## x3 3.5000 0.9967 3.512 0.00158 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.229 on 27 degrees of freedom ## Multiple R-squared: 0.3141, Adjusted R-squared: 0.2633 ## F-statistic: 6.181 on 2 and 27 DF, p-value: 0.006163 anova(fit.coffee) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 2 61.4 30.7000 6.1812 0.006163 ** ## Residuals 27 134.1 4.9667 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 model.matrix(fit.coffee) ## (Intercept) x2 x3 ## 1 1 0 0 ## 2 1 0 0 ## 3 1 0 0 ## 4 1 0 0 ## 5 1 0 0 ## 6 1 0 0 ## 7 1 0 0 ## 8 1 0 0 ## 9 1 0 0 ## 10 1 0 0 ## 11 1 1 0 ## 12 1 1 0 ## 13 1 1 0 ## 14 1 1 0 ## 15 1 1 0 ## 16 1 1 0 ## 17 1 1 0 ## 18 1 1 0 ## 19 1 1 0 ## 20 1 1 0 ## 21 1 0 1 ## 22 1 0 1 ## 23 1 0 1 ## 24 1 0 1 ## 25 1 0 1 ## 26 1 0 1 ## 27 1 0 1 ## 28 1 0 1 ## 29 1 0 1 ## 30 1 0 1 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$x ## [1] &quot;contr.treatment&quot; \\(X_1\\) is dropped. Compare ANOVA table with the 1-way ANOVA results. Using lm with the constrain on the sum of effects equals 0. options(contrasts=c(&#39;contr.sum&#39;, &#39;contr.sum&#39;)) fit.coffee &lt;- lm(y~x, data = coffee.data) summary(fit.coffee) ## ## Call: ## lm(formula = y ~ x, data = coffee.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.400 -2.075 -0.300 1.675 3.700 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 246.5000 0.4069 605.822 &lt; 2e-16 *** ## x1 -1.7000 0.5754 -2.954 0.00642 ** ## x2 -0.1000 0.5754 -0.174 0.86333 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.229 on 27 degrees of freedom ## Multiple R-squared: 0.3141, Adjusted R-squared: 0.2633 ## F-statistic: 6.181 on 2 and 27 DF, p-value: 0.006163 anova(fit.coffee) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 2 61.4 30.7000 6.1812 0.006163 ** ## Residuals 27 134.1 4.9667 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 model.matrix(fit.coffee) ## (Intercept) x1 x2 ## 1 1 1 0 ## 2 1 1 0 ## 3 1 1 0 ## 4 1 1 0 ## 5 1 1 0 ## 6 1 1 0 ## 7 1 1 0 ## 8 1 1 0 ## 9 1 1 0 ## 10 1 1 0 ## 11 1 0 1 ## 12 1 0 1 ## 13 1 0 1 ## 14 1 0 1 ## 15 1 0 1 ## 16 1 0 1 ## 17 1 0 1 ## 18 1 0 1 ## 19 1 0 1 ## 20 1 0 1 ## 21 1 -1 -1 ## 22 1 -1 -1 ## 23 1 -1 -1 ## 24 1 -1 -1 ## 25 1 -1 -1 ## 26 1 -1 -1 ## 27 1 -1 -1 ## 28 1 -1 -1 ## 29 1 -1 -1 ## 30 1 -1 -1 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$x ## [1] &quot;contr.sum&quot; options(contrasts=c(&#39;contr.treatment&#39;, &#39;contr.treatment&#39;)) # to reset Using lm, without intercept (this is the means model): fit.coffee &lt;- lm(y ~ x - 1, data = coffee.data) summary(fit.coffee) ## ## Call: ## lm(formula = y ~ x - 1, data = coffee.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.400 -2.075 -0.300 1.675 3.700 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## x1 244.8000 0.7047 347.4 &lt;2e-16 *** ## x2 246.4000 0.7047 349.6 &lt;2e-16 *** ## x3 248.3000 0.7047 352.3 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.229 on 27 degrees of freedom ## Multiple R-squared: 0.9999, Adjusted R-squared: 0.9999 ## F-statistic: 1.223e+05 on 3 and 27 DF, p-value: &lt; 2.2e-16 anova(fit.coffee) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 3 1822929 607643 122344 &lt; 2.2e-16 *** ## Residuals 27 134 5 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 model.matrix(fit.coffee) ## x1 x2 x3 ## 1 1 0 0 ## 2 1 0 0 ## 3 1 0 0 ## 4 1 0 0 ## 5 1 0 0 ## 6 1 0 0 ## 7 1 0 0 ## 8 1 0 0 ## 9 1 0 0 ## 10 1 0 0 ## 11 0 1 0 ## 12 0 1 0 ## 13 0 1 0 ## 14 0 1 0 ## 15 0 1 0 ## 16 0 1 0 ## 17 0 1 0 ## 18 0 1 0 ## 19 0 1 0 ## 20 0 1 0 ## 21 0 0 1 ## 22 0 0 1 ## 23 0 0 1 ## 24 0 0 1 ## 25 0 0 1 ## 26 0 0 1 ## 27 0 0 1 ## 28 0 0 1 ## 29 0 0 1 ## 30 0 0 1 ## attr(,&quot;assign&quot;) ## [1] 1 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$x ## [1] &quot;contr.treatment&quot; 1), 2) and 3) give the same ANOVA table. In 4), when the model does not include an intercept, the ANOVA table shows the uncorrected SS, i.e. \\(\\bar{y}\\) not substracted: SOURCE df SS Regression p SSR Error n-p SSE Total n SST Where \\(SSR = \\sum \\hat{y}^2\\), \\(SSE = \\sum(y - \\hat{y})^{2}\\) and \\(SST = \\sum y^{2}\\). It is also possible to fit the model by explicitly specifying the dummy variables. # or using dummy variables d1 &lt;- as.numeric(coffee.data$x == 1) d2 &lt;- as.numeric(coffee.data$x == 2) d3 &lt;- as.numeric(coffee.data$x == 3) fit.coffee.dummy &lt;- lm(coffee.data$y ~ d1 +d2) summary(fit.coffee.dummy) ## ## Call: ## lm(formula = coffee.data$y ~ d1 + d2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.400 -2.075 -0.300 1.675 3.700 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 248.3000 0.7047 352.326 &lt; 2e-16 *** ## d1 -3.5000 0.9967 -3.512 0.00158 ** ## d2 -1.9000 0.9967 -1.906 0.06730 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.229 on 27 degrees of freedom ## Multiple R-squared: 0.3141, Adjusted R-squared: 0.2633 ## F-statistic: 6.181 on 2 and 27 DF, p-value: 0.006163 #no intercept fit.coffee.dummy2 &lt;- lm(coffee.data$y ~ d1 +d2 +d3 - 1) summary(fit.coffee.dummy2) ## ## Call: ## lm(formula = coffee.data$y ~ d1 + d2 + d3 - 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.400 -2.075 -0.300 1.675 3.700 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## d1 244.8000 0.7047 347.4 &lt;2e-16 *** ## d2 246.4000 0.7047 349.6 &lt;2e-16 *** ## d3 248.3000 0.7047 352.3 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.229 on 27 degrees of freedom ## Multiple R-squared: 0.9999, Adjusted R-squared: 0.9999 ## F-statistic: 1.223e+05 on 3 and 27 DF, p-value: &lt; 2.2e-16 4.10 Confidence intervals and hypothesis tests for linear combinations of \\(\\boldsymbol{\\beta}\\) From the theory of OLS: \\[\\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta},(\\mathbf{X}^T\\mathbf{X})^{-1}\\sigma^2)\\] and \\[\\mathbf{c}^T\\hat{\\boldsymbol{\\beta}} \\sim N(\\mathbf{c}^T\\boldsymbol{\\beta},\\mathbf{c}^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{c}\\sigma^2)\\] For the caffeine example (1 way ANOVA model): suppose we want to compare the treatment means with the control mean, that is, we want a CI for: \\[\\frac{\\mu_2+\\mu_3}{2}-\\mu_1\\] Let \\(\\mathbf{c}^T= (-1, 1/2, 1/2)\\), \\(\\boldsymbol{\\beta}^T = (\\mu_1, \\mu_2, \\mu_3)\\). \\[\\mathbf{c}^T\\boldsymbol{\\beta} = -\\mu_1+\\mu_2/2+ \\mu_3/2\\] is estimated by: \\[\\mathbf{c}^T\\hat{\\boldsymbol{\\beta}} = -\\bar{y}_{1.}+\\bar{y}_{2.}/2+ \\bar{y}_{3.}/2\\] and the variance is: \\[\\begin{bmatrix} -1 &amp; 1/2 &amp; 1/2 \\end{bmatrix} \\frac{1}{10}\\mathbf{I}_3 \\begin{bmatrix} -1 \\\\ 1/2 \\\\ 1/2\\end{bmatrix}\\sigma^2 = \\frac{3}{20}\\sigma^2\\] So, the \\(100 \\times (1- \\alpha) \\%\\)CI is: \\[\\mathbf{c}^T\\hat{\\boldsymbol{\\beta}} \\pm t_{27}(\\alpha/2) \\sqrt{\\frac{3}{20}\\hat{\\sigma}^2}\\] The df is: \\(n-g = 30-3 = 27\\). We could also test hypotheses e.g. \\(H_o: \\mathbf{c}^T\\boldsymbol{\\beta} = 0\\). The test statistic: \\[\\frac{\\mathbf{c}^T\\hat{\\boldsymbol{\\beta}}}{\\sqrt{\\frac{3}{20}\\hat{\\sigma}^2}} \\sim t_{27}.\\] References Bowerman, Bruce L., and Daniel Schafer. 1990. Linear Statistical Models. 2nd ed. Thomson Wadsworth. Cryer, J. D., and R. B. Miller. 1991. Statistics for Business: Data Analysis and Modelling. Business Statistics. PWS-Kent. Draper, Norman Richard, and Harry Smith. 1966. Applied Regression Analysis. Wiley Series in Probability and Mathematical Statistics. Wiley. Stapleton, James H. 2009. Linear Statistical Models. 2nd ed. Wiley Series in Probability and Statistics. Wiley. "],["model-comparisons-and-testing-for-lack-of-fit.html", "Chapter 5 Model comparisons and testing for lack of fit 5.1 F-tests for comparing two models 5.2 Sequential sums of squares 5.3 Testing for lack of fit 5.4 Added variable plots", " Chapter 5 Model comparisons and testing for lack of fit 5.1 F-tests for comparing two models 5.1.1 Example: Model A: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1\\) Model B: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\) i.e. in Model A, \\(\\beta_2=\\beta_3=0\\). Model A is the reduced or simpler model and model B is the full model. The \\(\\mbox{SSE}\\) for Model B will be smaller than the \\(\\mbox{SSE}\\) for Model A but is the reduction enough to justify the two extra parameters? We have: Model A: \\[\\mbox{SST} = \\mbox{SSR}(A) + \\mbox{SSE}(A)\\] Model B: \\[\\mbox{SST} = \\mbox{SSR}(B) + \\mbox{SSE}(B)\\] Note: \\[\\mbox{SSE}(A)-\\mbox{SSE}(B)=\\mbox{SSR}(B)-\\mbox{SSR}(A)\\] 5.1.2 F-test to compare models: Model A: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + ... + \\beta_q x_q\\) Model B: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + ... + \\beta_k x_k\\) where \\(q&lt;k\\) and Model A is nested within Model B. \\(H_0\\): \\(\\beta_{q+1} = \\beta_{q+2} = ... = \\beta_k = 0\\) \\(H_A\\): At least one \\(\\beta_{q+1}, ... , \\beta_k \\neq 0.\\) \\[F =\\frac{(\\mbox{SSE}(A)-\\mbox{SSE}(B))/(k-q)}{\\mbox{SSE}(B)/(n-p)}.\\] Under \\(H_0\\), \\[F \\sim F_{(k-q),(n-p)},\\] where \\(p = (k+1).\\) Note: Equivalently, the F-test can be written as: \\[F =\\frac{(\\mbox{SSR}(B)-\\mbox{SSR}(A))/(k-q)}{\\mbox{SSE}(B)/(n-p)}.\\] Note: Models A and B must be hierarchical for the F-test to be valid. 5.1.3 Example: Steam data This data is from a study undertaken to understand the factors that caused energy consumption in detergent manufacturing over a 25 month period. Example from Draper and Smith (1966). The data variables are: y = STEAM Pounds of steam used monthly. x1 = TEMP Average atmospheric temperature (\\(^o\\)F). x2 = INV Inventory: pounds of real fatty acid in storage per month. x3 = PROD Pounds of crude glycerin made. x4 = WIND Average wind velocity (in mph). x5 = CDAY Calendar days per month. x6 = OPDAY Operating days per month. x7 = FDAY Days below \\(32^o\\)F. x8 = WIND2 Average wind velocity squared. x9 = STARTS Number of production start-ups during the month. library(tidyverse) steamdata &lt;- read.csv(&quot;data/Steam.csv&quot;) steamdata |&gt; glimpse() ## Rows: 25 ## Columns: 10 ## $ STEAM &lt;dbl&gt; 10.98, 11.13, 12.51, 8.40, 9.27, 8.73, 6.36, 8.50, 7.82, 9.14, … ## $ TEMP &lt;dbl&gt; 35.3, 29.7, 30.8, 58.8, 61.4, 71.3, 74.4, 76.7, 70.7, 57.5, 46.… ## $ INV &lt;dbl&gt; 5.20, 5.12, 6.19, 3.89, 6.28, 5.76, 3.45, 6.57, 5.69, 6.14, 4.8… ## $ PROD &lt;dbl&gt; 0.61, 0.64, 0.78, 0.49, 0.84, 0.74, 0.42, 0.87, 0.75, 0.76, 0.6… ## $ WIND &lt;dbl&gt; 7.4, 8.0, 7.4, 7.5, 5.5, 8.9, 4.1, 4.1, 4.1, 4.5, 10.3, 6.9, 6.… ## $ CDAY &lt;int&gt; 31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31, 31, 28, 31, 30,… ## $ OPDAY &lt;int&gt; 20, 20, 23, 20, 21, 22, 11, 23, 21, 20, 20, 21, 21, 19, 23, 20,… ## $ FDAY &lt;int&gt; 22, 25, 17, 22, 0, 0, 0, 0, 0, 0, 11, 12, 25, 18, 5, 7, 0, 0, 0… ## $ WIND2 &lt;dbl&gt; 54.8, 64.0, 54.8, 56.3, 30.3, 79.2, 16.8, 16.8, 16.8, 20.3, 106… ## $ STARTS &lt;int&gt; 4, 5, 4, 4, 5, 4, 2, 5, 4, 5, 4, 4, 5, 5, 4, 4, 6, 4, 3, 4, 4, … steamdata |&gt; dplyr::select(STEAM, TEMP, INV, PROD) |&gt; pairs() Model A: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1\\) Model B: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\) where \\(x_1\\) = TEMP, \\(x_2\\) = INV, \\(x_3\\) = PROD. modelA &lt;- lm(STEAM ~ TEMP, steamdata) modelB &lt;- lm(STEAM ~ TEMP + INV + PROD, steamdata) summary(modelA) ## ## Call: ## lm(formula = STEAM ~ TEMP, data = steamdata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.6789 -0.5291 -0.1221 0.7988 1.3457 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.62299 0.58146 23.429 &lt; 2e-16 *** ## TEMP -0.07983 0.01052 -7.586 1.05e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8901 on 23 degrees of freedom ## Multiple R-squared: 0.7144, Adjusted R-squared: 0.702 ## F-statistic: 57.54 on 1 and 23 DF, p-value: 1.055e-07 summary(modelB) ## ## Call: ## lm(formula = STEAM ~ TEMP + INV + PROD, data = steamdata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.2348 -0.4116 0.1240 0.3744 1.2979 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.514814 1.062969 8.951 1.30e-08 *** ## TEMP -0.079928 0.007884 -10.138 1.52e-09 *** ## INV 0.713592 0.502297 1.421 0.17 ## PROD 0.330497 3.267694 0.101 0.92 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.652 on 21 degrees of freedom ## Multiple R-squared: 0.8601, Adjusted R-squared: 0.8401 ## F-statistic: 43.04 on 3 and 21 DF, p-value: 3.794e-09 anova(modelA) ## Analysis of Variance Table ## ## Response: STEAM ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## TEMP 1 45.592 45.592 57.543 1.055e-07 *** ## Residuals 23 18.223 0.792 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(modelB) ## Analysis of Variance Table ## ## Response: STEAM ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## TEMP 1 45.592 45.592 107.2523 1.046e-09 *** ## INV 1 9.292 9.292 21.8588 0.0001294 *** ## PROD 1 0.004 0.004 0.0102 0.9203982 ## Residuals 21 8.927 0.425 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 \\(H_0\\): \\(\\beta_2 = \\beta_3 = 0\\) \\(H_A\\): At least one \\(\\beta_2, \\beta_3 \\neq 0\\) \\[\\begin{align*} F_{obs} &amp; = \\frac{(\\mbox{SSE}(A)-\\mbox{SSE}(B))/(k-q)}{\\mbox{SSE}(B)/(n-p)}\\\\ &amp; = \\frac{(18.223-8.927)/(3-1)}{8.927/(25-4)}=10.93.\\\\ \\end{align*}\\] \\(F_{(0.05,2,21)} = 3.467\\), \\(F_{(0.01,2,21)} = 5.780\\) P-value \\(&lt;0.01\\), we reject \\(H_0\\) and conclude that at least one of \\(\\beta_2\\), \\(\\beta_3\\) differ from 0. anova(modelA, modelB) ## Analysis of Variance Table ## ## Model 1: STEAM ~ TEMP ## Model 2: STEAM ~ TEMP + INV + PROD ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 23 18.223 ## 2 21 8.927 2 9.2964 10.934 0.0005569 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.2 Sequential sums of squares 5.2.1 Example: Model A: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1\\) Model B: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2+ \\beta_3 x_3\\) As noted earlier, the reduction in \\(\\mbox{SSE}\\) going from Model A to B, is equivalent to the increase in \\(\\mbox{SSR}\\), i.e. \\[\\mbox{SSE}(A)-\\mbox{SSE}(B)=\\mbox{SSR}(B)-\\mbox{SSR}(A).\\] We can denote: \\[\\mbox{SSR}(B|A)=\\mbox{SSR}(B)-\\mbox{SSR}(A).\\] These are the sequential sums of squares. We can write: \\[\\begin{align*} \\mbox{SST} &amp; = \\mbox{SSR}(B) + \\mbox{SSE}(B)\\\\ &amp; = \\mbox{SSR}(A) +\\mbox{SSR}(B) - \\mbox{SSR}(A) + \\mbox{SSE}(B)\\\\ &amp; = \\mbox{SSR}(A) + \\mbox{SSR}(B|A) + \\mbox{SSE}(B).\\\\ \\mbox{SST} - \\mbox{SSE}(B) &amp;= \\mbox{SSR}(A) + \\mbox{SSR}(B|A)\\\\ \\mbox{SSR}(B) &amp;= \\mbox{SSR}(A) + \\mbox{SSR}(B|A).\\\\ \\end{align*}\\] If model A is appropriate, \\(\\mbox{SSR}(B|A)\\) should be small. 5.2.2 Example: Steam data Model A: \\(\\mathbb{E}[y] = \\beta_0\\) Model B: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1\\) Model C: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\) Model D: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\) SOURCE df seqSS Notation TEMP 1 45.592 SSR(B|A) INV 1 9.292 SSR(C|B) PROD 1 0.004 SSR(D|C) From the ANOVA table, \\[\\begin{align*} \\mbox{SSR}(D)&amp; =54.889\\\\ &amp; = \\mbox{SSR}(B|A) + \\mbox{SSR}(C|B) + \\mbox{SSR}(D|C)\\\\ \\end{align*}\\] We can use the F-test for comparing two models to test Seq SS. 1): Model A: \\(\\mathbb{E}[y] = \\beta_0\\) Model D: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\) \\(H_0\\): \\(\\beta_1 = \\beta_2 = \\beta_3 = 0\\) \\(H_a\\): Not all \\(\\beta_i\\) are 0 \\(\\mbox{SSR}(A) = 0\\) \\(\\mbox{SSR}(D|A) = \\mbox{SSR}(D) = 54.889.\\) \\[F_{obs} = \\frac{\\mbox{SSR}(D|A)/(k-q)}{\\mbox{SSE}(D)/(n-p)} = \\frac{54.889/(3-0)}{8.927/(25-4)}=43.04 \\] P-value \\(&lt; 0.001\\), we reject \\(H_0\\) and conclude that not all \\(\\beta_i\\) are 0. 2): Model C: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\) Model D: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\) \\(H_0\\): \\(\\beta_3 = 0\\) \\(H_a\\): \\(\\beta_3 \\neq 0\\) \\(\\mbox{SSR}(D|C) = 0.004\\) \\[F_{obs} = \\frac{\\mbox{SSR}(D|C)/(k-q)}{\\mbox{SSE}(D)/(n-p)} = \\frac{0.004/1}{8.927/21} = 0.01\\] \\(F_{(0.1,1,21)} = 2.96096\\), so P-value \\(&gt;0.05\\). We fail to reject \\(H_0\\) and conclude there is no evidence \\(\\beta_3 \\neq 0\\), i.e. \\(x_3\\) is not needed in the model. This F-test is equivalent to a t-test for \\(\\beta_3\\): \\[T = 0.1\\] \\[F = (0.1)(0.1) = 0.01\\] The p-value for both tests \\(= 0.92\\). Note: The Seq SS values depend on the order in which the variables are added to the model (unless the variables are uncorrelated). The anova and aov functions in R implement a sequential sum of squares (type I). Function Anova(, type= 2) in library(car) gives the adjusted SS (type II) modelB &lt;- lm(STEAM ~ TEMP + INV + PROD, steamdata) anova(modelB) ## Analysis of Variance Table ## ## Response: STEAM ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## TEMP 1 45.592 45.592 107.2523 1.046e-09 *** ## INV 1 9.292 9.292 21.8588 0.0001294 *** ## PROD 1 0.004 0.004 0.0102 0.9203982 ## Residuals 21 8.927 0.425 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(lm(STEAM ~ PROD + INV + TEMP, steamdata)) ## Analysis of Variance Table ## ## Response: STEAM ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## PROD 1 5.958 5.958 14.015 0.001197 ** ## INV 1 5.242 5.242 12.330 0.002076 ** ## TEMP 1 43.690 43.690 102.776 1.524e-09 *** ## Residuals 21 8.927 0.425 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 library(car) Anova(modelB, type= 2) ## Anova Table (Type II tests) ## ## Response: STEAM ## Sum Sq Df F value Pr(&gt;F) ## TEMP 43.690 1 102.7760 1.524e-09 *** ## INV 0.858 1 2.0183 0.1701 ## PROD 0.004 1 0.0102 0.9204 ## Residuals 8.927 21 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.3 Testing for lack of fit When replicate values of response are available at some or all of the \\(X\\) values, a formal test of model adequacy is available. The test is based on comparing the fitted value to the average response for that level of \\(X\\). NOTATION: Suppose there are \\(g\\) different values of \\(X\\) and at the \\(i^{th}\\) of these, there are \\(n_i\\) observations of \\(Y\\). Let \\(\\bar{y}_{i.}=\\frac{1}{n_i}\\sum_{j=1}^{n_i} y_{ij}\\), \\(\\quad i=1, ..., g.\\) Note: this is the estimate of the group means in the 1-way ANOVA model (means model): \\(y_{ij} = \\mu_{i} + \\epsilon_{ij}\\), where \\(\\epsilon_{ij}\\) iid \\(N(0, \\sigma^2)\\). Then the pure error sums of squares, \\[\\begin{align*} \\mbox{SS}_{\\mbox{PE}}&amp; =\\sum_{i=1}^g \\sum_{j=1}^{n_i} (y_{ij}- \\bar{y}_{i.})^2\\\\ df_{PE} &amp; = \\sum_{i=1}^g (n_i-1)=n-g, \\hspace{1cm} \\mbox{where } n=n_1+...+n_g.\\\\ \\end{align*}\\] Therefore \\[\\frac{\\sum_{i=1}^g \\sum_{j=1}^{n_i} (y_{ij}- \\bar{y}_{i.})^2}{n-g}\\] is an estimator of \\(\\sigma^2\\). NOTE: Here we use the replicates to obtain an estimate of \\(\\sigma^2\\) which is independent of the fitted model (SLR). *This estimator of \\(\\sigma^2\\) corresponds to the \\(\\mbox{MSE}\\) in the ANOVA table for the 1-way ANOVA model. The 1-way ANOVA model has \\(g\\) parameters. The SLR model has \\(2\\) parameters. The latter is more restrictive as it requires linearity. \\(df_{PE} = n-g\\), \\(df_{SLR} = n-2\\). The SLR model has a residual SS which is \\(\\geq\\) residual SS from the means model, i.e. \\(\\mbox{SSE} \\geq \\mbox{SS}_{\\mbox{PE}}\\). A large difference \\(\\mbox{SSE} - \\mbox{SS}_{\\mbox{PE}}\\) indicates lack of fit of the regression line. \\(\\mbox{SS}(\\mbox{lack of fit})= \\mbox{SSE} - \\mbox{SS}_{\\mbox{PE}} = \\sum_{i,j} (\\hat{y}_{i,j} - \\bar{y}_i)^2\\), the sum of squared distances of between the SLR estimate and the means model estimate of \\(\\mathbb{E}(Y_{i,j})\\). Lack of fit is tested by the statistic: \\[F_{obs}=\\frac{\\left ( \\mbox{SSE}-\\mbox{SS}_{\\mbox{PE}} \\right )/(g-2)}{\\mbox{SS}_{\\mbox{PE}}/(n-g)}.\\] \\(H_0\\): Regression model fits well \\(H_A\\): Regression model displays lack of fit Under \\(H_0\\), \\(F_{obs} \\sim F_{g-2,n-g}\\). Note: This generalises to multiple predictors - the pure error estimate of \\(\\sigma^2\\) is based on SS between \\(y_i\\) for cases with the same values on all predictors. \\(df_{SLR} = p\\) instead of 2. Reject for large values of \\(F_{obs}\\). 5.3.1 Example: Voltage Example from Ramsey and Schafer (2002) (case0802 in library(Sleuth3)). Batches of electrical fluids were subjected to constant voltages until the insulating properties of the fluid broke down. \\(Y\\): time to breakdown \\(X\\): Voltage The scatterplot of \\(Y\\) vs. \\(X\\) shows evidence of non-linearity and non-constant variance. The response was log transformed to resolve this. \\(H_0: \\beta_1=0\\) \\(H_A: \\beta_1 \\neq 0\\) \\(F = 78.4\\), \\(p&lt;0.001\\). We reject \\(H_0\\) and conclude that \\(\\beta_1 \\neq 0\\). anova(lm(log(TIME)~VOLTAGE, data.voltage)) ## Analysis of Variance Table ## ## Response: log(TIME) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## VOLTAGE 1 190.15 190.151 78.141 3.34e-13 *** ## Residuals 74 180.07 2.433 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 \\(H_0:\\) S.L.R model is appropriate/correct model \\(H_A:\\) S.L.R model has lack of fit. anova(lm(log(TIME)~as.factor(VOLTAGE) ,data.voltage)) ## Analysis of Variance Table ## ## Response: log(TIME) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## as.factor(VOLTAGE) 6 196.48 32.746 13.004 8.871e-10 *** ## Residuals 69 173.75 2.518 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 \\[F=\\frac{(180.07-173.75)/(7-2)}{173.75/(76-7)}=0.5\\] \\(F=0.50, p=0.773\\). We conclude that there is no evidence of lack of fit. 5.4 Added variable plots In simple linear regression we can assess the importance of a predictor by: t-statistic \\(\\mbox{SSR}\\) \\(R^2\\) \\(Y\\)-\\(X\\) plot. The analogues in multiple regression for assessing the importance of a predictor in the presence of other predictors are: t-statistic Seq/Extra SS partial \\(R^2\\) added variable plot. 5.4.1 Example: STEAM vs. TEMP, INV, PROD Model A: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1+ \\beta_2 x_2\\) Model B: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\) where \\(x_1\\) = TEMP, \\(x_2\\) = INV, \\(x_3\\) = PROD. The t-statistic for PROD is small: \\(T=0.10, p=0.920\\) \\(\\mbox{SSR}(B|A) = 0.004\\) is also small. The partial \\(R^2\\) for PROD is the proportion of variability in the response unexplained by TEMP and INV that is explained by PROD \\[\\begin{align*} R^2(\\mbox{PROD|TEMP, INV})&amp; =\\frac{\\mbox{SSR}(B|A)}{\\mbox{SSE}(A)} &amp; = \\frac{0.004}{8.931} = 0.00045=0.045\\%\\\\ \\end{align*}\\] The added variable plot shows the relationship between a response and a predictor, adjusting for other predictors in the model. ‘Adjusting’ \\(Y\\) for predictors \\(X_1,...,X_k\\) is achieved by computing the residuals from the regression of \\(Y\\) on \\(X_1,...,X_k\\). The resulting residuals can be thought of as \\(Y\\) with the effect of \\(X_1,...,X_k\\) removed. 5.4.2 Example: Added variable plot for PROD. i.e. should we add PROD to the model containing the predictors TEMP and INV? (Response is STEAM). Compute \\(e\\)(STEAM\\(|\\) TEMP, INV), i.e. the residuals from regression of STEAM on TEMP and INV. Compute \\(e\\)(PROD\\(|\\) TEMP, INV), i.e. the residuals from regression of PROD on TEMP and INV. AVP for PROD: Plot \\(e\\)(STEAM\\(|\\) TEMP, INV) vs. \\(e\\)(PROD\\(|\\) TEMP, INV). We can also do: AVP INV: Plot \\(e\\)(STEAM\\(|\\) TEMP, PROD) vs. \\(e\\)(INV\\(|\\) TEMP, PROD) AVP TEMP: Plot \\(e\\)(STEAM\\(|\\) INV, PROD) vs. \\(e\\)(TEMP\\(|\\) INV, PROD) 5.4.3 Example: Steam data cont’d fit1 &lt;- lm(STEAM ~ TEMP + INV, steamdata) fit2 &lt;- lm(PROD ~ TEMP + INV, steamdata) summary(lm(resid(fit1)~ resid(fit2))) ## ## Call: ## lm(formula = resid(fit1) ~ resid(fit2)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.2348 -0.4116 0.1240 0.3744 1.2979 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.716e-18 1.246e-01 0.000 1.000 ## resid(fit2) 3.305e-01 3.122e+00 0.106 0.917 ## ## Residual standard error: 0.623 on 23 degrees of freedom ## Multiple R-squared: 0.0004869, Adjusted R-squared: -0.04297 ## F-statistic: 0.0112 on 1 and 23 DF, p-value: 0.9166 Alternatively you can use the avPlots function in the library(car). library(car) avPlots(modelB) # in library(car) 5.4.4 Properties of AVPs: Estimated intercept is 0. Slope of the line in AVP for PROD equals \\(\\hat{\\beta}\\) (the coefficient of PROD in the model with TEMP, INV and PROD as predictors. Residuals in AVP equal residuals from regression of STEAM on TEMP, INV and PROD. \\(R^2\\) in AVP for PROD is the partial \\(R^2\\) for PROD, i.e. \\(R^2\\)(PROD\\(|\\)TEMP,INV). \\(\\hat{\\sigma}^2\\) from AVP for PROD \\(\\approx \\hat{\\sigma}^2\\) from full model. \\[\\hat{\\sigma}^2_{AVP}(n-2) = \\hat{\\sigma}^2_{full}(n-p)\\] The points in an AVP are clustered tightly around a line if and only if the variable is important. AV plots may also show outliers, or if the apparent adjusted association between \\(Y\\) and \\(X_j\\) is due to an influence point. References Draper, Norman Richard, and Harry Smith. 1966. Applied Regression Analysis. Wiley Series in Probability and Mathematical Statistics. Wiley. Ramsey, Fred, and Daniel Schafer. 2002. The Statistical Sleuth: A Course in Methods of Data Analysis. 2nd ed. Duxbury Press. "],["diagnostic-methods-in-more-details.html", "Chapter 6 Diagnostic methods (in more details) 6.1 Model assumptions 6.2 Residuals 6.3 Leverage values 6.4 Standardised residuals 6.5 Leave-one-out methods 6.6 Other influence measures 6.7 Testing outliers 6.8 Diagnostics examples (two case studies)", " Chapter 6 Diagnostic methods (in more details) 6.1 Model assumptions The assumptions can be stated in terms of the error vector: \\(\\mathbb{E}(\\boldsymbol{\\epsilon}) = \\mathbf{0}\\) \\(\\mbox{Var}(\\boldsymbol{\\epsilon}) = \\sigma^2\\mathbf{I}_n\\) \\(\\boldsymbol{\\epsilon} \\sim N(\\mathbf{0}, \\sigma^2\\mathbf{I}_n)\\) Since we do not observe \\(\\boldsymbol{\\epsilon}\\) we cannot check assumptions directly. Instead we observe residuals \\(\\mathbf{e}\\). 6.2 Residuals Residuals are the key to assessing model problems. \\[\\mathbf{e} = \\mathbf{Y} - \\mathbf{\\hat{Y}} = \\mathbf{Y} - \\mathbf{H}\\mathbf{Y} = (\\mathbf{I} - \\mathbf{H})\\mathbf{Y} \\] How do \\(\\boldsymbol{\\epsilon}\\) and \\(\\mathbf{e}\\) differ? If the model is correct, \\(\\mathbb{E}(\\mathbf{e}) = \\mathbb{E}(\\mathbf{Y}) - \\mathbb{E}(\\mathbf{\\hat{Y}})= \\mathbf{0}\\) (the same) \\(\\mbox{Var}(\\mathbf{e}) = (\\mathbf{I} - \\mathbf{H})\\sigma^2\\) (different). Like errors, residuals have mean 0, but Var(\\(e_i) = (1-h_{ii})\\sigma^2\\) so their variance is not quite constant (variance is smaller for \\(h_{ii}\\) close to 1). Note: this results shows that the residuals may have different variances even when \\(y_i\\)s have the same variance (\\(\\sigma^2\\)) because the precision of the fitted values depends on the pattern of \\(X_i\\)s. Cov(\\(e_i, e_j) = -h_{ii}\\sigma^2\\), for \\(i \\neq j\\). So the residuals are correlated, but in practice this correlation is generally not important or visible in residual plots. We plot: \\(e_i\\) vs \\(\\hat{y}_i\\) (residual vs fit), \\(e_i\\) vs \\(X_{ij}\\) (residual vs predictor \\(j\\)). The SLR conclusions are clear cut. In multiple regression deviations from the ideal pattern indicate model problems but precise diagnosis is more difficult. Figures above: ideal pattern: random scatter of points around 0 line non-constant variance, variability of residuals is changing curvature suggests that: \\(\\mathbb{E}(e_i) \\neq 0\\), thus, \\(\\mathbb{E}(\\epsilon_i)\\neq 0\\) curvature and non-constant variance 6.3 Leverage values \\[\\begin{align*} \\mathbf{\\hat{Y}} &amp; = \\mathbf{H}\\mathbf{Y}\\\\ \\mathbf{H} &amp; = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\\\ h_{ii} &amp; = \\mathbf{X}_i^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}_i\\\\ \\end{align*}\\] \\(h_{ii}\\) is known as the leverage of case \\(i\\). where \\(\\mathbf{X}_i\\) is row \\(i\\) of \\(\\mathbf{X}\\) matrix in a column vector. In SLR, \\[\\begin{align*} h_{ii} &amp; = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{S_{xx}}. \\end{align*}\\] A similar formula can be derived for multiple regression: \\[h_{ii} = \\frac{1}{n} + (\\mathbf{X}_i^* -\\mathbf{M})(\\mathbf{\\tilde{X}}^T\\mathbf{\\tilde{X}})^{-1}(\\mathbf{X}_i^* - \\mathbf{M}),\\] where \\(\\mathbf{X}_i^*\\) is row \\(i\\) of \\(\\mathbf{X}\\) matrix without the column of \\(\\mathbf{1}\\)s. i.e. \\(\\mathbf{X}_i =\\begin{bmatrix} 1 &amp; \\mathbf{X}_i^* \\end{bmatrix}\\) \\(\\mathbf{M}\\) is the mean vector \\[\\mathbf{M} =\\begin{bmatrix} \\bar{X}_{.1}\\\\ \\vdots \\\\ \\bar{X}_{.k} \\end{bmatrix}\\] \\(\\tilde{\\mathbf{X}}\\) is the matrix of centered \\(x\\) - data values, i.e. \\[\\tilde{\\mathbf{X}} =\\begin{bmatrix} (X_{11} - \\bar{X}_{.1}) &amp; \\dots &amp; (X_{1k} - \\bar{X}_{.k}) \\\\ (X_{21} - \\bar{X}_{.1}) &amp; \\dots &amp; (X_{2k} - \\bar{X}_{.k}) \\\\ \\vdots &amp; \\vdots &amp;\\vdots \\\\ (X_{n1} - \\bar{X}_{.1}) &amp; \\dots &amp; (X_{nk} - \\bar{X}_{.k}) \\end{bmatrix} \\] \\(\\bar{X}_{.1}, \\dots, \\bar{X}_{.k}\\) are the means of the \\(k\\) predictors. Therefore \\(h_{ii}\\) measures the `distance’ of case \\(i\\) from the average case \\(\\mathbf{M}\\). Example \\(k=2\\): contours of constant \\(h_{ii}\\) All points on an ellipse have the same \\(h_{ii}\\) value. 6.3.1 Properties of \\(h_{ii}\\): \\(0 \\leq h_{ii} \\leq 1\\) or if the model has an intercept \\(\\frac{1}{n} \\leq h_{ii} \\leq 1\\) \\(\\sum_{i=1}^n h_{ii} =p\\) where \\(p\\) is the number of parameters \\(\\sum_{i=1}^n h_{ij} =\\sum_{j=1}^n h_{ij} = 1\\), i.e. row sum = col sum = 1. If \\(h_{ii}\\) is large (rule of thumb: large if \\(\\geq 2 \\times \\mbox{average } h_{ii} = 2p/n\\)), then the case may be: a mistake a high influence case (i.e. has a big impact on results). If \\(h_{ii}\\) is extremely large (i.e. close to 1): \\(\\hat{y}_i \\approx y_i\\) so \\(e_i \\approx 0\\) and Var(\\(\\hat{y}_i)= h_{ii}\\sigma^2 \\approx \\sigma^2 \\approx\\) Var(\\(y_i\\)). In this case \\(\\mathbb{E}(y_i)\\) is imprecisely estimated. In practice, this rarely happens. 6.4 Standardised residuals When we compare residuals (\\(e_i\\)) for different observations we should take into account that their variances may differ: \\[\\mbox{Var}(e_i) = (1-h_{ii})\\sigma^2.\\] We can standardise them by dividing by \\(\\sqrt{(1-h_{ii})}\\hat{\\sigma}\\), where \\(\\hat{\\sigma}\\) is the estimate based on the SSE. We get standardised residuals: \\[r_i=\\frac{e_i}{\\sqrt{(1-h_{ii})}\\hat{\\sigma}}.\\] Then we have: \\(\\mathbb{E}(r_i) = 0\\) and \\(\\mbox{Var}(r_i)\\approx 1\\) (constant). Since \\(r_i\\), unlike \\(e_i\\), are on a common scale it is easier/fairer to compare them. Standardised residuals are useful in detecting anomalous observations or outliers. Note: Cases with \\(|r_i| \\geq 2\\) are not well fit. \\(r_i\\) are often used in place of \\(e_i\\) in residual plots and normal probability plots. \\(r_i\\) (and \\(e_i\\)) are not independent. 6.5 Leave-one-out methods Remember that as \\(h_{ii}\\) approaches 1, the variance of the residual approaches 0, indicating that the fitted value \\(\\hat{y}_i\\) is pulled close to the observed value \\(y_i\\). So leverage \\(h_{ii}\\) is the potential influence of the \\(i^{th}\\) observation. Observations with high leverage need to be inspected carefully as they might have a large influence on the fit. Note that potential influence is not necessarily the same thing as actual influence, since it is might be the case that the observation is in line with rest of the data, and fitting the model without this observation would give a prediction close to the observed \\(y_i\\) anyhow. One way to examine actual influence of case \\(i\\) is to compare the regression results with case \\(i\\) to those without case \\(i\\). Denote the fitted values with all cases included as \\(\\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_n\\) as usual. Denote the fitted values with case \\(i\\) removed as \\(\\hat{y}_{1(i)}, \\hat{y}_{2(i)}, ..., \\hat{y}_{n(i)}\\), \\(i = 1,...,n\\). Cook’s distance measures the influence of the \\(i^{th}\\) case by: \\[D_i = \\sum_{j=1}^n \\frac{(\\hat{y}_{j}-\\hat{y}_{j(i)})^2}{p \\hat{\\sigma}^2}.\\] Note this is proportional to the Euclidean distance (SS) between fitted values obtained by omitting the \\(i^{th}\\) observation \\(\\hat{y}_{j(i)}\\) and fitted values based on all the data \\(\\hat{y}_{j}\\). Typically we examine the case with the largest \\(D_i\\) further or, in the case of large datasets, the few cases with the largest \\(D_i\\) values, as these have the largest influence. So does the computation of Cook’s distance \\(D_i\\) for all observations require refitting the model \\(n\\) times? This would be computationally expensive! It turns out that we can rewrite the above formula for \\(D_i\\) as a function of the standardised residual \\(r_i\\) and the leverage \\(h_{ii}\\). So in the end we only need to fit the model once and then we can compute \\(D_i\\) from the complete data regression results. Shortcut formula: \\[\\hat{y}_{j(i)}=\\hat{y}_{j}- \\frac{h_{ij}}{1-h_{ii}}e_i.\\] Thus: \\[\\hat{y}_{j}-\\hat{y}_{j(i)}= \\frac{h_{ij}}{1-h_{ii}}e_i\\] and \\[\\begin{align*} \\sum_j(\\hat{y}_{j}-\\hat{y}_{j(i)})^2 &amp; = \\frac{e_i^2}{(1-h_{ii})^2}\\sum_j h_{ij}^2\\\\ &amp; = \\frac{e_i^2 h_{ii}}{(1-h_{ii})^2},\\\\ \\end{align*}\\] from the properties of the H matrix (symmetric and idempotent). Hence: \\[\\begin{align*} D_i&amp; = \\frac{e_i^2 h_{ii}}{p \\hat{\\sigma}^2(1-h_{ii})^2}\\\\ D_i&amp; = \\frac{r_i^2h_{ii}}{p(1-h_{ii})}.\\\\ \\end{align*}\\] In general, high influence or outlier cases have either: Big \\(|r_i|\\) and big \\(h_{ii}\\). Big \\(|r_i|\\) and moderate \\(h_{ii}\\). Moderate \\(|r_i|\\) and big \\(h_{ii}\\). 6.6 Other influence measures \\(\\hat{\\beta_j}-\\hat{\\beta}_{j(i)}\\) (the effect of leaving case \\(i\\) out) \\(\\hat{\\sigma}_{(i)}\\) (estimate of \\(\\sigma\\) with case \\(i\\) omitted). Used in: “studentised residual” = \\(\\frac{\\hat{\\epsilon_i}}{\\sqrt{1-h_{ii}}\\hat{\\sigma}_{(i)}}\\) 6.7 Testing outliers If an individual case is suspected of being an outlier a formal hypothesis test can be performed to verify. Details of the test are in Chapter 9 of Weisberg (2005). For more on leverages and influence see Chapter 2 of Rodrı́guez (2007). For a very detailed exposition on linear model diagnostics see Chapter 11 and 12 of Fox (2016). 6.8 Diagnostics examples (two case studies) 6.8.1 Example 1: Brain size versus body gestation period and litter Example from Ramsey and Schafer (2002) (case0902 in library(Sleuth3)). It is known that body size of mammals is a good predictor of brain size but it was of interest to know if gestation period and litter size were also good predictors. This data contains average values of brain weight, body weight, gestation length and litter size in 96 species of mammals. brain.data &lt;- read.csv(&quot;data/Species_brain.csv&quot;) Does the model fit well? Outliers, nonlinearity? NOTE: in MTB you can use brushing (set species as ID variables) and link the graphs to explore the data. #library(car) #library(MASS) fit1 &lt;- lm(BRAIN ~ BODY + GESTATION + LITTER, brain.data) summary(fit1) ## ## Call: ## lm(formula = BRAIN ~ BODY + GESTATION + LITTER, data = brain.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1026.68 -62.08 17.29 51.73 988.76 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -225.29213 83.05875 -2.712 0.00797 ** ## BODY 0.98588 0.09428 10.457 &lt; 2e-16 *** ## GESTATION 1.80874 0.35445 5.103 1.79e-06 *** ## LITTER 27.64864 17.41429 1.588 0.11579 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 224.6 on 92 degrees of freedom ## Multiple R-squared: 0.81, Adjusted R-squared: 0.8038 ## F-statistic: 130.7 on 3 and 92 DF, p-value: &lt; 2.2e-16 # anova(fit1) Clearly the model is defective. The normal probability plot shows that the residuals come from a heavy tailed distribution. The residual vs fit plot shows a linear pattern for the majority of data and a few outliers. Most of the pattern is hidden because the data is clumped together. We also observe unequal variance. African elephant, Hippopotamus, Dolphin, Human have large standardised residuals. African elephant, Hippopotamus and Dolphin, have large influence. fit1 |&gt; augment(brain.data) |&gt; arrange(desc(.cooksd)) |&gt; head() ## # A tibble: 6 × 11 ## SPECIES BRAIN BODY GESTATION LITTER .fitted .resid .hat .sigma .cooksd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 African ele… 4480 2800 655 1 3748. 732. 0.719 173. 24.3 ## 2 Hippopotamus 590 1400 240 1 1617. -1027. 0.251 188. 2.34 ## 3 Dolphin 1600 160 360 1 611. 989. 0.0791 198. 0.452 ## 4 Human being 1300 65 270 1 355. 945. 0.0352 202. 0.168 ## 5 Tapir 250 230 390 1 735. -485. 0.0945 219. 0.134 ## 6 Domestic pig 180 190 115 8 391. -211. 0.170 224. 0.0548 ## # ℹ 1 more variable: .std.resid &lt;dbl&gt; fit1 |&gt; augment(brain.data) |&gt; arrange(desc(.hat)) |&gt; head() ## # A tibble: 6 × 11 ## SPECIES BRAIN BODY GESTATION LITTER .fitted .resid .hat .sigma .cooksd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 African… 4480 2.8e+3 655 1 3748. 732. 0.719 173. 2.43e+1 ## 2 Hippopo… 590 1.4e+3 240 1 1617. -1027. 0.251 188. 2.34e+0 ## 3 Domesti… 180 1.9e+2 115 8 391. -211. 0.170 224. 5.48e-2 ## 4 Rat II 2.38 3.4e-1 21 8 34.2 -31.8 0.134 226. 8.98e-4 ## 5 Rat I 0.72 5 e-2 23 7.3 18.2 -17.5 0.102 226. 1.92e-4 ## 6 Tapir 250 2.3e+2 390 1 735. -485. 0.0945 219. 1.34e-1 ## # ℹ 1 more variable: .std.resid &lt;dbl&gt; fit1 |&gt; augment(brain.data) |&gt; arrange(desc(abs(.std.resid))) |&gt; head(5) ## # A tibble: 5 × 11 ## SPECIES BRAIN BODY GESTATION LITTER .fitted .resid .hat .sigma .cooksd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 African ele… 4480 2800 655 1 3748. 732. 0.719 173. 24.3 ## 2 Hippopotamus 590 1400 240 1 1617. -1027. 0.251 188. 2.34 ## 3 Dolphin 1600 160 360 1 611. 989. 0.0791 198. 0.452 ## 4 Human being 1300 65 270 1 355. 945. 0.0352 202. 0.168 ## 5 Tapir 250 230 390 1 735. -485. 0.0945 219. 0.134 ## # ℹ 1 more variable: .std.resid &lt;dbl&gt; Added-variable plots can be used for detecting influential data. For example, AVP for body: Hippo and African elephant have a large body given the other variables (litter and gestation). Humans, dolphins and African elephant have a large brain for their litter and gestation. Together Humans, dolphins and hippos reduce the body slope, African elephant, while a high-leverage point, is more in line with the rest of the data. The raw data have a lot of skeweness. This naturally generates a lot of outliers and obscures patterns in the data. Transform the data by taking the logs to improve the model and fit. The skeweness is gone. The plot of log(brain) vs log(body) shows a strong linear pattern - not apparent with the untransformed variables. Therefore the log transformation of these two variables seems appropriate. The other variables are also less skewed than before. fit2 &lt;- lm(log(BRAIN) ~ log(BODY) + log(GESTATION) + log(LITTER), brain.data) summary(fit2) ## ## Call: ## lm(formula = log(BRAIN) ~ log(BODY) + log(GESTATION) + log(LITTER), ## data = brain.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.95415 -0.29639 -0.03105 0.28111 1.57491 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.85482 0.66167 1.292 0.19962 ## log(BODY) 0.57507 0.03259 17.647 &lt; 2e-16 *** ## log(GESTATION) 0.41794 0.14078 2.969 0.00381 ** ## log(LITTER) -0.31007 0.11593 -2.675 0.00885 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4748 on 92 degrees of freedom ## Multiple R-squared: 0.9537, Adjusted R-squared: 0.9522 ## F-statistic: 631.6 on 3 and 92 DF, p-value: &lt; 2.2e-16 #anova(fit2) fit2 |&gt; augment(brain.data) |&gt; arrange(desc(abs(.std.resid))) |&gt; head(3) ## # A tibble: 3 × 11 ## SPECIES BRAIN BODY GESTATION LITTER .fitted .resid .hat .sigma .cooksd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Human being 1300 65 270 1 5.60 1.57 0.0225 0.447 0.0647 ## 2 Dolphin 1600 160 360 1 6.23 1.14 0.0312 0.462 0.0483 ## 3 Tapir 250 230 390 1 6.48 -0.954 0.0342 0.466 0.0370 ## # ℹ 1 more variable: .std.resid &lt;dbl&gt; The biggest standardised residual is now case 24 (Human) who have a larger brain than predicted by the model. fit2 |&gt; augment(brain.data) |&gt; arrange(desc(.hat)) |&gt; head(6) ## # A tibble: 6 × 11 ## SPECIES BRAIN BODY GESTATION LITTER .fitted .resid .hat .sigma .cooksd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Quokka 17.5 3.5 26 1 2.94 -0.0747 0.259 0.477 2.93e-3 ## 2 Domestic… 180 190 115 8 5.21 -0.0176 0.146 0.477 6.89e-5 ## 3 Gentle l… 7.8 0.22 145 2 1.85 0.205 0.116 0.477 6.93e-3 ## 4 Nutria 23 5 132 5.5 3.29 -0.157 0.114 0.477 3.95e-3 ## 5 Hyrax 20.5 3.8 225 2.4 3.61 -0.594 0.100 0.473 4.85e-2 ## 6 Hampster… 1.12 0.13 16 6.3 0.270 -0.156 0.0711 0.477 2.23e-3 ## # ℹ 1 more variable: .std.resid &lt;dbl&gt; fit2 |&gt; augment(brain.data) |&gt; arrange(desc(.cooksd)) |&gt; head(3) ## # A tibble: 3 × 11 ## SPECIES BRAIN BODY GESTATION LITTER .fitted .resid .hat .sigma .cooksd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Hippopotam… 590 1.4e3 240 1 7.31 -0.931 0.0645 0.467 0.0709 ## 2 Human being 1300 6.5e1 270 1 5.60 1.57 0.0225 0.447 0.0647 ## 3 Hyrax 20.5 3.8e0 225 2.4 3.61 -0.594 0.100 0.473 0.0485 ## # ℹ 1 more variable: .std.resid &lt;dbl&gt; Highest leverage is case 1 (quokka), but it doesn’t have high influence (this also goes for all cases with \\(h_{ii} &gt; 0.083 = 2p/n\\), where average \\(h_{ii} = p/n\\), except for case 73 (hyrax) which has longer gestation). The residual distribution shows some skeweness, but is closer to normal than before. The residual vs fit plot shows some evidence of non-constant variance. Perhaps some other transformation would be better? To answer the original question, clearly gestation time and litter size are important predictors of brain size in the presence of body size. 6.8.2 Example 2: Rat data Example from Weisberg (2005). Experiment to investigate amount of drug present in liver of rat. BodyWt = body weight of the rat LiverWt = measured after sacrifice Dose = dose given, proportional to body weight EndDose = dose of drug recovered after sacrifice of the animal Experimantal hypothesis: no relationship between EndDose and 3 predictors. rats.data &lt;- read.table(&quot;data/rat.txt&quot;, header = TRUE) rats.data |&gt; dplyr::select(EndDose, BodyWt, LiverWt, Dose)|&gt; pairs() #library(car) #library(MASS) fit1 &lt;- lm(EndDose ~ BodyWt + LiverWt + Dose, data = rats.data) summary(fit1) ## ## Call: ## lm(formula = EndDose ~ BodyWt + LiverWt + Dose, data = rats.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.100557 -0.063233 0.007131 0.045971 0.134691 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.265922 0.194585 1.367 0.1919 ## BodyWt -0.021246 0.007974 -2.664 0.0177 * ## LiverWt 0.014298 0.017217 0.830 0.4193 ## Dose 4.178111 1.522625 2.744 0.0151 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.07729 on 15 degrees of freedom ## Multiple R-squared: 0.3639, Adjusted R-squared: 0.2367 ## F-statistic: 2.86 on 3 and 15 DF, p-value: 0.07197 #anova(fit1) It appears as if a combination of bodyweight and dose are relevant to EndDose. However, they are not individually related to EndDose even though \\(X_1 \\propto X_3\\) Case 3 has large influence (see Cook’s distance plot). Case 3 is an unusual combination of dose and bodyweight. fit1 |&gt; augment(rats.data) |&gt; arrange(desc(.hat)) |&gt; head() ## # A tibble: 6 × 10 ## BodyWt LiverWt Dose EndDose .fitted .resid .hat .sigma .cooksd .std.resid ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 190 9 1 0.56 0.536 0.0241 0.851 0.0782 0.930 0.807 ## 2 200 7.2 1 0.23 0.298 -0.0677 0.392 0.0766 0.203 -1.12 ## 3 149 5.2 0.75 0.21 0.308 -0.0982 0.316 0.0734 0.273 -1.54 ## 4 195 10 0.98 0.41 0.360 0.0496 0.254 0.0785 0.0469 0.743 ## 5 186 6.8 0.94 0.28 0.339 -0.0588 0.217 0.0780 0.0510 -0.859 ## 6 146 7.3 0.73 0.3 0.318 -0.0184 0.195 0.0798 0.00425 -0.265 A rat was given a dose that was too high for his bodyweight. avPlots(fit1) Redo the analysis with case 3 removed: fit1 &lt;- lm(EndDose ~ BodyWt + LiverWt + Dose, data = rats.data[-3,]) summary(fit1) ## ## Call: ## lm(formula = EndDose ~ BodyWt + LiverWt + Dose, data = rats.data[-3, ## ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.102154 -0.056486 0.002838 0.046519 0.137059 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.311427 0.205094 1.518 0.151 ## BodyWt -0.007783 0.018717 -0.416 0.684 ## LiverWt 0.008989 0.018659 0.482 0.637 ## Dose 1.484877 3.713064 0.400 0.695 ## ## Residual standard error: 0.07825 on 14 degrees of freedom ## Multiple R-squared: 0.02106, Adjusted R-squared: -0.1887 ## F-statistic: 0.1004 on 3 and 14 DF, p-value: 0.9585 #anova(fit1) Experimental hypothesis is validated as all coefficients have large P-values. 6.8.2.1 Should Unusual Data Be Discarded? Although problematic data should not be ignored, they also should not be deleted automatically. It is important to investigate why an observation is unusual. Truly bad data (e.g.rats) can be corrected or thrown away. When a discrepant data-point is correct, we may be able to understand why the observation is unusual. For Species Brain data, it makes sense that humans enjoy brain size not accounted for by the other variables. In a case like this, we may choose to deal separately with an outlying observation. Outliers or influential data may motivate model respecification e.g. the introduction of additional explanatory variables. However, we must be careful to avoid overfitting the data i.e. permitting a small portion of the data to determine the form of the model. A more extensive discussion can be found in Fox (2016) Chapter 11.7, pg 288-289. References ———. 2016. Applied Regression Analysis and Generalized Linear Models. 3rd ed. SAGE Publications. Ramsey, Fred, and Daniel Schafer. 2002. The Statistical Sleuth: A Course in Methods of Data Analysis. 2nd ed. Duxbury Press. Rodrı́guez, G. 2007. “Lecture Notes on Generalized Linear Models.” http://data.princeton.edu/wws509/notes/. Weisberg, Sanford. 2005. Applied linear regression. Wiley-Blackwell. "],["special-cases-of-multiple-regression.html", "Chapter 7 Special cases of multiple regression 7.1 Categorical and continuous predictors (binary categories) 7.2 Categorical and continuous predictors (more than two categories) 7.3 Two way anova: two categorical predictors 7.4 Polynomial regression 7.5 Confounding example 7.6 Quadratic terms and interactions 7.7 An example with two continuous and two categorical predictors", " Chapter 7 Special cases of multiple regression 7.1 Categorical and continuous predictors (binary categories) Example (from Ramsey and Schafer (2002) pg 236, 245): \\(Y\\): average number of flowers per plant (meadowfoam). Light intensity: 150, 300, 450, 600, 750, 900 (\\(\\mu\\) mol/\\(m^2\\)/sec) Timing: Timing of onset of light treatment Early/Late. Coded 0/1. Suppose data is in the table below (every 2nd row) and consider the following models: Parallel lines model (model A): \\[\\mathbb{E}(y)= \\beta_0+\\beta_1(timing)+ \\beta_2 (light)\\] Separate lines model (model B): \\[\\mathbb{E}(y)= \\beta_0+\\beta_1(timing)+ \\beta_2 (light) + \\beta_3 (timing \\times light)\\] Give the design matrix and the parameter vector for both models Test \\(H_0: \\beta_3 = 0\\). flowers.data &lt;- read.csv(&quot;data/flowers.csv&quot;) flowers.data |&gt; glimpse() ## Rows: 24 ## Columns: 4 ## $ Flowers &lt;dbl&gt; 62.3, 77.4, 55.3, 54.2, 49.6, 61.9, 39.4, 45.7, 31.3, 44.9, … ## $ Timing &lt;chr&gt; &quot;Early&quot;, &quot;Early&quot;, &quot;Early&quot;, &quot;Early&quot;, &quot;Early&quot;, &quot;Early&quot;, &quot;Early… ## $ Time &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ Intensity &lt;int&gt; 150, 150, 300, 300, 450, 450, 600, 600, 750, 750, 900, 900, … flowers.data &lt;- flowers.data |&gt; mutate(Timing = as.factor(Timing)) Parallel lines model: \\[\\mathbb{E}(y)= \\beta_0+\\beta_1(timing)+ \\beta_2 (light)\\] \\[\\mathbf{X} = \\begin{bmatrix} 1 &amp; 0 &amp; 150 \\\\ 1 &amp; 0 &amp; 300\\\\ 1 &amp; 0 &amp; 450 \\\\ 1 &amp; 0 &amp; 600\\\\ 1 &amp; 0 &amp; 750\\\\ 1 &amp; 0 &amp; 900\\\\ 1 &amp; 1 &amp; 150 \\\\ 1 &amp; 1 &amp; 300\\\\ 1 &amp; 1 &amp; 450 \\\\ 1 &amp; 1 &amp; 600 \\\\ 1 &amp; 1 &amp; 750\\\\ 1 &amp; 1 &amp; 900 \\\\ \\end{bmatrix}\\] \\(\\boldsymbol{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2\\\\ \\end{bmatrix}\\) Separate lines model: \\[\\mathbb{E}(y)= \\beta_0+\\beta_1(timing)+ \\beta_2 (light) + \\beta_3 (timing \\times light)\\] \\[\\mathbf{X} = \\begin{bmatrix} 1 &amp; 0 &amp; 150 &amp;0 \\\\ 1 &amp; 0 &amp; 300 &amp;0 \\\\ 1 &amp; 0 &amp; 450 &amp;0 \\\\ 1 &amp; 0 &amp; 600 &amp;0 \\\\ 1 &amp; 0 &amp; 750 &amp;0 \\\\ 1 &amp; 0 &amp; 900 &amp;0 \\\\ 1 &amp; 1 &amp; 150 &amp;150 \\\\ 1 &amp; 1 &amp; 300 &amp;300 \\\\ 1 &amp; 1 &amp; 450 &amp;450 \\\\ 1 &amp; 1 &amp; 600 &amp;600 \\\\ 1 &amp; 1 &amp; 750 &amp;750 \\\\ 1 &amp; 1 &amp; 900 &amp;900 \\\\ \\end{bmatrix}\\] \\[\\boldsymbol{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2\\\\ \\beta_3\\\\ \\end{bmatrix}\\] To test \\(H_0: \\beta_3 = 0\\), P-value = 0.910, so cannot reject \\(H_0\\) (See table of coefficients, output below). Model A: fit1 &lt;- lm(Flowers ~ Intensity + Timing, flowers.data) summary(fit1) ## ## Call: ## lm(formula = Flowers ~ Intensity + Timing, data = flowers.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.652 -4.139 -1.558 5.632 12.165 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 71.305833 3.273772 21.781 6.77e-16 *** ## Intensity -0.040471 0.005132 -7.886 1.04e-07 *** ## TimingLate 12.158333 2.629557 4.624 0.000146 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.441 on 21 degrees of freedom ## Multiple R-squared: 0.7992, Adjusted R-squared: 0.78 ## F-statistic: 41.78 on 2 and 21 DF, p-value: 4.786e-08 Model B: fit2 &lt;- lm(Flowers ~ Intensity * Timing, flowers.data) summary(fit2) ## ## Call: ## lm(formula = Flowers ~ Intensity * Timing, data = flowers.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.516 -4.276 -1.422 5.473 11.938 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 71.623333 4.343305 16.491 4.14e-13 *** ## Intensity -0.041076 0.007435 -5.525 2.08e-05 *** ## TimingLate 11.523333 6.142360 1.876 0.0753 . ## Intensity:TimingLate 0.001210 0.010515 0.115 0.9096 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.598 on 20 degrees of freedom ## Multiple R-squared: 0.7993, Adjusted R-squared: 0.7692 ## F-statistic: 26.55 on 3 and 20 DF, p-value: 3.549e-07 model.matrix(fit2) ## (Intercept) Intensity TimingLate Intensity:TimingLate ## 1 1 150 0 0 ## 2 1 150 0 0 ## 3 1 300 0 0 ## 4 1 300 0 0 ## 5 1 450 0 0 ## 6 1 450 0 0 ## 7 1 600 0 0 ## 8 1 600 0 0 ## 9 1 750 0 0 ## 10 1 750 0 0 ## 11 1 900 0 0 ## 12 1 900 0 0 ## 13 1 150 1 150 ## 14 1 150 1 150 ## 15 1 300 1 300 ## 16 1 300 1 300 ## 17 1 450 1 450 ## 18 1 450 1 450 ## 19 1 600 1 600 ## 20 1 600 1 600 ## 21 1 750 1 750 ## 22 1 750 1 750 ## 23 1 900 1 900 ## 24 1 900 1 900 ## attr(,&quot;assign&quot;) ## [1] 0 1 2 3 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$Timing ## [1] &quot;contr.treatment&quot; 7.2 Categorical and continuous predictors (more than two categories) Example: (from Ramsey and Schafer (2002)): \\(Y\\): Measure of energy \\(X_1\\): Measure of weight Group: Type of flyer (1,2,3). Z1, Z2, Z3 (dummy variables). flying.data &lt;- read.csv(&quot;data/flying.csv&quot;) flying.data |&gt; glimpse() ## Rows: 20 ## Columns: 6 ## $ Group &lt;int&gt; 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3 ## $ x1 &lt;dbl&gt; 6.658011, 6.442540, 5.552960, 5.752573, 3.190476, 3.555348, 4.28… ## $ y &lt;dbl&gt; 3.77734812, 3.54961737, 3.14845333, 3.10906094, 0.90016136, 1.36… ## $ G1 &lt;int&gt; 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ## $ G2 &lt;int&gt; 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0 ## $ G3 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1 flying.data &lt;- flying.data |&gt; mutate(Group = as.factor(Group)) flying.data |&gt; ggplot(aes(x = x1, y = y, col = Group)) + geom_point() Parallel lines model (model A): \\[\\mathbb{E}(y)= \\beta_0+\\beta_1 z_2+ \\beta_2 z_3 + \\beta_3 x_1\\] Separate lines model (model B): \\[\\mathbb{E}(y)= \\beta_0+\\beta_1 z_2+ \\beta_2 z_3 + \\beta_3 x_1 + \\beta_4 x_1 z_2+ \\beta_5 x_1 z_3\\] Hypothesis testing: Test \\(H_0: \\beta_4 = \\beta_5 = 0\\) by comparing the two models using an F-test. Test \\(H_0: \\beta_1 = \\beta_2 = 0\\) by comparing the parallel lines model to the model \\(\\mathbb{E}(y)= \\beta_0+\\beta_3 x_1\\) using an F-test. Give the design matrix and the parameter vector for both models. Test \\(H_0: \\beta_4 = \\beta_5 = 0\\), i.e. \\(H_0:\\) Model A is correct \\(H_A:\\) Model B is preferable to Model A \\[\\begin{align*} F &amp; =\\frac{(\\mbox{SSE}(A)-\\mbox{SSE}(B))/(k-q)}{\\mbox{SSE}(B)/(n-p)}\\\\ &amp; =\\frac{(0.5533- 0.5049)/(5-3)}{0.5049/(20-6)}\\\\ &amp; =\\frac{0.0242}{0.0361}\\\\ &amp; = 0.67.\\\\ \\end{align*}\\] \\(F_{(2,14)}(0.95) = 3.73 &gt; 0.67\\) so we cannot reject \\(H_0\\), model A is OK. Test \\(H_0: \\beta_1 = \\beta_2 = 0\\), i.e. let model C = one group model: \\[\\mathbb{E}(y)= \\beta_0+ \\beta_3 x_1 \\] \\(H_0:\\) Model C is correct \\(H_A:\\) Model A is preferable to Model C \\[\\begin{align*} F &amp; =\\frac{(\\mbox{SSE}(C)-\\mbox{SSE}(A))/(k-q)}{\\mbox{SSE}(A)/(n-p)}\\\\ &amp; =\\frac{\\mbox{SSR}(A|C)/(3-1)}{0.5533/(20-4)}\\\\ &amp; =\\frac{0.0296/2}{0.0346}\\\\ &amp; = 0.43\\\\ \\end{align*}\\] We don’t need to see the fit for Model C, take Seq SS. \\(F_{(2,16)}(0.95) = 3.63 &gt; 0.43\\) so we cannot reject \\(H_0\\), model C is adequate. fitA &lt;- lm(y ~ x1 + Group, data = flying.data) summary(fitA) ## ## Call: ## lm(formula = y ~ x1 + Group, data = flying.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.23224 -0.12199 -0.03637 0.12574 0.34457 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.57636 0.28724 -5.488 4.96e-05 *** ## x1 0.81496 0.04454 18.297 3.76e-12 *** ## Group2 0.10226 0.11418 0.896 0.384 ## Group3 0.07866 0.20268 0.388 0.703 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.186 on 16 degrees of freedom ## Multiple R-squared: 0.9815, Adjusted R-squared: 0.9781 ## F-statistic: 283.6 on 3 and 16 DF, p-value: 4.464e-14 anova(fitA) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x1 1 29.3919 29.3919 849.9108 2.691e-15 *** ## Group 2 0.0296 0.0148 0.4276 0.6593 ## Residuals 16 0.5533 0.0346 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 fitB &lt;- lm(y ~ x1 * Group, data = flying.data) summary(fitB) ## ## Call: ## lm(formula = y ~ x1 * Group, data = flying.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.25152 -0.12643 -0.00954 0.08124 0.32840 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.2024 1.2613 -0.161 0.8748 ## x1 0.5898 0.2061 2.861 0.0126 * ## Group2 -1.3784 1.2952 -1.064 0.3053 ## Group3 -1.2681 1.2854 -0.987 0.3406 ## x1:Group2 0.2456 0.2134 1.151 0.2691 ## x1:Group3 0.2149 0.2236 0.961 0.3529 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1899 on 14 degrees of freedom ## Multiple R-squared: 0.9832, Adjusted R-squared: 0.9771 ## F-statistic: 163.4 on 5 and 14 DF, p-value: 6.696e-12 anova(fitB) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x1 1 29.3919 29.3919 815.0383 8.265e-14 *** ## Group 2 0.0296 0.0148 0.4100 0.6713 ## x1:Group 2 0.0484 0.0242 0.6718 0.5265 ## Residuals 14 0.5049 0.0361 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(fitA, fitB) ## Analysis of Variance Table ## ## Model 1: y ~ x1 + Group ## Model 2: y ~ x1 * Group ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 16 0.55332 ## 2 14 0.50487 2 0.04845 0.6718 0.5265 fitC &lt;- lm(y ~ x1, data = flying.data) summary(fitC) ## ## Call: ## lm(formula = y ~ x1, data = flying.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.21143 -0.14422 -0.04284 0.09681 0.37695 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.46826 0.13716 -10.71 3.1e-09 *** ## x1 0.80861 0.02684 30.13 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.18 on 18 degrees of freedom ## Multiple R-squared: 0.9806, Adjusted R-squared: 0.9795 ## F-statistic: 907.6 on 1 and 18 DF, p-value: &lt; 2.2e-16 anova(fitC) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x1 1 29.3919 29.3919 907.64 &lt; 2.2e-16 *** ## Residuals 18 0.5829 0.0324 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.3 Two way anova: two categorical predictors Two way analysis of variance (ANOVA) without interactions is the same a regression with two categorical explanatory variables. Two way analysis of variance (ANOVA) with interactions is the same a regression with two categorical explanatory variables plus a third categorical explanatory variable for the interaction. Remember that when the explanatory variable is categorical, conceptually it is recoded using dummy or indicator variables. Typically: Y = response. Two predictors A and B are categorical, with \\(a\\) levels of A and \\(b\\) levels of B. 7.3.1 Example: paint adhesion In an experiment to improve paint adhesion, the factors examined were: A = application method (\\(a = 2\\)). B = paint type (\\(b = 3\\)), It is not correct to find the best paint and then separately find the best application method because that may not be the best overall combination. paint &lt;- read.csv(here(&quot;data&quot;, &quot;paint.csv&quot;), header = TRUE) paint$Primer &lt;- as.factor(paint$Primer) paint$Method &lt;- as.factor(paint$Method) paint |&gt; ggplot(aes(x = Primer, y = Adhesion, colour = Method, group = Method)) + geom_point(alpha = 0.5) This is an example of experimental data, with \\(3 \\times 2\\) experimental conditions. In this example there are 3 replicates for each experimental condition, i.e there are 18 observations in total. The dataset could be summarized in a 3 by 2 table of means. In experimental data, the experimenter will often set up the data with equal number of observations per cell. This is described as balanced data. Observational data is unlikely to be balanced. 7.3.2 Notation Let \\(Y_{ijk}\\) be the \\(k\\)th response at level \\(i\\) of A and \\(j\\) of B. Let \\(\\mu_{ij} = \\mathbb{E}(Y_{ijk})\\) be the mean response at level \\(i\\) of A and \\(j\\) of B. The model is \\[Y_{ijk} = \\mu_{ij} + \\epsilon_{ijk}\\] where \\(\\epsilon_{ijk} \\sim N(0, \\sigma^2)\\) and are independent. This is the same as a one way ANOVA model with a single categorical predictor with \\(ab\\) levels. The above is known as the means model. It is convenient to decompose \\(\\mu_{ij}\\) into \\[\\mu_{ij} = \\mu + \\alpha_i + \\beta_j + \\gamma_{ij}\\] so that \\[Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_{ij} + \\epsilon_{ijk}\\] We can think of these parameters as: \\(\\mu\\) is the overall level, or mean \\(\\alpha_i\\) is the additional effect of level \\(i\\) of A \\(\\beta_i\\) is the additional effect of level \\(j\\) of B \\(\\gamma_{ij}\\) is the interaction effect of level \\(i\\) of A and \\(j\\) of B. The above is known as the effects model. However, the effects model is overparameterized. Means model has \\(ab\\) \\(\\mu_{ij}\\) parameters, and in the effects model parameterisation, we express these \\(\\mu_{ij}\\) in terms of \\(1+a+b+ ab\\) parameters. Thus we need to impose \\(1+a+b\\) identifability constraints. Standard identifiability constraints are: \\(\\sum_i \\alpha_i = 0\\), \\(\\sum_j \\beta_j = 0\\), \\(\\sum_i \\gamma_{ij} = 0\\) and \\(\\sum_j \\gamma_{ij} = 0\\). 7.3.3 Two way ANOVA in regression setup Let \\(A_i\\) be indicator for level \\(i\\) of A. Let \\(B_j\\) be indicator for level \\(j\\) of B. Then \\(A_i B_j\\) is an indicator for when A is at level \\(i\\), B is at level \\(j\\). Supposing \\(a=2\\), \\(b=3\\), we can rewrite the effects model as \\[\\begin{align} y = \\mu &amp;+ \\alpha_1A_1+ \\alpha_2A_2 \\notag \\\\ &amp;+ \\beta_1B_1+ \\beta_2B_2 + \\beta_3B_3 \\notag \\\\ &amp;+ \\gamma_{11}A_1B_1 + \\gamma_{12}A_1B_2 + \\gamma_{13}A_1B_3 + \\gamma_{21}A_2B_1 + \\gamma_{22}A_2B_2 + \\gamma_{23}A_2B_3 \\\\ &amp; + \\epsilon \\end{align}\\] This regression model is over-parameterized. If we construct the \\(\\mathbf{X}\\) matrix, the columns are not linearly independent. This means that the \\(\\mathbf{X}^T \\mathbf{X}\\) matrix does not have full rank and its inverse does not exist. In this context, the obvious identifiability constraints are to set one of the \\(\\alpha\\)’s to zero, i.e. \\(\\alpha_1 =0\\). Also \\(\\beta_1 =0\\) and \\(0 = \\gamma_{11}= \\gamma_{12}=\\gamma_{13}=\\gamma_{21}\\), basically any parameter with an index of 1. Then the model becomes \\[\\begin{align} y = \\mu &amp;+ \\alpha_2A_2 \\notag \\\\ &amp;+ \\beta_2B_2 + \\beta_3B_3 \\notag \\\\ &amp; + \\gamma_{22}A_2B_2 + \\gamma_{23}A_2B_3 \\\\ &amp; + \\epsilon \\end{align}\\] which has the required number of parameters. Note that the parameter interpretation depends on the constraints chosen. Here for example the cell means are 7.3.4 Interaction plots: When we plot means of data, we can assess whether the lines connecting means are parallel or not, indicating interaction or not. paint_means &lt;- paint |&gt; group_by(Primer, Method) |&gt; summarise(Means = mean(Adhesion)) require(gridExtra) plot1 &lt;- paint_means |&gt; ggplot(aes(x = Primer, y = Means, colour = Method, group = Method)) + geom_point(size = 4) + geom_line() plot2 &lt;-paint_means |&gt; ggplot(aes(x = Method, y = Means, colour = Primer, group = Primer)) + geom_point(size = 4) + geom_line() grid.arrange(plot1, plot2, ncol=2) 7.3.5 Testing hypotheses Interactions: Test \\(H_0: \\gamma_{ij} = 0\\), all \\(i,j\\). Main effect of A: Test \\(H_0: \\alpha_{i} = 0\\), all \\(i\\). Main effect of B: Test \\(H_0: \\beta_{j} = 0\\), all \\(j\\). Perform (1) first. Only perform (2) and (3) if there is no evidence of interaction. It does not make sense to test for main effects in the presence of interaction. 7.3.6 Two way ANOVA in R fit1 &lt;- lm(Adhesion ~ Primer * Method, data = paint) coef(fit1) ## (Intercept) Primer2 Primer3 Method2 Primer2:Method2 ## 4.2666667 1.0333333 -0.4333333 1.0333333 -0.2666667 ## Primer3:Method2 ## 0.3000000 anova(fit1) ## Analysis of Variance Table ## ## Response: Adhesion ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Primer 2 4.5811 2.2906 27.8581 3.097e-05 *** ## Method 1 4.9089 4.9089 59.7027 5.357e-06 *** ## Primer:Method 2 0.2411 0.1206 1.4662 0.2693 ## Residuals 12 0.9867 0.0822 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Is there evidence of an interaction between primer and method? fit1 |&gt; model.matrix() |&gt; head() ## (Intercept) Primer2 Primer3 Method2 Primer2:Method2 Primer3:Method2 ## 1 1 0 0 0 0 0 ## 2 1 0 0 0 0 0 ## 3 1 0 0 0 0 0 ## 4 1 1 0 0 0 0 ## 5 1 1 0 0 0 0 ## 6 1 1 0 0 0 0 Refit the model: fit2 &lt;- lm(Adhesion ~ Primer + Method, data = paint) coef(fit2) ## (Intercept) Primer2 Primer3 Method2 ## 4.2611111 0.9000000 -0.2833333 1.0444444 anova(fit2) ## Analysis of Variance Table ## ## Response: Adhesion ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Primer 2 4.5811 2.2906 26.119 1.884e-05 *** ## Method 1 4.9089 4.9089 55.975 2.960e-06 *** ## Residuals 14 1.2278 0.0877 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Based on this model, what combination of primer and method are the best to use? fit2 |&gt; broom::augment(paint) |&gt; ggplot(aes(x = Primer, y = Adhesion, col = Method, group = Method)) + geom_point() + geom_line(aes(x = Primer, y = .fitted)) 7.4 Polynomial regression Data on paper strength and percent of hardwood in the pulp batch from package (Arnholt and Evans 2023). load(here(&quot;data&quot;, &quot;hardwood.rda&quot;)) dr |&gt; glimpse() ## Rows: 19 ## Columns: 2 ## $ tensile &lt;dbl&gt; 6.3, 11.1, 20.0, 24.0, 26.1, 30.0, 33.8, 34.0, 38.1, 39.9, 42… ## $ hardwood &lt;dbl&gt; 1.0, 1.5, 2.0, 3.0, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 8.0, 9… dr |&gt; ggplot(aes(x = hardwood, y = tensile)) + geom_point() fit1 &lt;- lm(tensile ~ hardwood, data = dr) summary(fit1) ## ## Call: ## lm(formula = tensile ~ hardwood, data = dr) ## ## Residuals: ## Min 1Q Median 3Q Max ## -25.986 -3.749 2.938 7.675 15.840 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 21.3213 5.4302 3.926 0.00109 ** ## hardwood 1.7710 0.6478 2.734 0.01414 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.82 on 17 degrees of freedom ## Multiple R-squared: 0.3054, Adjusted R-squared: 0.2645 ## F-statistic: 7.474 on 1 and 17 DF, p-value: 0.01414 The straight line does not fit the data. The regression is significant but this does not tell you about goodness of fit. 7.4.1 Model 2: Quadratic regression Include \\(x^2\\) as a predictor. fit2 &lt;- lm(tensile ~ poly(hardwood,2), data = dr) summary(fit2) ## ## Call: ## lm(formula = tensile ~ poly(hardwood, 2), data = dr) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.8503 -3.2482 -0.7267 4.1350 6.5506 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.184 1.014 33.709 2.73e-16 *** ## poly(hardwood, 2)1 32.302 4.420 7.308 1.76e-06 *** ## poly(hardwood, 2)2 -45.396 4.420 -10.270 1.89e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.42 on 16 degrees of freedom ## Multiple R-squared: 0.9085, Adjusted R-squared: 0.8971 ## F-statistic: 79.43 on 2 and 16 DF, p-value: 4.912e-09 Comment: the quadratic model is a better fit. The quadratic term is significant. plot(fit2,1) Note: we can specify this model in two ways: using poly and \\(I(x,2)\\). However, \\(x\\), \\(I(x^2)\\) etc will be correlated and correlated variables can cause problems. The use of poly() lets you avoid this by producing orthogonal polynomials. fit2b &lt;- lm(tensile ~ hardwood + I(hardwood ^2), data = dr) summary(fit2b) ## ## Call: ## lm(formula = tensile ~ hardwood + I(hardwood^2), data = dr) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.8503 -3.2482 -0.7267 4.1350 6.5506 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.67419 3.39971 -1.963 0.0673 . ## hardwood 11.76401 1.00278 11.731 2.85e-09 *** ## I(hardwood^2) -0.63455 0.06179 -10.270 1.89e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.42 on 16 degrees of freedom ## Multiple R-squared: 0.9085, Adjusted R-squared: 0.8971 ## F-statistic: 79.43 on 2 and 16 DF, p-value: 4.912e-09 dr |&gt; ggplot(aes(x = hardwood, y = tensile)) + geom_point() + geom_line(aes(y=fitted(fit2)))+ geom_line(aes(y=fitted(fit2b)), col = 2) 7.4.2 Model 3: Cubic regression fit3 &lt;- lm(tensile ~ poly(hardwood,3), data = dr) summary(fit3) ## ## Call: ## lm(formula = tensile ~ poly(hardwood, 3), data = dr) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.6250 -1.6109 0.0413 1.5892 5.0216 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.1842 0.5931 57.641 &lt; 2e-16 *** ## poly(hardwood, 3)1 32.3021 2.5850 12.496 2.48e-09 *** ## poly(hardwood, 3)2 -45.3963 2.5850 -17.561 2.06e-11 *** ## poly(hardwood, 3)3 -14.5740 2.5850 -5.638 4.72e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.585 on 15 degrees of freedom ## Multiple R-squared: 0.9707, Adjusted R-squared: 0.9648 ## F-statistic: 165.4 on 3 and 15 DF, p-value: 1.025e-11 Comment: the cubic model is a better fit. The quadratic term is significant. plot(fit3,1) dr |&gt; ggplot(aes(x = hardwood, y = tensile)) + geom_point() + geom_line(aes(y=fitted(fit3)), col = 2) 7.4.3 Model 4: Quartic fit4 &lt;- lm(tensile ~ poly(hardwood,4), data = dr) summary(fit4) ## ## Call: ## lm(formula = tensile ~ poly(hardwood, 4), data = dr) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.1384 -1.0550 -0.3203 1.0779 4.5030 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.1842 0.5824 58.696 &lt; 2e-16 *** ## poly(hardwood, 4)1 32.3021 2.5386 12.724 4.39e-09 *** ## poly(hardwood, 4)2 -45.3963 2.5386 -17.883 4.87e-11 *** ## poly(hardwood, 4)3 -14.5740 2.5386 -5.741 5.10e-05 *** ## poly(hardwood, 4)4 -3.1647 2.5386 -1.247 0.233 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.539 on 14 degrees of freedom ## Multiple R-squared: 0.9736, Adjusted R-squared: 0.9661 ## F-statistic: 129.1 on 4 and 14 DF, p-value: 6.994e-11 Comment: the quartic term is not significant. plot(fit4,1) dr |&gt; ggplot(aes(x = hardwood, y = tensile)) + geom_point() + geom_line(aes(y=fitted(fit4)), col = 2) 7.5 Confounding example In autumn, small winged fruit called samara fall off maple trees, spinning as they go. A forest scientist studied the relationship between how fast they fell and their “disk loading” (a quantity based on their size and weight). The data give the loadings and fall velocities for fruit from three trees. From Ryan, Joiner, and Rogosa (1994). Y = Velocity X = Load A = Tree: 3 levels tree &lt;- read.table(here(&quot;data&quot;, &quot;samara.txt&quot;), header=TRUE) tree |&gt; glimpse() ## Rows: 36 ## Columns: 3 ## $ Tree &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2… ## $ Load &lt;dbl&gt; 0.239, 0.208, 0.223, 0.224, 0.246, 0.213, 0.198, 0.219, 0.241… ## $ Velocity &lt;dbl&gt; 1.34, 1.06, 1.14, 1.13, 1.35, 1.23, 1.23, 1.15, 1.25, 1.24, 1… tree$Tree &lt;- as.factor(tree$Tree) tree &lt;- tree[complete.cases(tree),] Some models we could fit: Different intercepts, no slope \\[y = \\beta_0 + \\beta_1z_2 + \\beta_2z_3 + \\epsilon\\] Common slope and intercept \\[y = \\beta_0 + \\beta_1x + \\epsilon\\] Common slope \\[y = \\beta_0 + \\beta_1x + \\beta_2z_2 + \\beta_3z_3 + \\epsilon\\] Separate lines \\[y = \\beta_0 + \\beta_1x + \\beta_2z_2 + \\beta_3z_3 + \\\\ \\beta_4 x \\times z_2 + \\beta_5 x \\times z_3+ \\epsilon\\] Here \\(z_i\\) is an indicator variable for the \\(i\\)th level of the factor A. fit0 &lt;-lm(Velocity~Tree, data=tree) tree |&gt; ggplot(aes(x = Tree, y = Velocity, color = Tree)) + geom_point() + geom_point(aes(y=fitted(fit0)), pch = 3, col= &quot;black&quot;) summary(fit0) ## ## Call: ## lm(formula = Velocity ~ Tree, data = tree) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.300000 -0.082500 0.005833 0.087500 0.250000 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.23417 0.03676 33.571 &lt; 2e-16 *** ## Tree2 -0.05417 0.05316 -1.019 0.316 ## Tree3 -0.28333 0.05199 -5.450 5.37e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1274 on 32 degrees of freedom ## Multiple R-squared: 0.5097, Adjusted R-squared: 0.479 ## F-statistic: 16.63 on 2 and 32 DF, p-value: 1.117e-05 fit1 &lt;- lm(Velocity~Load, data=tree) tree |&gt; ggplot(aes(x = Load, y = Velocity, color = Tree)) + geom_point() + geom_line(aes(y=fitted(fit1))) fit2 &lt;- lm(Velocity~Load+Tree, data=tree) tree |&gt; ggplot(aes(x = Load, y = Velocity, color = Tree)) + geom_point() + geom_line(aes(y=fitted(fit2))) fit3 &lt;- lm(Velocity~Tree * Load, data=tree) tree |&gt; ggplot(aes(x = Load, y = Velocity, color = Tree)) + geom_point() + geom_line(aes(y=fitted(fit3))) From the plots above we can see that the velocity is different between the trees. Particularly, tree 3 seems to have the lowest velocity. We can also see that tree 2 has a wide range of velocities. Is this difference in velocity because of the size of the samara fruits or the tree also affects it? Would we expect a samara of specific load to fall with the same velocity from tree 1 as it would have fallen from tree 3? We can answer these questions by comparing the models above. summary(fit3) ## ## Call: ## lm(formula = Velocity ~ Tree * Load, data = tree) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.120023 -0.049465 -0.001298 0.049938 0.145571 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.5414 0.2632 2.057 0.0488 * ## Tree2 -0.8408 0.3356 -2.505 0.0181 * ## Tree3 -0.2987 0.4454 -0.671 0.5078 ## Load 3.0629 1.1599 2.641 0.0132 * ## Tree2:Load 3.7343 1.5000 2.490 0.0188 * ## Tree3:Load 0.8205 2.2837 0.359 0.7220 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.07554 on 29 degrees of freedom ## Multiple R-squared: 0.8436, Adjusted R-squared: 0.8167 ## F-statistic: 31.29 on 5 and 29 DF, p-value: 7.656e-11 \\(\\beta_5\\) is the difference in the change in mean circumference associated with a 1 unit increase in Load for tree 3 as opposed to tree 1. Write down a 95% confidence interval for \\(\\beta_5\\). What is the estimated change in mean circumference associated with a 1 unit increase in Load for tree 3? 7.5.1 Comparing models 2 and 3: Test \\(H_0: \\beta_4= \\beta_5 = 0\\) versus \\(H_a: \\beta_4, \\beta_5\\) are not both zero. If you do not reject \\(H_0\\), then use model 2 to describe the association between Velocity, Load and tree. anova(fit2, fit3) ## Analysis of Variance Table ## ## Model 1: Velocity ~ Load + Tree ## Model 2: Velocity ~ Tree * Load ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 31 0.20344 ## 2 29 0.16549 2 0.037949 3.325 0.05011 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.5.2 Comparing models 1 and 2: Starting from model 2 as the full model, we can see if model 1 is an appropriate simplification: Test \\(H_0: \\beta_2= \\beta_3 = 0\\) versus \\(H_a: \\beta_2, \\beta_3\\) are not both zero. anova(fit1, fit2) ## Analysis of Variance Table ## ## Model 1: Velocity ~ Load ## Model 2: Velocity ~ Load + Tree ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 33 0.21476 ## 2 31 0.20344 2 0.011322 0.8626 0.4319 In conclusion, we found out that the Tree doesn’t affect the velocity, only the Load does. So for samara fruits of the same Load, the velocity would be the same for every tree. The reason why the velocity in the Tree 3 is smaller is because it has smaller fruits. 7.5.3 Type 1 and type 2 sums of squares anova(fit3) ## Analysis of Variance Table ## ## Response: Velocity ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Tree 2 0.53942 0.269708 47.262 7.488e-10 *** ## Load 1 0.31554 0.315542 55.294 3.406e-08 *** ## Tree:Load 2 0.03795 0.018975 3.325 0.05011 . ## Residuals 29 0.16549 0.005707 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 car::Anova(fit3) ## Anova Table (Type II tests) ## ## Response: Velocity ## Sum Sq Df F value Pr(&gt;F) ## Tree 0.011322 2 0.992 0.38306 ## Load 0.315542 1 55.294 3.406e-08 *** ## Tree:Load 0.037949 2 3.325 0.05011 . ## Residuals 0.165492 29 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.5.4 Another factor Suppose the data was collected over 2 Days, and it was suspected that day might also impact on the relationship between Load and Velocity. tree$Day &lt;- factor(rep(1:2, length=35)) fit4 &lt;- lm(Velocity ~ Tree*Load*Day, data=tree) tree$fit4 &lt;- fitted(fit4) ggplot(tree, aes(x = Load, y = Velocity, color = Tree)) + geom_point()+ facet_wrap(~Day) + stat_smooth(method = &quot;lm&quot;, se = FALSE) # ggplot(tree, aes(x = Load, y = Velocity, color = Tree)) + geom_point()+ facet_wrap(~Day)+ geom_line(aes(y=fit4 )) # the same This model allows for a 3-way interaction between Load, Tree and Day. We can say that the interaction between Tree and Load varies with Day, or equivalently, that the interaction between Load and Day varies with Tree. To assess whether Day may be omitted, compare to fit3: anova(fit3, fit4) ## Analysis of Variance Table ## ## Model 1: Velocity ~ Tree * Load ## Model 2: Velocity ~ Tree * Load * Day ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 29 0.16549 ## 2 23 0.14061 6 0.024887 0.6785 0.6684 7.6 Quadratic terms and interactions Example from Ramsey and Schafer (2002) pg 252. The data on corn yields and rainfall are in `RainfallData.csv’, or library(Sleuth3) in ‘ex0915’. Variables: Yield: corn yield (bushels/acre) Rainfall: rainfall (inches/year) Year: year. Link: http://www.rpubs.com/kdomijan/332466 7.7 An example with two continuous and two categorical predictors FEV data - for a full description see Kahn (2005). Response variable: fev (forced expiratory volume) measures respiratory function. Predictors: age, height, gender and smoke. The dataset is in library(covreg). load(here(&quot;data&quot;, &quot;fev.rda&quot;)) glimpse(fev) ## Rows: 654 ## Columns: 5 ## $ age &lt;int&gt; 9, 8, 7, 9, 9, 8, 6, 6, 8, 9, 6, 8, 8, 8, 8, 7, 5, 6, 9, 9, 5, … ## $ fev &lt;dbl&gt; 1.708, 1.724, 1.720, 1.558, 1.895, 2.336, 1.919, 1.415, 1.987, … ## $ height &lt;dbl&gt; 57.0, 67.5, 54.5, 53.0, 57.0, 61.0, 58.0, 56.0, 58.5, 60.0, 53.… ## $ male &lt;int&gt; 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, … ## $ smoke &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … fev &lt;- fev |&gt; mutate(male = as.factor(male), smoke = as.factor(smoke)) Does smoking affect pulmonary function? From this plot it appers that smoking improves lung function! However, in the plot, smoking is not adjusted for age and body size. Let’s visualize the effect of smoking on fev accounting for age and height: Smokers (blue) tend to have lower fev compared to individuals of same age/height. Younger children are less likely to be smokers, but more likely to have have lower fev than older (and bigger) ones. This is an example of Simpson’s paradox. Height and age are not independent - older children tend to be taller, but this trend tapers off after 14. What about gender? For children older than 10, boys are taller than girls. Males (blue) have higher fev, but this is confounded with height. Let’s fit some models. fit1 &lt;- lm(fev~ age + smoke, data = fev) summary(fit1) ## ## Call: ## lm(formula = fev ~ age + smoke, data = fev) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.68070 -0.35220 -0.04599 0.35034 2.08515 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.349135 0.081787 4.269 2.26e-05 *** ## age 0.232476 0.008223 28.272 &lt; 2e-16 *** ## smoke1 -0.208911 0.080573 -2.593 0.00973 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.564 on 651 degrees of freedom ## Multiple R-squared: 0.5782, Adjusted R-squared: 0.5769 ## F-statistic: 446.1 on 2 and 651 DF, p-value: &lt; 2.2e-16 Age has a positive effect of fev and smoking negative. The plot of age by fev suggested that we should include a smoking by age interaction. Our second model includes interaction, which is shown to be significant: fit2 &lt;- lm(fev~ age * smoke, data = fev) summary(fit2) ## ## Call: ## lm(formula = fev ~ age * smoke, data = fev) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.77573 -0.34712 -0.03269 0.33471 2.05749 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.242056 0.083021 2.916 0.00367 ** ## age 0.243704 0.008371 29.114 &lt; 2e-16 *** ## smoke1 1.904894 0.424926 4.483 8.70e-06 *** ## age:smoke1 -0.159959 0.031594 -5.063 5.38e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5536 on 650 degrees of freedom ## Multiple R-squared: 0.5942, Adjusted R-squared: 0.5923 ## F-statistic: 317.2 on 3 and 650 DF, p-value: &lt; 2.2e-16 Display the fitted model: Does the model fit well? Let’s take a look at the fits vs residuals diagnostic plot (coloured by gender). We see: Increasing variance of the residuals. Gender or height might account for some of this. Remember that there is a relationship between age and height, as well as height and gender. The scatterplots indicated that a quadratic fit might be a good option for height. fit3 &lt;- lm(fev~ height + I(height^2)+ smoke , data = fev) summary(fit3) ## ## Call: ## lm(formula = fev ~ height + I(height^2) + smoke, data = fev) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.80474 -0.23068 -0.00234 0.21778 1.99269 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.0404522 1.5047762 4.014 6.66e-05 *** ## height -0.2507239 0.0499314 -5.021 6.63e-07 *** ## I(height^2) 0.0031632 0.0004121 7.676 6.04e-14 *** ## smoke1 -0.0185929 0.0563310 -0.330 0.741 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.413 on 650 degrees of freedom ## Multiple R-squared: 0.7741, Adjusted R-squared: 0.7731 ## F-statistic: 742.6 on 3 and 650 DF, p-value: &lt; 2.2e-16 We see that: Quadratic term is significant Smoking is no longer significant Interaction with height? Display the fitted model: Let’s consider another, more complex model: fit4 &lt;- lm(fev~ male+ height + I(height^2) + age + smoke, data = fev) summary(fit4) ## ## Call: ## lm(formula = fev ~ male + height + I(height^2) + age + smoke, ## data = fev) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.61264 -0.22793 0.00617 0.22435 1.80390 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.8630488 1.4990763 4.578 5.62e-06 *** ## male1 0.0945539 0.0328676 2.877 0.00415 ** ## height -0.2732761 0.0496785 -5.501 5.44e-08 *** ## I(height^2) 0.0031165 0.0004086 7.628 8.54e-14 *** ## age 0.0699792 0.0091943 7.611 9.62e-14 *** ## smoke1 -0.1325347 0.0570996 -2.321 0.02059 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3951 on 648 degrees of freedom ## Multiple R-squared: 0.7939, Adjusted R-squared: 0.7923 ## F-statistic: 499.2 on 5 and 648 DF, p-value: &lt; 2.2e-16 Does the model fit well? The points below are coloured by gender. We see that even with such a small dataset, many models are possible. One can consider adding interactions to the last model above. It can be helpful to think of relationships between variables in a structured way. This is sometimes set out in a directed graph (causal diagram). One could argue that height is a consequence of smoke (unlike confounders age and sex). But including height in our model (adjusting for height) we might get a downward biased (incorrect) estimate of the effect of smoking. For an indepth discussion of causal analysis of this datatset see Cummiskey et al. (2020). Without height: fit5 &lt;- lm(fev~ male*smoke+age*smoke, data = fev) summary(fit5) ## ## Call: ## lm(formula = fev ~ male * smoke + age * smoke, data = fev) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.87731 -0.33574 -0.02565 0.30803 1.93283 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.123763 0.081090 1.526 0.12744 ## male1 0.278636 0.043524 6.402 2.95e-10 *** ## smoke1 2.065510 0.404618 5.105 4.36e-07 *** ## age 0.240731 0.007974 30.188 &lt; 2e-16 *** ## male1:smoke1 0.456419 0.141805 3.219 0.00135 ** ## smoke1:age -0.181915 0.030397 -5.985 3.59e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5265 on 648 degrees of freedom ## Multiple R-squared: 0.6341, Adjusted R-squared: 0.6312 ## F-statistic: 224.6 on 5 and 648 DF, p-value: &lt; 2.2e-16 References Arnholt, Alan T., and Ben Evans. 2023. BSDA: Basic Statistics and Data Analysis. https://doi.org/10.32614/CRAN.package.BSDA. Cummiskey, Kevin, Bryan Adams, James Pleuss, Dusty Turner, Nicholas Clark, and Krista Watts. 2020. “Causal Inference in Introductory Statistics Courses.” Journal of Statistics Education 28 (1): 2–8. https://doi.org/10.1080/10691898.2020.1713936. Kahn, Michael. 2005. “An Exhalent Problem for Teaching Statistics.” Journal of Statistics Education 13 (2). https://doi.org/10.1080/10691898.2005.11910559. Ramsey, Fred, and Daniel Schafer. 2002. The Statistical Sleuth: A Course in Methods of Data Analysis. 2nd ed. Duxbury Press. Ryan, B. F., B. L. Joiner, and D. Rogosa. 1994. Minitab Handbook 3rd edition. Duxbury Press. "],["about.html", "Chapter 8 About 8.1 License 8.2 Citation", " Chapter 8 About 8.1 License This book is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License https://creativecommons.org/licenses/by-nc-sa/4.0/. 8.2 Citation A BibTeX entry for LaTeX users is: @Manual{, title = {Notes for ST463/ST683 Linear Models 1}, author = {Katarina Domijan, Catherine Hurley}, year = {2018}, organization = {Department of Mathematics and Statistics, Maynooth University}, address = {Maynooth, Ireland}, url = {https://bookdown.org/domijank/ST463/}, doi = {10.5281/zenodo.17945213}, } The following packages were used in creating these notes: ## [1] &quot;base (4.5.1)&quot; &quot;bookdown (0.46)&quot; &quot;broom (1.0.11)&quot; ## [4] &quot;car (3.1.3)&quot; &quot;carData (3.0.5)&quot; &quot;condvis2 (0.1.2)&quot; ## [7] &quot;datasauRus (0.1.9)&quot; &quot;datasets (4.5.1)&quot; &quot;distributional (0.5.0)&quot; ## [10] &quot;dplyr (1.1.4)&quot; &quot;forcats (1.0.1)&quot; &quot;GGally (2.4.0)&quot; ## [13] &quot;ggdist (3.3.3)&quot; &quot;ggplot2 (4.0.1)&quot; &quot;graphics (4.5.1)&quot; ## [16] &quot;grDevices (4.5.1)&quot; &quot;gridExtra (2.3)&quot; &quot;here (1.0.2)&quot; ## [19] &quot;knitr (1.50)&quot; &quot;lubridate (1.9.4)&quot; &quot;MASS (7.3.65)&quot; ## [22] &quot;methods (4.5.1)&quot; &quot;purrr (1.2.0)&quot; &quot;readr (2.1.6)&quot; ## [25] &quot;rosdata (0.1.1)&quot; &quot;scatterplot3d (0.3.44)&quot; &quot;Sleuth3 (1.0.6)&quot; ## [28] &quot;stats (4.5.1)&quot; &quot;stringr (1.6.0)&quot; &quot;tibble (3.3.0)&quot; ## [31] &quot;tidyr (1.3.1)&quot; &quot;tidyverse (2.0.0)&quot; &quot;utils (4.5.1)&quot; ## [34] &quot;xtable (1.8.4)&quot; "],["references.html", "References", " References Allaire, JJ, Yihui Xie, Christophe Dervieux, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, et al. 2025. Rmarkdown: Dynamic Documents for r. https://github.com/rstudio/rmarkdown. Arnholt, Alan T., and Ben Evans. 2023. BSDA: Basic Statistics and Data Analysis. https://doi.org/10.32614/CRAN.package.BSDA. Bowerman, Bruce L., and Daniel Schafer. 1990. Linear Statistical Models. 2nd ed. Thomson Wadsworth. Cryer, J. D., and R. B. Miller. 1991. Statistics for Business: Data Analysis and Modelling. Business Statistics. PWS-Kent. Cummiskey, Kevin, Bryan Adams, James Pleuss, Dusty Turner, Nicholas Clark, and Krista Watts. 2020. “Causal Inference in Introductory Statistics Courses.” Journal of Statistics Education 28 (1): 2–8. https://doi.org/10.1080/10691898.2020.1713936. Draper, Norman Richard, and Harry Smith. 1966. Applied Regression Analysis. Wiley Series in Probability and Mathematical Statistics. Wiley. F. L. Ramsey, Original by, D. W. Schafer; modifications by Daniel W. Schafer, Jeannie Sifneos, Berwin A. Turlach; vignettes contributed by Nicholas Horton, Linda Loi, Kate Aloisio, Ruobing Zhang, and with corrections by Randall Pruim. 2016. Sleuth3: Data Sets from Ramsey and Schafer’s \"Statistical Sleuth (3rd Ed)\". https://CRAN.R-project.org/package=Sleuth3. Fox, John. 2005. Regression diagnostics. Sage Publishers. ———. 2016. Applied Regression Analysis and Generalized Linear Models. 3rd ed. SAGE Publications. Fox, John, and Sanford Weisberg. 2011. An R Companion to Applied Regression. Second. Thousand Oaks CA: Sage. http://socserv.socsci.mcmaster.ca/jfox/Books/Companion. Fox, John, Sanford Weisberg, and Brad Price. 2022. carData: Companion to Applied Regression Data Sets. https://doi.org/10.32614/CRAN.package.carData. Gelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Analytical Methods for Social Research. Cambridge University Press. https://doi.org/10.1017/9781139161879. ———. 2025. Rosdata: Data for Regression and Other Stories. https://github.com/avehtari/ROS-Examples. Hurley, Catherine B., Mark O’Connell, and Katarina Domijan. 2022. “Interactive Slice Visualization for Exploring Machine Learning Models.” Journal of Computational and Graphical Statistics 31 (1): 1–13. https://doi.org/10.1080/10618600.2021.1983439. Hurley, Catherine, Mark OConnell, and Katarina Domijan. 2022. Condvis2: Interactive Conditional Visualization for Supervised and Unsupervised Models in Shiny. https://doi.org/10.32614/CRAN.package.condvis2. Kahn, Michael. 2005. “An Exhalent Problem for Teaching Statistics.” Journal of Statistics Education 13 (2). https://doi.org/10.1080/10691898.2005.11910559. McIntyre, Lauren. 1994. “Using Cigarette Data for an Introduction to Multiple Regression.” Journal of Statistics Education 2 (1): null. https://doi.org/10.1080/10691898.1994.11910468. Pearson, K, and A Lee. 1903. “On the Laws of Inheritance in Man: I. Inheritance of Physical Characters.” Biometrika 2: 357–462. Ramsey, Fred, and Daniel Schafer. 2002. The Statistical Sleuth: A Course in Methods of Data Analysis. 2nd ed. Duxbury Press. Robinson, David, Alex Hayes, and Simon Couch. 2022. Broom: Convert Statistical Objects into Tidy Tibbles. https://CRAN.R-project.org/package=broom. Rodrı́guez, G. 2007. “Lecture Notes on Generalized Linear Models.” http://data.princeton.edu/wws509/notes/. Ryan, B. F., B. L. Joiner, and D. Rogosa. 1994. Minitab Handbook 3rd edition. Duxbury Press. Stapleton, James H. 2009. Linear Statistical Models. 2nd ed. Wiley Series in Probability and Statistics. Wiley. Venables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with s. Fourth. New York: Springer. http://www.stats.ox.ac.uk/pub/MASS4. Weisberg, Sanford. 2005. Applied linear regression. Wiley-Blackwell. Wickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. http://ggplot2.org. Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686. Wickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2022. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr. Xie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.name/knitr/. ———. 2018. Bookdown: Authoring Books and Technical Documents with r Markdown. https://CRAN.R-project.org/package=bookdown. Xie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. R Markdown: The Definitive Guide. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown. Xie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. R Markdown Cookbook. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown-cookbook. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
