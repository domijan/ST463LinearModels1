<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 3 Simple Linear regression | Linear Models Lecture Notes</title>
<meta name="author" content="Katarina Domijan">
<meta name="description" content="3.1 Ordinary least squares We have seen some introductory examples in Section 2.2.5. Fuel consumption example, what is the `best fitting line’ to summarise the linear trend? \[y_i =\beta_{0} +...">
<meta name="generator" content="bookdown 0.46 with bs4_book()">
<meta property="og:title" content="Chapter 3 Simple Linear regression | Linear Models Lecture Notes">
<meta property="og:type" content="book">
<meta property="og:description" content="3.1 Ordinary least squares We have seen some introductory examples in Section 2.2.5. Fuel consumption example, what is the `best fitting line’ to summarise the linear trend? \[y_i =\beta_{0} +...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 3 Simple Linear regression | Linear Models Lecture Notes">
<meta name="twitter:description" content="3.1 Ordinary least squares We have seen some introductory examples in Section 2.2.5. Fuel consumption example, what is the `best fitting line’ to summarise the linear trend? \[y_i =\beta_{0} +...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
          margin-bottom: 0em;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Linear Models Lecture Notes</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Module Preliminaries</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">2</span> Introduction</a></li>
<li><a class="active" href="SLR.html"><span class="header-section-number">3</span> Simple Linear regression</a></li>
<li><a class="" href="multiple-regression.html"><span class="header-section-number">4</span> Multiple regression</a></li>
<li><a class="" href="model-comparisons-and-testing-for-lack-of-fit.html"><span class="header-section-number">5</span> Model comparisons and testing for lack of fit</a></li>
<li><a class="" href="diagnostic-methods-in-more-details.html"><span class="header-section-number">6</span> Diagnostic methods (in more details)</a></li>
<li><a class="" href="special-cases-of-multiple-regression.html"><span class="header-section-number">7</span> Special cases of multiple regression</a></li>
<li><a class="" href="about.html"><span class="header-section-number">8</span> About</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="SLR" class="section level1" number="3">
<h1>
<span class="header-section-number">3</span> Simple Linear regression<a class="anchor" aria-label="anchor" href="#SLR"><i class="fas fa-link"></i></a>
</h1>
<div id="OLS" class="section level2" number="3.1">
<h2>
<span class="header-section-number">3.1</span> Ordinary least squares<a class="anchor" aria-label="anchor" href="#OLS"><i class="fas fa-link"></i></a>
</h2>
<p>We have seen some introductory examples in Section <a href="intro.html#fuel">2.2.5</a>.</p>
<p>Fuel consumption example, what is the `best fitting line’ to summarise the linear trend?
<span class="math display">\[y_i =\beta_{0} + \beta_{1}x_i + \epsilon_i.\]</span></p>
<p>The method of ordinary least squares chooses <span class="math inline">\(\beta_{0}\)</span>, <span class="math inline">\(\beta_{1}\)</span> to minimise:</p>
<p><span class="math display">\[\begin{align*}
S(\beta_{0}, \beta_{1}) &amp; =\sum_{i=1}^{n}\epsilon_i^2 \\
&amp; = \sum_{i=1}^{n}(y_i-\beta_0-\beta_1x_i)^2\\
\end{align*}\]</span></p>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-22-1.png" width="432" style="display: block; margin: auto;"></div>
<p>The least squares estimators <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> must satisfy: <span class="math inline">\(\frac{\delta S}{\delta \beta_0} = 0\)</span> and <span class="math inline">\(\frac{\delta S}{\delta \beta_1} = 0\)</span>.</p>
<p><span class="math display">\[\begin{align*}
\frac{\delta S}{\delta \beta_0} &amp; = - 2\sum_{i=1}^{n} (y_i-\beta_0-\beta_1x_i) \\
\frac{\delta S}{\delta \beta_1} &amp; = - 2\sum_{i=1}^{n} x_i(y_i-\beta_0-\beta_1x_i).
\end{align*}\]</span></p>
<p>Setting these to 0 at <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> gives:</p>
<p><span class="math display" id="eq:normal2">\[\begin{align}
\sum_{i=1}^{n} (y_i-\hat{\beta}_0-\hat{\beta}_1x_i) &amp; =0 \tag{3.1}\\
\sum_{i=1}^{n} x_i(y_i-\hat{\beta}_0-\hat{\beta}_1x_i)&amp; =0. \tag{3.2}
\end{align}\]</span>
These equations (<a href="SLR.html#eq:normal1">(3.1)</a> and <a href="SLR.html#eq:normal2">(3.2)</a>) are called the <strong>normal equations</strong>.
From <a href="SLR.html#eq:normal1">(3.1)</a>:</p>
<p><span class="math display">\[\begin{align*}
\sum_{i=1}^{n} y_i-n\hat{\beta}_0-\hat{\beta}_1\sum_{i=1}^{n}x_i &amp; =0\\
\hat{\beta}_0&amp; =\bar{y}-\hat{\beta}_1\bar{x}.
\end{align*}\]</span></p>
<p>Substitute into <a href="SLR.html#eq:normal2">(3.2)</a>:</p>
<p><span class="math display">\[\begin{align*}
\sum_{i=1}^{n}x_i( y_i-\bar{y}+\hat{\beta}_1\bar{x}-\hat{\beta}_1x_i) &amp; =0\\
\sum_{i=1}^{n}x_i( y_i-\bar{y}) &amp; =\hat{\beta}_1\sum_{i=1}^{n}x_i(x_i-\bar{x})\\
\hat{\beta}_1&amp; = \frac{\sum_{i=1}^{n}x_i( y_i-\bar{y})}{\sum_{i=1}^{n}x_i(x_i-\bar{x})}\\
&amp; =\frac{\sum_{i=1}^{n}(x_i-\bar{x})( y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}\\
&amp; =\frac{S_{xy}}{S_{xx}}.
\end{align*}\]</span></p>
<p>Some notation:
<span class="math display">\[\begin{align*}
S_{xx} &amp; =\sum_{i=1}^{n}(x_{i} - \bar{x})^{2} =\sum_{i=1}^{n}x_{i}^{2} - n\bar{x}^2 \\
S_{yy} &amp; =\sum_{i=1}^{n}(y_{i} - \bar{y})^{2}   =\sum_{i=1}^{n}y_{i}^{2} - n\bar{y}^2 \\
S_{xy} &amp; =\sum_{i=1}^{n}(x_{i} - \bar{x})(y_{i} - \bar{y}) = \sum_{i=1}^{n}x_{i}y_{i} - n\bar{x}\bar{y}
\end{align*}\]</span></p>
<p>So, the equation of the OLS fitted line is given by:
<span class="math display">\[\hat{y} =\hat{\beta}_{0} + \hat{\beta}_{1}x,\]</span></p>
<p>where <span class="math display">\[\hat{\beta}_{1} = \frac{S_{xy}}{S_{xx}}\]</span>
and <span class="math display">\[\hat{\beta}_{0} = \bar{y}-\hat{\beta}_1\bar{x}.\]</span></p>
<div id="residuals" class="section level3" number="3.1.1">
<h3>
<span class="header-section-number">3.1.1</span> Residuals<a class="anchor" aria-label="anchor" href="#residuals"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>fitted value</strong> at each observation is:
<span class="math display">\[\hat{y}_i =\hat{\beta}_{0} + \hat{\beta}_{1}x_i\]</span></p>
<p>The <strong>residuals</strong> are computed as:
<span class="math display">\[e_i = y_i-\hat{y}_i\]</span></p>
</div>
<div id="some-algebraic-implications-of-the-ols-fit" class="section level3" number="3.1.2">
<h3>
<span class="header-section-number">3.1.2</span> Some algebraic implications of the OLS fit<a class="anchor" aria-label="anchor" href="#some-algebraic-implications-of-the-ols-fit"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\sum_{i=1}^n e_i = \sum_{i=1}^n (y_i - \hat{y}_i) = 0\)</span> (residuals sum to 0)</p></li>
<li><p><span class="math inline">\(\sum_{i=1}^n x_i e_i = \sum_{i=1}^n x_i(y_i - \hat{y}_i) = 0\)</span></p></li>
<li><p><span class="math inline">\(\sum_{i=1}^n y_i = \sum_{i=1}^n \hat{y}_i\)</span> (from <a href="SLR.html#eq:normal1">(3.1)</a>)</p></li>
<li><p><span class="math inline">\(\bar{y} = \hat{\beta}_0+\hat{\beta}_1\bar{x}\)</span> (OLS line always goes through the mean of the sample)</p></li>
<li><p><span class="math inline">\(\sum_{i=1}^n \hat{y}_ie_i  = 0\)</span> (from <a href="SLR.html#eq:normal1">(3.1)</a> and <a href="SLR.html#eq:normal2">(3.2)</a>).</p></li>
</ol>
</div>
<div id="ols-estimates-for-the-fuel-consumption-example" class="section level3" number="3.1.3">
<h3>
<span class="header-section-number">3.1.3</span> OLS Estimates for the Fuel Consumption Example<a class="anchor" aria-label="anchor" href="#ols-estimates-for-the-fuel-consumption-example"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span> <span class="op">&lt;-</span> <span class="va">Temp</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">Fuel</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">8</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, xsq <span class="op">=</span> <span class="va">x</span><span class="op">^</span><span class="fl">2</span>, ysq <span class="op">=</span> <span class="va">y</span><span class="op">^</span><span class="fl">2</span>, xy <span class="op">=</span> <span class="va">x</span> <span class="op">*</span> <span class="va">y</span><span class="op">)</span></span></code></pre></div>
<pre><code>##         x    y     xsq    ysq     xy
## [1,] 28.0 12.4  784.00 153.76 347.20
## [2,] 28.0 11.7  784.00 136.89 327.60
## [3,] 32.5 12.4 1056.25 153.76 403.00
## [4,] 39.0 10.8 1521.00 116.64 421.20
## [5,] 45.9  9.4 2106.81  88.36 431.46
## [6,] 57.8  9.5 3340.84  90.25 549.10
## [7,] 58.1  8.0 3375.61  64.00 464.80
## [8,] 62.5  7.5 3906.25  56.25 468.75</code></pre>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 351.8</code></pre>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 81.7</code></pre>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 43.975</code></pre>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 10.2125</code></pre>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">x</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 16874.76</code></pre>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">y</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 859.91</code></pre>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">x</span> <span class="op">*</span> <span class="va">y</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 3413.11</code></pre>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Sxx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">x</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">-</span> <span class="va">n</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span></span>
<span><span class="va">Sxx</span></span></code></pre></div>
<pre><code>## [1] 1404.355</code></pre>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Syy</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">y</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">-</span> <span class="va">n</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span></span>
<span><span class="va">Syy</span></span></code></pre></div>
<pre><code>## [1] 25.54875</code></pre>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Sxy</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">x</span> <span class="op">*</span> <span class="va">y</span><span class="op">)</span> <span class="op">-</span> <span class="va">n</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span><span class="va">Sxy</span></span></code></pre></div>
<pre><code>## [1] -179.6475</code></pre>
<p>Calculate <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\hat{\beta}_{1} &amp; = \frac{S_{xy}}{S_{xx}} =   \frac{\sum_{i=1}^{n}x_{i}y_{i} - n\bar{x}\bar{y}}{\sum_{i=1}^{n}x_{i}^{2} - n\bar{x}^{2}} \\
&amp; =\frac{-179.65}{1404.355} = -0.128
\end{align*}\]</span></p>
<p><span class="math inline">\(\hat{\beta}_{0} = \bar{y} - \hat{\beta}_{1}\bar{x} = 10.212 - ( - 0.128)(43.98) =15.84\)</span></p>
<p>The equation of the fitted line is <span class="math inline">\(\hat{y}= 15.84 - 0.128 x\)</span>.</p>
</div>
<div id="interpretation-of-the-fitted-simple-linear-regression-line-parameter-estimates" class="section level3" number="3.1.4">
<h3>
<span class="header-section-number">3.1.4</span> Interpretation of the fitted simple linear regression line: Parameter estimates<a class="anchor" aria-label="anchor" href="#interpretation-of-the-fitted-simple-linear-regression-line-parameter-estimates"><i class="fas fa-link"></i></a>
</h3>
<p>-0.128 is the estimated change in mean fuel use for a 1<span class="math inline">\(^oF\)</span> increase in temperature.</p>
<p>In theory, 15.84 is the estimated mean fuel use at a temperature of 0<span class="math inline">\(^oF\)</span>.</p>
<p>However, we have no reason to believe this is a good estimate because our data contains no information about the fuel-temperature relationship below 28<span class="math inline">\(^oF\)</span>.</p>
</div>
<div id="predicting" class="section level3" number="3.1.5">
<h3>
<span class="header-section-number">3.1.5</span> Predicting<a class="anchor" aria-label="anchor" href="#predicting"><i class="fas fa-link"></i></a>
</h3>
<p>The fitted line allows us to predict fuel use at any temperature within the range of the data.</p>
<p>For example, at <span class="math inline">\(x=30^oF\)</span>:
<span class="math display">\[\hat{y}_i = 15.84 - 0.128 \times 30 = 12.\]</span>
12 units of fuel is the estimated fuel use at <span class="math inline">\(30^oF\)</span>.</p>
<p>E.g. at <span class="math inline">\(x=40\)</span>; <span class="math inline">\(\hat{y} = 10.721\)</span>, at <span class="math inline">\(x=50\)</span>; <span class="math inline">\(\hat{y} = 9.442\)</span>.</p>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-24-1.png" width="288" style="display: block; margin: auto;"></div>
</div>
</div>
<div id="the-formal-simple-linear-regression-model" class="section level2" number="3.2">
<h2>
<span class="header-section-number">3.2</span> The formal simple linear regression model<a class="anchor" aria-label="anchor" href="#the-formal-simple-linear-regression-model"><i class="fas fa-link"></i></a>
</h2>
<p>The SLR model tries to capture two features:</p>
<ul>
<li><p>a linear trend and</p></li>
<li>
<p>fluctuations (scatter about that trend).</p>
<p><!-- In using $\hat{y}_i$ to predict the $y_i$ we make a prediction error: $e_{i}=y_i-\hat{y}_i$. This is the vertical line from the fitted line to the data point $(x_i, y_i)$. Remember that least squares criterion chooses the values of the parameters to minimize the sum of squared prediction errors. --></p>
</li>
</ul>
<p>Because of random variations in experimental conditions we do not expect to get the same value of <span class="math inline">\(y\)</span> even if we keep repeating the experiment at various fixed <span class="math inline">\(x\)</span> values.</p>
<p>SLR model tries to model the scatter about the regression line. We will have to make some assumptions about the behaviour of these chance fluctuations.</p>
<div id="model" class="section level3" number="3.2.1">
<h3>
<span class="header-section-number">3.2.1</span> Model<a class="anchor" aria-label="anchor" href="#model"><i class="fas fa-link"></i></a>
</h3>
<p>The SLR model is of the form:</p>
<p><span class="math display">\[y_{i} = \beta_{0} + \beta_{1}x_{i} + \epsilon_{i}, \hspace{0.5cm}  \epsilon_{i} \sim N(0, \sigma^{2}), \hspace{0.5cm} i=1,...,n. \]</span></p>
<ul>
<li><p><span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are unknown parameters</p></li>
<li><p><span class="math inline">\(y\)</span> and <span class="math inline">\(\epsilon\)</span> are random</p></li>
<li><p><span class="math inline">\(x\)</span> is assumed non-random</p></li>
</ul>
<p>We use errors <span class="math inline">\(\epsilon_{i}\)</span> to model the chance fluctuations about the regression line (i.e. the underlying true line).</p>
<p>So the SLR model assumes that these errors, i.e. vertical distances from the observed point to the regression line, are, on average, equal to zero. It also assumes that they are normally distributed.</p>
<p>Another assumption is that the <span class="math inline">\(\epsilon_{i}\)</span> values are <strong>independent</strong> and <strong>identically distributed</strong> (IID).</p>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-25-1.png" width="768" style="display: block; margin: auto;"></div>
</div>
<div id="assumptions" class="section level3" number="3.2.2">
<h3>
<span class="header-section-number">3.2.2</span> Assumptions<a class="anchor" aria-label="anchor" href="#assumptions"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p><span class="math inline">\(\mathbb{E}[\epsilon_{i}] = 0\)</span>, so <span class="math inline">\(\mathbb{E}[y_{i}] = \beta_0 + \beta_1x_i+ \mathbb{E}[\epsilon_i] = \beta_0 + \beta_1x_i\)</span>.</p></li>
<li><p>Var(<span class="math inline">\(\epsilon_i\)</span>) = <span class="math inline">\(\sigma^2\)</span>. Equivalently Var(<span class="math inline">\(y_{i}\)</span>) = <span class="math inline">\(\sigma^2\)</span>.</p></li>
<li><p><span class="math inline">\(\epsilon_i\)</span> are independent (therefore <span class="math inline">\(y_{i}\)</span> also are).</p></li>
<li><p><span class="math inline">\(\epsilon_i \sim N(0, \sigma^2)\)</span>. Equivalently <span class="math inline">\(y_i \sim N(\beta_0 + \beta_1x_i, \sigma^2)\)</span>.</p></li>
</ul>
<p>NOTE: if <span class="math inline">\(x_i\)</span> are random then the model says that
<span class="math inline">\(\mathbb{E}[y_{i}|x_i] = \beta_0 + \beta_1x_i\)</span> and Var(<span class="math inline">\(y_{i}|x_i\)</span>) = <span class="math inline">\(\sigma^2\)</span>.</p>
</div>
<div id="estimation-of-sigma2" class="section level3" number="3.2.3">
<h3>
<span class="header-section-number">3.2.3</span> Estimation of <span class="math inline">\(\sigma^2\)</span><a class="anchor" aria-label="anchor" href="#estimation-of-sigma2"><i class="fas fa-link"></i></a>
</h3>
<p>NOTE: <span class="math inline">\(\sigma^2\)</span> = Var(<span class="math inline">\(\epsilon_i\)</span>)</p>
<p>The <strong>errors</strong> <span class="math inline">\(\epsilon_i\)</span> are not observable, but the <strong>residuals</strong>, <span class="math inline">\(e_i\)</span> should have similar properties.</p>
<p>We estimate <span class="math inline">\(\sigma^2\)</span> by</p>
<p><span class="math display">\[\hat{\sigma}^2 = \frac{\sum_{i=1}^n e_i^2}{n-2}.\]</span></p>
<p><span class="math inline">\(n-2\)</span> is the degrees of freedom and <span class="math inline">\(\sum_{i=1}^n e_i^2\)</span> is called the residual sum of squares, denoted <span class="math inline">\(\mbox{SSE}\)</span>.</p>
</div>
<div id="review-of-some-probability-results" class="section level3" number="3.2.4">
<h3>
<span class="header-section-number">3.2.4</span> Review of some probability results<a class="anchor" aria-label="anchor" href="#review-of-some-probability-results"><i class="fas fa-link"></i></a>
</h3>
<p>Let <span class="math inline">\(U\)</span>, <span class="math inline">\(W\)</span> and <span class="math inline">\(Z\)</span> be three random variables:</p>
<p><span class="math inline">\(\mathbb{E}[U]\)</span> = mean of the distribution of <span class="math inline">\(U\)</span></p>
<p>Var <span class="math inline">\((U) = \mathbb{E}[U^2] - (\mathbb{E}[U])^2\)</span></p>
<p>Cov(<span class="math inline">\(U,U\)</span>) = Var(<span class="math inline">\(U\)</span>)</p>
<p>Cov(<span class="math inline">\(U, W\)</span>) = <span class="math inline">\(\mathbb{E}[UW] - \mathbb{E}[U]\mathbb{E}[W]\)</span></p>
<p>If <span class="math inline">\(U\)</span> and <span class="math inline">\(W\)</span> are uncorrelated or independent then Cov(<span class="math inline">\(U,W\)</span>) = 0</p>
<p><span class="math inline">\(\mbox{Corr}(U,W) = \frac{\mbox{Cov}(U,W)}{\sqrt{\mbox{Var}(U)\mbox{Var}(W)}}\)</span></p>
<p>For constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>:</p>
<p><span class="math inline">\(\mathbb{E}[aU+bW] = a\mathbb{E}[U] + b\mathbb{E}[W]\)</span></p>
<p>Var(<span class="math inline">\(aU \pm bW\)</span>) = <span class="math inline">\(a^2\)</span>Var[<span class="math inline">\(U\)</span>] + <span class="math inline">\(b^2\)</span>Var[<span class="math inline">\(W\)</span>] <span class="math inline">\(\pm\)</span> <span class="math inline">\(2ab\)</span>Cov(<span class="math inline">\(U\)</span>,<span class="math inline">\(W\)</span>)</p>
<p>Cov(<span class="math inline">\(aU+bW, cZ\)</span>) = <span class="math inline">\(ac\)</span>Cov(<span class="math inline">\(U\)</span>,<span class="math inline">\(Z\)</span>) + <span class="math inline">\(bc\)</span>Cov(<span class="math inline">\(W\)</span>,<span class="math inline">\(Z\)</span>)</p>
</div>
<div id="prop" class="section level3" number="3.2.5">
<h3>
<span class="header-section-number">3.2.5</span> Properties of the estimates<a class="anchor" aria-label="anchor" href="#prop"><i class="fas fa-link"></i></a>
</h3>
<p>When the model holds:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\hat{\beta}_1 \sim N\left(\beta_1,\frac{\sigma^2}{S_{xx}}\right)\)</span></p></li>
<li><p><span class="math inline">\(\hat{\beta}_0 \sim N\left(\beta_0, \sigma^2\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}\right)\right)\)</span></p></li>
<li><p>Cov<span class="math inline">\((\hat{\beta}_0, \hat{\beta}_1) = -\sigma^2\frac{\bar{x}}{S_{xx}}\)</span></p></li>
<li><p><span class="math inline">\(\hat{y} \sim N\left(\beta_0 + \beta_1x, \sigma^2\left(\frac{1}{n}+ \frac{(x-\bar{x})^2}{S_{xx}}\right)\right)\)</span></p></li>
<li><p><span class="math inline">\((n-2)\frac{\hat{\sigma}^2}{\sigma^2} \sim \chi^2_{(n-2)}\)</span></p></li>
<li><p><span class="math inline">\(\mathbb{E}[\hat{\sigma}^2] = \sigma^2\)</span></p></li>
</ol>
<p>Proof of (1):</p>
<p>First show that <span class="math inline">\(\mathbb{E}[\hat{\beta}_1] = \beta_1\)</span>.</p>
<p><span class="math display">\[\begin{align*}
\hat{\beta}_1 &amp; = \frac{S_{xy}}{S_{xx}} \\
&amp; = \frac{\sum_{i=1}^{n}(x_i-\bar{x})( y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}  \\
&amp; = \frac{\sum_{i=1}^{n}(x_i-\bar{x})y_i}{\sum_{i=1}^{n}(x_i-\bar{x})^2}  \\
&amp; =  \sum_{i = 1}^n a_iy_i
\end{align*}\]</span></p>
<!-- %\mbox{ (since} \sum_{i=1}^{n}(x_i-\bar{x}) = 0) -->
<p>where <span class="math inline">\(a_i\)</span> depend only on <span class="math inline">\(x\)</span> and are NOT random.</p>
<p>By linearity of expectation:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[\hat{\beta}_1] &amp; = \sum_{i = 1}^n a_i\mathbb{E}[y_i]\\
&amp; = \sum_{i = 1}^n a_i (\beta_0+\beta_1x_i) \mbox{  (from the model assumptions)}\\
&amp; = \beta_0\sum_{i = 1}^n a_i+\beta_1\sum_{i = 1}^n a_i x_i
\end{align*}\]</span></p>
<p>But</p>
<p><span class="math display">\[\sum_{i = 1}^n a_i = \frac{\sum_{i=1}^{n}(x_i-\bar{x})}{\sum_{i=1}^{n}(x_i-\bar{x})^2} = 0,\]</span></p>
<p>And</p>
<p><span class="math display">\[\begin{align*}
\sum_{i = 1}^n a_i x_i &amp; =  \frac{\sum_{i=1}^{n}x_i(x_i-\bar{x})}{\sum_{i=1}^{n}(x_i-\bar{x})^2} \\
&amp; =  \frac{\sum_{i=1}^{n}x_i^2-n\bar{x}^2}{S_{xx}} \\
&amp; = \frac{S_{xx}}{S_{xx}} = 1.
\end{align*}\]</span></p>
<p>So <span class="math inline">\(\mathbb{E}[\hat{\beta}_1]  = \beta_1\)</span> as required.</p>
<p>Second, show that Var(<span class="math inline">\(\hat{\beta}_1\)</span>) = <span class="math inline">\(\frac{\sigma^2}{S_{xx}}\)</span></p>
<p><span class="math display">\[\begin{align*}
\mbox{Var}(\hat{\beta}_1) &amp; =  \mbox{Var} \left( \sum_{i = 1}^n a_i y_i \right)\\
&amp; =  \sum_{i = 1}^n a_i^2 \mbox{Var}(y_i) \mbox{(since $y_i$s are independent)}\\
  &amp; =  \sigma^2 \sum_{i = 1}^n a_i^2\\
  &amp; =  \sigma^2 \sum_{i = 1}^n \left( \frac{x_i-\bar{x}}{\sum_{i=1}^{n}(x_i-\bar{x})^2} \right)^2\\
&amp; =  \sigma^2  \frac{\sum_{i = 1}^n (x_i-\bar{x})^2}{(\sum_{i=1}^{n}(x_i-\bar{x})^2)^2} \\
  &amp; =  \sigma^2  \frac{S_{xx}}{(S_{xx})^2} \\
  &amp; =   \frac{ \sigma^2}{S_{xx}} \mbox{   (as required)}
\end{align*}\]</span></p>
<p>Finally, the normality assumption follows as <span class="math inline">\(\hat{\beta}_1\)</span> is a linear combination of normal random variables (<span class="math inline">\(y_i\)</span>s).</p>
<p>Proof of (2):</p>
<p>First show that <span class="math inline">\(\mathbb{E}[\hat{\beta}_0] = \beta_0\)</span>.</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[\hat{\beta}_0] &amp; = \mathbb{E}[\bar{y} - \hat{\beta}_1\bar{x}]\\
&amp; = \mathbb{E}[\bar{y}] - \beta_1 \bar{x}\\
&amp; = \frac{1}{n}\sum_{i = 1}^n\mathbb{E}[y_i] - \beta_1 \bar{x}\\
&amp; = \frac{1}{n}\sum_{i = 1}^n (\beta_0+ \beta_1 x_i) - \beta_1 \bar{x}\\
&amp; = \frac{1}{n}( n\beta_0 + \beta_1 \sum_{i = 1}^n x_i) - \beta_1 \bar{x}\\
&amp; = \beta_0 + \beta_1 \bar{x} - \beta_1 \bar{x}\\
&amp; = \beta_0 \mbox{ (as required)}
\end{align*}\]</span></p>
<p>Second, show that Var(<span class="math inline">\(\hat{\beta}_0\)</span>) = <span class="math inline">\(\sigma^2\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}\right)\)</span></p>
<p><span class="math display">\[\begin{align*}
\mbox{Var}(\hat{\beta}_0) &amp; =\mbox{Var}(\bar{y} - \hat{\beta}_1\bar{x}) \\
&amp; = \mbox{Var}(\bar{y}) + \bar{x}^2 \mbox{Var}(\hat{\beta}_1) - 2\bar{x}\mbox{Cov}(\bar{y}, \hat{\beta}_1)
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
\mbox{Cov}(\bar{y}, \hat{\beta}_1) &amp; =\mbox{Cov}\left( \frac{1}{n}\sum_{i = 1}^n y_i, \sum_{i = 1}^n a_iy_i \right)\\
&amp; =\sum_{i = 1}^n  \sum_{j = 1}^n \frac{1}{n} a_i\mbox{Cov}(y_i, y_j)\\
&amp; =\frac{1}{n} \sum_{i = 1}^n  \sum_{j = 1}^n a_i\mbox{Cov}(y_i, y_j)\\
&amp; =\frac{1}{n} \sum_{i = 1}^n   a_i\mbox{Cov}(y_i, y_i) \mbox{ (since $y_i$ are indep.)} \\
&amp; =\frac{\sigma^2}{n} \sum_{i = 1}^n   a_i\\
&amp; = 0
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
\mbox{Var}(\hat{\beta}_0) &amp; = \mbox{Var}(\bar{y}) + \bar{x}^2 \mbox{Var}(\hat{\beta}_1) \\
&amp; =  \frac{1}{n^2} \sum_{i = 1}^n  \mbox{Var}(y_i) + \bar{x}^2 \frac{\sigma^2}{S_{xx}}\\
&amp; =  \frac{1}{n^2} n\sigma^2 + \bar{x}^2 \frac{\sigma^2}{S_{xx}}\\
&amp; =  \sigma^2\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}\right) \mbox{ (as required)}
\end{align*}\]</span></p>
<p>Finally, the normality assumption follows as <span class="math inline">\(\hat{\beta}_0\)</span> is a linear combination of normal random variables (<span class="math inline">\(y_i\)</span>s and <span class="math inline">\(\hat{\beta}_1\)</span>).</p>
<p>Proof of (3):</p>
<p><span class="math display">\[\begin{align*}
\mbox{Cov}(\hat{\beta}_0, \hat{\beta}_1) &amp; =\mbox{Cov}(\bar{y} - \hat{\beta}_1\bar{x}, \hat{\beta}_1)\\
&amp; =\mbox{Cov}(\bar{y}, \hat{\beta}_1) - \mbox{Cov}(\hat{\beta}_1\bar{x}, \hat{\beta}_1) \\
&amp; = 0 - \bar{x}\mbox{Cov}(\hat{\beta}_1, \hat{\beta}_1) \\
&amp; = -\bar{x} \frac{\sigma^2}{S_{xx}}
\end{align*}\]</span></p>
<p>Proof of (4):</p>
<p>First show that <span class="math inline">\(\mathbb{E}[\hat{y}] = \beta_0 + \beta_1 x\)</span>.</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[\hat{y}] &amp; = \mathbb{E}[\hat{\beta}_0 + \hat{\beta}_1x]\\
&amp; = \mathbb{E}[\hat{\beta}_0] + \mathbb{E}[\hat{\beta}_1]x\\
&amp; = \beta_0 + \beta_1 x \mbox{ (as required)}
\end{align*}\]</span></p>
<p>Second, show that Var(<span class="math inline">\(\hat{y}\)</span>) = <span class="math inline">\(\sigma^2\left(\frac{1}{n} + \frac{(x-\bar{x})^2}{S_{xx}}\right)\)</span></p>
<p><span class="math display">\[\begin{align*}
\mbox{Var}(\hat{y}) &amp; =\mbox{Var}(\hat{\beta}_0 + \hat{\beta}_1 x) \\
   &amp; =\mbox{Var}(\bar{y} - \hat{\beta}_1\bar{x} + \hat{\beta}_1 x) \\
   &amp; =\mbox{Var}(\bar{y} + \hat{\beta}_1(x - \bar{x})) \\
    &amp; =\mbox{Var}(\bar{y}) + (x - \bar{x})^2 \mbox{Var}(\hat{\beta}_1) + 2(x - \bar{x})\mbox{Cov}(\bar{y}, \hat{\beta}_1)\\
        &amp; =\frac{\sigma^2}{n} + (x - \bar{x})^2 \frac{\sigma^2}{S_{xx}}\\
        &amp; =\sigma^2\left(\frac{1}{n} +  \frac{(x - \bar{x})^2}{S_{xx}}\right) \quad \mbox{ (as required)}.
\end{align*}\]</span></p>
<p>Finally, the normality assumption follows as <span class="math inline">\(\hat{y}\)</span> is a linear combination of <span class="math inline">\(y_i\)</span>s.</p>
</div>
<div id="special-cases" class="section level3" number="3.2.6">
<h3>
<span class="header-section-number">3.2.6</span> Special cases<a class="anchor" aria-label="anchor" href="#special-cases"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li><p>At <span class="math inline">\(x = 0\)</span>, <span class="math inline">\(\hat{y} = \hat{\beta}_0\)</span>.</p></li>
<li><p>At <span class="math inline">\(x = x_i\)</span>, <span class="math inline">\(\hat{y} = \hat{y}_i\)</span>.</p></li>
</ol>
<p><span class="math display">\[\begin{align*}
\mbox{Var}(\hat{y}_i) &amp; =\sigma^2\left(\frac{1}{n} +  \frac{(x_i - \bar{x})^2}{S_{xx}}\right)\\
     &amp; =\sigma^2h_{ii}
\end{align*}\]</span></p>
<p>NOTE: <span class="math inline">\(h_{ii} = \left(\frac{1}{n} +  \frac{(x_i - \bar{x})^2}{S_{xx}}\right)\)</span> is a distance value (see later!)</p>
</div>
</div>
<div id="simple-linear-regression-models-in-r" class="section level2" number="3.3">
<h2>
<span class="header-section-number">3.3</span> Simple linear regression models in R<a class="anchor" aria-label="anchor" href="#simple-linear-regression-models-in-r"><i class="fas fa-link"></i></a>
</h2>
<div id="r" class="section level3" number="3.3.1">
<h3>
<span class="header-section-number">3.3.1</span> R<a class="anchor" aria-label="anchor" href="#r"><i class="fas fa-link"></i></a>
</h3>
<p>Some useful code is below:</p>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mylm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Fuel</span> <span class="op">~</span> <span class="va">Temp</span>, data <span class="op">=</span> <span class="va">FuelTempData</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">mylm</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Fuel ~ Temp, data = FuelTempData)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.5663 -0.4432 -0.1958  0.2879  1.0560 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 15.83786    0.80177  19.754 1.09e-06 ***
## Temp        -0.12792    0.01746  -7.328  0.00033 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.6542 on 6 degrees of freedom
## Multiple R-squared:  0.8995, Adjusted R-squared:  0.8827 
## F-statistic: 53.69 on 1 and 6 DF,  p-value: 0.0003301</code></pre>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/anova.html">anova</a></span><span class="op">(</span><span class="va">mylm</span><span class="op">)</span></span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: Fuel
##           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## Temp       1 22.9808  22.981  53.695 0.0003301 ***
## Residuals  6  2.5679   0.428                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mylm</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">broom</span><span class="fu">::</span><span class="fu"><a href="https://generics.r-lib.org/reference/augment.html">augment</a></span><span class="op">(</span><span class="va">FuelTempData</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code>## # A tibble: 6 × 8
##    Temp  Fuel .fitted  .resid  .hat .sigma  .cooksd .std.resid
##   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;
## 1  28    12.4   12.3   0.144  0.307  0.712 0.0154       0.264 
## 2  28    11.7   12.3  -0.556  0.307  0.651 0.231       -1.02  
## 3  32.5  12.4   11.7   0.720  0.219  0.617 0.217        1.24  
## 4  39    10.8   10.8  -0.0489 0.143  0.716 0.000542    -0.0807
## 5  45.9   9.4    9.97 -0.566  0.128  0.663 0.0628      -0.927 
## 6  57.8   9.5    8.44  1.06   0.261  0.460 0.623        1.88</code></pre>
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mylm</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">broom</span><span class="fu">::</span><span class="fu"><a href="https://generics.r-lib.org/reference/augment.html">augment</a></span><span class="op">(</span><span class="va">FuelTempData</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Temp</span>, y <span class="op">=</span> <span class="va">Fuel</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Temp</span>, y <span class="op">=</span> <span class="va">.fitted</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-26-1.png" width="672" style="display: block; margin: auto;"></div>
</div>
</div>
<div id="statistical-inference" class="section level2" number="3.4">
<h2>
<span class="header-section-number">3.4</span> Statistical inference<a class="anchor" aria-label="anchor" href="#statistical-inference"><i class="fas fa-link"></i></a>
</h2>
<div id="r-simulation" class="section level3" number="3.4.1">
<h3>
<span class="header-section-number">3.4.1</span> R simulation:<a class="anchor" aria-label="anchor" href="#r-simulation"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p>Reminder: the linear relationship <span class="math inline">\(\mathbb{E}[y_{i}] = \beta_{0} + \beta_{1}x_{i}\)</span> is the <strong>underlying true line</strong> and the values of its parameters (intercept <span class="math inline">\(\beta_0\)</span> and slope <span class="math inline">\(\beta_1\)</span>) are unknown.</p></li>
<li><p>We <strong>estimate</strong> the parameters with <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>.</p></li>
<li><p>The parameter estimates have sampling distributions.</p></li>
</ul>
<p>To simulate a dataset, choose true model parameters <span class="math inline">\(\beta_0\)</span> <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p>
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">beta_0</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">beta_1</span> <span class="op">&lt;-</span> <span class="fl">2</span></span>
<span><span class="va">sigma_sq</span> <span class="op">&lt;-</span> <span class="fl">10</span></span></code></pre></div>
<p>Choose sample size <span class="math inline">\(n\)</span>:</p>
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">20</span> <span class="co"># number of observations in the sample</span></span></code></pre></div>
<p>Simulate <span class="math inline">\(x_i\)</span> and <span class="math inline">\(\epsilon_i\)</span> for <span class="math inline">\(i = 1,..., n\)</span>:</p>
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">12</span><span class="op">)</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="op">-</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">eps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">sigma_sq</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>Calculate <span class="math inline">\(y_i\)</span>:</p>
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">beta_0</span> <span class="op">+</span> <span class="va">x</span> <span class="op">*</span> <span class="va">beta_1</span> <span class="op">+</span> <span class="va">eps</span></span></code></pre></div>
<p>Let’s have a look at our made up data:</p>
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span><span class="va">beta_0</span>, <span class="va">beta_1</span>, col <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="co"># true model</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span>, col <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span><span class="op">)</span> <span class="co"># fit the model</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, col <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-31-1.png" width="672" style="display: block; margin: auto;"></div>
<p>Red is the true model and green line is estimated from the simmulated dataset.</p>
<p>Let’s compare the estimated parameters with <span class="math inline">\(\beta_0\)</span> <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\sigma^2\)</span>:</p>
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">hat_beta_0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">hat_beta_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span>
<span><span class="va">s</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="va">n</span> <span class="op">-</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">hat_beta_0</span> </span></code></pre></div>
<pre><code>## (Intercept) 
##    1.073254</code></pre>
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">hat_beta_1</span> </span></code></pre></div>
<pre><code>##        x 
## 2.107893</code></pre>
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">s</span><span class="op">^</span><span class="fl">2</span></span></code></pre></div>
<pre><code>## [1] 6.541065</code></pre>
<p>We can think of these as single realisations from their sampling distribution of the estimators</p>
<!-- # can also obtain s^2 from: anova(fit)[["Mean Sq"]][2] or summary(fit) gives s -->
<p>Some useful calculations for sampling distributions of estimators:</p>
<div class="sourceCode" id="cb59"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x_bar</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="va">Sxx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">x</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">-</span> <span class="va">n</span> <span class="op">*</span> <span class="op">(</span><span class="va">x_bar</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="va">var_beta_0</span> <span class="op">&lt;-</span> <span class="va">sigma_sq</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="va">n</span> <span class="op">+</span> <span class="va">x_bar</span><span class="op">^</span><span class="fl">2</span> <span class="op">/</span> <span class="va">Sxx</span><span class="op">)</span></span>
<span><span class="va">var_beta_1</span> <span class="op">&lt;-</span> <span class="va">sigma_sq</span> <span class="op">/</span> <span class="va">Sxx</span></span>
<span></span>
<span><span class="va">est_cov</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="va">sigma_sq</span> <span class="op">*</span> <span class="va">x_bar</span> <span class="op">/</span> <span class="va">Sxx</span> <span class="co"># estimated covariance from one sample</span></span>
<span></span>
<span><span class="va">se_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">sigma_sq</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="va">n</span> <span class="op">+</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">x_bar</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">/</span> <span class="va">Sxx</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>Let’s plot the sampling dists for these estimators:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\hat{\beta}_1 \sim N\left(\beta_1,\frac{\sigma^2}{S_{xx}}\right)\)</span></li>
</ol>
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">hat_beta_1</span><span class="op">)</span><span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>xdist <span class="op">=</span> <span class="fu">dist_normal</span><span class="op">(</span><span class="va">beta_1</span>, <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">var_beta_1</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">stat_slab</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">hat_beta_1</span>, y <span class="op">=</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">"sampling distribution"</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hat</a></span><span class="op">(</span><span class="va">beta</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"density"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-34-1.png" width="672"></div>
<ol start="2" style="list-style-type: decimal">
<li><span class="math inline">\(\hat{\beta}_0 \sim N\left(\beta_0, \sigma^2\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}\right)\right)\)</span></li>
</ol>
<div class="sourceCode" id="cb61"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">hat_beta_0</span><span class="op">)</span><span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>xdist <span class="op">=</span> <span class="fu">dist_normal</span><span class="op">(</span><span class="va">beta_0</span>, <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">var_beta_0</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">stat_slab</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">hat_beta_0</span>, y <span class="op">=</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">"sampling distribution"</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hat</a></span><span class="op">(</span><span class="va">beta</span><span class="op">)</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"density"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-35-1.png" width="672"></div>
<ol start="3" style="list-style-type: decimal">
<li><span class="math inline">\((n-2)\frac{\hat{\sigma}^2}{\sigma^2} \sim \chi^2_{(n-2)}\)</span></li>
</ol>
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">s</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>xdist <span class="op">=</span> <span class="fu">dist_chisq</span><span class="op">(</span><span class="va">n</span><span class="op">-</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">stat_slab</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">s</span><span class="op">^</span><span class="fl">2</span> <span class="op">*</span><span class="op">(</span><span class="va">n</span> <span class="op">-</span> <span class="fl">2</span><span class="op">)</span> <span class="op">/</span> <span class="va">sigma_sq</span>, y <span class="op">=</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">"sampling distribution"</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="op">(</span><span class="va">n</span> <span class="op">-</span> <span class="fl">2</span><span class="op">)</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hat</a></span><span class="op">(</span><span class="va">sigma</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">/</span> <span class="va">sigma</span><span class="op">^</span><span class="fl">2</span> <span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"density"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-36-1.png" width="672"></div>
<p>Now let’s repeat this simulation process 500 times:</p>
<div class="sourceCode" id="cb63"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">500</span> <span class="co"># number of simulations</span></span>
<span><span class="va">estimates</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fl">0</span>, <span class="va">N</span>, <span class="fl">3</span><span class="op">)</span> <span class="co"># object to save parameter estimates</span></span>
<span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">N</span><span class="op">)</span></span>
<span><span class="op">{</span></span>
<span>  <span class="co"># x &lt;- runif(n, -5, 5)</span></span>
<span>  <span class="va">eps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">sigma_sq</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">y</span> <span class="op">&lt;-</span> <span class="va">beta_0</span> <span class="op">+</span> <span class="va">x</span> <span class="op">*</span> <span class="va">beta_1</span> <span class="op">+</span> <span class="va">eps</span></span>
<span></span>
<span>  <span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span><span class="op">)</span></span>
<span>  <span class="va">estimates</span><span class="op">[</span><span class="va">i</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span>  <span class="va">estimates</span><span class="op">[</span><span class="va">i</span>, <span class="fl">2</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span>
<span>  <span class="va">estimates</span><span class="op">[</span><span class="va">i</span>, <span class="fl">3</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/anova.html">anova</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">[[</span><span class="st">"Mean Sq"</span><span class="op">]</span><span class="op">]</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="co"># sigamsq</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Plot the <span class="math inline">\(N = 500\)</span> fitted lines:</p>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-38-1.png" width="672"></div>
<!-- cov(estimates[,1], estimates[,2]) # covariance by simulation -->
<!-- est_cov #compare to -->
<!-- plot(estimates[,1], estimates[,2]) #visualise covariance -->
<p>Plot the estimates and compare to sampling distribution curves:</p>
<div class="sourceCode" id="cb64"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">estimates</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">beta_hat_0</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_dots</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">stat_slab</span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>xdist <span class="op">=</span> <span class="fu">dist_normal</span><span class="op">(</span><span class="va">beta_0</span>, <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">var_beta_0</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>, </span>
<span>            col <span class="op">=</span> <span class="fl">1</span>, fill <span class="op">=</span> <span class="cn">NA</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">"sampling distribution and simulation estimates"</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hat</a></span><span class="op">(</span><span class="va">beta</span><span class="op">)</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"Density"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-39-1.png" width="672" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb65"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">estimates</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">beta_hat_1</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_dots</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">stat_slab</span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>xdist <span class="op">=</span> <span class="fu">dist_normal</span><span class="op">(</span><span class="va">beta_1</span>, <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">var_beta_1</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>, </span>
<span>            col <span class="op">=</span> <span class="fl">1</span>, fill <span class="op">=</span> <span class="cn">NA</span>, height <span class="op">=</span> <span class="fl">0.75</span><span class="op">)</span>  <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">"sampling distribution and simulation estimates"</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hat</a></span><span class="op">(</span><span class="va">beta</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"Density"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-39-2.png" width="672" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb66"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">estimates</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">sigma_sq_hat</span><span class="op">*</span><span class="op">(</span><span class="va">n</span> <span class="op">-</span> <span class="fl">2</span><span class="op">)</span> <span class="op">/</span> <span class="va">sigma_sq</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_dots</span><span class="op">(</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">stat_slab</span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>xdist <span class="op">=</span> <span class="fu">dist_chisq</span><span class="op">(</span><span class="va">n</span><span class="op">-</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span>, </span>
<span>            col <span class="op">=</span> <span class="fl">1</span>, fill <span class="op">=</span> <span class="cn">NA</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">"sampling distribution and simulation estimates"</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="op">(</span><span class="va">n</span> <span class="op">-</span> <span class="fl">2</span><span class="op">)</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hat</a></span><span class="op">(</span><span class="va">sigma</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">/</span> <span class="va">sigma</span><span class="op">^</span><span class="fl">2</span> <span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"Density"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-39-3.png" width="672" style="display: block; margin: auto;"></div>
<!--   # beta_1 -->
<!-- hist(estimates$beta_hat_1, breaks = 50, freq = FALSE, xlab = expression(hat(beta)[1]), main = expression(hat(beta)[1])) -->
<!-- curve(dnorm(x, beta_1, sqrt(var_beta_1)), col = 2, add = TRUE) -->
<!-- hist(estimates[, 1], breaks = 50, freq = FALSE, xlab = expression(hat(beta)[0]), main = expression(hat(beta)[0])) -->
<!-- curve(dnorm(x, beta_0, sqrt(var_beta_0)), col = 2, add = TRUE) -->
<!-- # sigma_sq -->
<!-- hist((n - 2) / sigma_sq * estimates[, 3], breaks = 50, freq = FALSE, xlab = expression(hat(sigma)^2), main = expression(hat(sigma)^2)) -->
<!-- curve(dchisq(x, df = n - 2), col = 2, add = TRUE) -->
<!-- NOTE: superimposed curves are calculated from one (first) sample of the data and the known parameters. The distributions are from \@ref(prop): -->
<!-- * $\hat{\beta}_1 \sim N\left(\beta_1,\frac{\sigma^2}{S_{xx}}\right)$ -->
<!-- * $\hat{\beta}_0 \sim N\left(\beta_0, \sigma^2\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}\right)\right)$ -->
<!-- * $(n-2)\frac{\hat{\sigma}^2}{\sigma^2} \sim \chi^2_{(n-2)}$ -->
</div>
<div id="inference-for-beta_1" class="section level3" number="3.4.2">
<h3>
<span class="header-section-number">3.4.2</span> Inference for <span class="math inline">\(\beta_1\)</span><a class="anchor" aria-label="anchor" href="#inference-for-beta_1"><i class="fas fa-link"></i></a>
</h3>
<div id="confidence-interval" class="section level4" number="3.4.2.1">
<h4>
<span class="header-section-number">3.4.2.1</span> Confidence Interval<a class="anchor" aria-label="anchor" href="#confidence-interval"><i class="fas fa-link"></i></a>
</h4>
<p><span class="math inline">\(\hat{\beta}_1\)</span> estimates <span class="math inline">\(\beta_1\)</span>, for example the change in fuel use for a 1<span class="math inline">\(^oF\)</span> increase in temperature.</p>
<p>We would like to construct a confidence interval for <span class="math inline">\(\beta_1\)</span>. This will give us an interval where we are confident the true <span class="math inline">\(\beta_1\)</span> lies.</p>
<p>The key to obtaining a C.I. is the fact that:</p>
<p><span class="math display">\[\hat{\beta}_1 \sim N(\beta_1, \frac{\sigma^2}{S_{xx}}).\]</span></p>
<p>Equivalently</p>
<p><span class="math display">\[\frac{\hat{\beta}_{1} - \beta_{1}}{\sigma/\sqrt{S_{xx}}} \sim N(0,1).\]</span></p>
<p>And, when we replace <span class="math inline">\(\sigma\)</span> by <span class="math inline">\(\hat{\sigma}\)</span> we have:</p>
<p><span class="math display">\[\frac{\hat{\beta}_{1} - \beta_{1}}{\hat{\sigma}/\sqrt{S_{xx}}} \sim  t_{n-2}.\]</span></p>
<p>The df is <span class="math inline">\(n-2\)</span> because this is the df associated with the estimate of <span class="math inline">\(\sigma\)</span>.</p>
<p>In general, when:</p>
<p><span class="math display">\[\frac{\mbox{Est - parameter}}{\mbox{S.E.(Est)}} \sim \mbox{distribution}.\]</span></p>
<p>A C.I. for the parameter is given by:</p>
<p><span class="math display">\[\mbox{Est} \pm \mbox{(quantile from distribution)} \times \mbox{S.E.(Est)}.\]</span></p>
<p>A <span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence interval for <span class="math inline">\(\beta_{1}\)</span>:</p>
<p><span class="math display">\[\hat{\beta}_{1} \pm t_{n-2}(\alpha/2) \times \sqrt{\frac{\hat{\sigma}^2}{S_{xx}}}.\]</span></p>
<p>For the fuel use data, a <span class="math inline">\(95\%\)</span> C.I. for <span class="math inline">\(\beta_1\)</span> is:</p>
<p><span class="math display">\[\begin{align*}
\hat{\beta}_1 &amp;\pm t_6(0.025) \times \sqrt{\frac{\hat{\sigma}^2}{S_{xx}}}\\
&amp; = -0.128 \pm 2.45 \times 0.018\\
&amp; = (-0.171, -0.085)
\end{align*}\]</span></p>
<p>We are <span class="math inline">\(95\%\)</span> confident that the average fuel use drop is between 0.085 and 0.171.</p>
</div>
<div id="hypothesis-test" class="section level4" number="3.4.2.2">
<h4>
<span class="header-section-number">3.4.2.2</span> Hypothesis test<a class="anchor" aria-label="anchor" href="#hypothesis-test"><i class="fas fa-link"></i></a>
</h4>
<p>In some settings we may wish to test:</p>
<p><span class="math inline">\(H_{0}: \beta_{1} = 0\)</span> versus <span class="math inline">\(H_{A}: \beta_{1} \ne 0\)</span>.</p>
<p>The null hypothesis here is that <span class="math inline">\(\mathbb{E}[y] = \beta_0\)</span> i.e. <span class="math inline">\(\mathbb{E}[y]\)</span> is not linearly related to <span class="math inline">\(x\)</span>.</p>
<p>Under <span class="math inline">\(H_0\)</span>:</p>
<p><span class="math display">\[t_{obs} = \frac{\hat{\beta}_{1} - 0}{\hat{\sigma}/\sqrt{S_{xx}}} \sim t_{n-2}.\]</span></p>
<p>P-value = <span class="math inline">\(P[T_{n-2} \geq |t_{obs}|]\)</span></p>
<p>Reject <span class="math inline">\(H_0\)</span> for small p-values, typically <span class="math inline">\(p&lt; 0.05\)</span>.</p>
<p>In the fuel use example:</p>
<p><span class="math display">\[t_{obs} = \frac{-0.128-0}{0.018} = -7.33\]</span></p>
<p>and p-value <span class="math inline">\(&lt; 0.001\)</span>, so we reject <span class="math inline">\(H_0\)</span> and conclude that <span class="math inline">\(\beta_{1} \ne 0\)</span>.</p>
<p>We could also test <span class="math inline">\(H_0: \beta_1=b\)</span> by computing:</p>
<p><span class="math display">\[\frac{\hat{\beta}_{1} - b}{\mbox{S.E}.(\hat{\beta}_{1})}.\]</span></p>
</div>
</div>
<div id="inference-for-beta_0" class="section level3" number="3.4.3">
<h3>
<span class="header-section-number">3.4.3</span> Inference for <span class="math inline">\(\beta_0\)</span><a class="anchor" aria-label="anchor" href="#inference-for-beta_0"><i class="fas fa-link"></i></a>
</h3>
<p>A <span class="math inline">\(95\%\)</span> C.I. for <span class="math inline">\(\beta_0\)</span> is:</p>
<p><span class="math display">\[\hat{\beta}_{0} \pm t_{n-2}(\alpha/2) \times \mbox{S.E.}(\hat{\beta}_{0}).\]</span></p>
<p>Note:
<span class="math inline">\(\mbox{S.E.}(\hat{\beta}_0) = \sqrt{\hat{\sigma}^2\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}\right)}\)</span></p>
<p>For the fuel use data:</p>
<p><span class="math display">\[\begin{align*}
&amp; = 15.84 \pm 2.45 \times 0.8018\\
&amp; = (13.88, 17.80)
\end{align*}\]</span></p>
<p>We can also test for a particular value of <span class="math inline">\(\beta_0\)</span>, e.g.</p>
<p><span class="math inline">\(H_0: \beta_0 = 0\)</span> vs. <span class="math inline">\(H_A: \beta_0 \neq 0\)</span></p>
<p>The null hypothesis here is that <span class="math inline">\(\mathbb{E}[y] = \beta_1 x\)</span> i.e. the line passes through the origin.</p>
<p>The test statistic is</p>
<p><span class="math display">\[\frac{\hat{\beta}_{0} - \beta_0}{\mbox{S.E.}(\hat{\beta}_{0})} = 19.75.\]</span></p>
<p>for the fuel data.</p>
<p>P-value <span class="math inline">\(= 2P[T_6 \geq 19.75] &lt;0.001.\)</span></p>
<p>Note: This is for illustration, in practice with this particular data we would not do this. Why?</p>
</div>
<div id="inference-for-mean-response" class="section level3" number="3.4.4">
<h3>
<span class="header-section-number">3.4.4</span> Inference for mean response<a class="anchor" aria-label="anchor" href="#inference-for-mean-response"><i class="fas fa-link"></i></a>
</h3>
<p>Suppose we want to estimate <span class="math inline">\(\mu=\mathbb{E}[y]\)</span> at a particular value of <span class="math inline">\(x\)</span>.</p>
<p>At <span class="math inline">\(x_0\)</span> let:</p>
<p><span class="math display">\[\mu_0 = \mathbb{E}[y_0] = \beta_0 + \beta_1 x_0\]</span></p>
<p>We can estimate <span class="math inline">\(\mu_0\)</span> by:</p>
<p><span class="math display">\[\hat{y}_0 = \hat{\beta}_0 + \hat{\beta}_1 x_0.\]</span></p>
<p>e.g. we estimate the mean fuel use at temperature <span class="math inline">\(50^oF\)</span> by</p>
<p><span class="math display">\[\hat{y}_0 = \hat{\beta}_0 + \hat{\beta}_1 \times 50 =15.84-0.128 \times 50 =9.44.\]</span></p>
<p>A <span class="math inline">\(95\%\)</span> C.I. for <span class="math inline">\(\mu_0\)</span> is given by</p>
<p><span class="math display">\[\begin{align*}
\mbox{Est} &amp;\pm t_{n-2}(\alpha/2) \times \mbox{S.E.(Est)}\\
\hat{y}_0 &amp;\pm t_{n-2}(\alpha/2) \times \mbox{S.E.}_{\mbox{fit}}(\hat{y}_0)
\end{align*}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{align*}
\mbox{S.E.}_{\mbox{fit}}(\hat{y}_0)&amp; =\hat{\sigma}\sqrt{h_{00}}\\
&amp; = \hat{\sigma}\sqrt{\frac{1}{n}+\frac{(x_0-\bar{x})^2}{S_{xx}}}\\
&amp; = 0.65 \times \sqrt{0.15}=0.254
\end{align*}\]</span></p>
<p>The <span class="math inline">\(95\%\)</span> C.I. is</p>
<p><span class="math display">\[\begin{align*}
&amp; = 9.44 \pm 2.45 \times 0.254\\
&amp; = (8.8, 10.1)
\end{align*}\]</span></p>
<p>This interval contains the true mean fuel use at <span class="math inline">\(50^oF\)</span> with <span class="math inline">\(95\%\)</span> confidence.</p>
</div>
<div id="inference-for-prediction" class="section level3" number="3.4.5">
<h3>
<span class="header-section-number">3.4.5</span> Inference for prediction<a class="anchor" aria-label="anchor" href="#inference-for-prediction"><i class="fas fa-link"></i></a>
</h3>
<p>Suppose we want to predict the response at a particular value of <span class="math inline">\(x\)</span>.</p>
<p>At <span class="math inline">\(x_0\)</span>, let <span class="math inline">\(y_0\)</span> be the unobserved response.</p>
<p>From our model:</p>
<p><span class="math display">\[y_0 =\mu_0 + \epsilon= \beta_0 + \beta_1 x_0 + \epsilon\]</span></p>
<p>and we estimate (predict) it by:</p>
<p><span class="math display">\[\hat{y}_0 = \hat{\beta}_0 + \hat{\beta}_1 x_0.\]</span></p>
<p>Note: <strong>estimation of a random variable is called prediction</strong>.</p>
<p>In our example, we predict that the fuel use at temp <span class="math inline">\(50^oF\)</span> as:</p>
<p><span class="math display">\[\hat{y}_0 = 15.84-0.128\times 50=9.44\]</span></p>
<p>Note that the prediction of future response equals the estimate of the mean response, however the associated standard errors (and hence confidence intervals) are different.</p>
<p>Confidence intervals for random variables are called <strong>prediction intervals</strong> (PIs).</p>
<p>In our prediction of <span class="math inline">\(y_0\)</span> by <span class="math inline">\(\hat{y}_0\)</span> the prediction error <span class="math inline">\(y_0-\hat{y}_0\)</span> is with variance:</p>
<p><span class="math display">\[\begin{align*}
\mbox{Var}(y_0-\hat{y}_0)&amp; = \mbox{Var}(y_0)+ \mbox{Var}(\hat{y}_0)\mbox{       (indep as $y_0$ is out of sample)}\\
&amp; =\sigma^2 + \sigma^2\times h_{00} \\
&amp; = \sigma^2(1+h_{00})\\
&amp; = \sigma^2\left(1+\frac{1}{n}+\frac{(x_0-\bar{x})^2}{S_{xx}}\right)
\end{align*}\]</span></p>
<p>The <span class="math inline">\(\mbox{S.E.}\)</span> of the prediction is then:</p>
<p><span class="math display">\[\mbox{S.E.}_{\mbox{pred}} (\hat{y}_0) = \hat{\sigma}\sqrt{1+\frac{1}{n}+\frac{(x_0-\bar{x})^2}{S_{xx}}}\]</span></p>
<p>The <span class="math inline">\(95\%\)</span> prediction interval is</p>
<p><span class="math display">\[\hat{y}_0 \pm t_{n-2}(\alpha/2) \times \mbox{S.E.}_{\mbox{pred}} (\hat{y}_0)\]</span></p>
<p>For <span class="math inline">\(x_0\)</span> = 50, <span class="math inline">\(\hat{y}_0 =9.442\)</span>:</p>
<p><span class="math display">\[\mbox{S.E.}_{\mbox{pred}}= \sqrt{1+0.15} \times 0.654 = 0.702\]</span></p>
<p>So the <span class="math inline">\(95\%\)</span> P.I. is:</p>
<p><span class="math display">\[\begin{align*}
&amp; = 9.44 \pm 2.45 \times 0.702\\
&amp; = (7.72, 11.16)
\end{align*}\]</span></p>
<p>We are <span class="math inline">\(95\%\)</span> sure that the interval contains the actual fuel use on a week with temp = <span class="math inline">\(50^oF\)</span>.</p>
</div>
<div id="plotting-cis-and-pis-in-r" class="section level3" number="3.4.6">
<h3>
<span class="header-section-number">3.4.6</span> Plotting CIs and PIs in R<a class="anchor" aria-label="anchor" href="#plotting-cis-and-pis-in-r"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb67"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mylm</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">broom</span><span class="fu">::</span><span class="fu"><a href="https://generics.r-lib.org/reference/augment.html">augment</a></span><span class="op">(</span><span class="va">FuelTempData</span>, interval <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"confidence"</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="fl">4</span><span class="op">)</span></span></code></pre></div>
<pre><code>## # A tibble: 4 × 10
##    Temp  Fuel .fitted .lower .upper  .resid  .hat .sigma  .cooksd .std.resid
##   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;
## 1  28    12.4    12.3   11.4   13.1  0.144  0.307  0.712 0.0154       0.264 
## 2  28    11.7    12.3   11.4   13.1 -0.556  0.307  0.651 0.231       -1.02  
## 3  32.5  12.4    11.7   10.9   12.4  0.720  0.219  0.617 0.217        1.24  
## 4  39    10.8    10.8   10.2   11.5 -0.0489 0.143  0.716 0.000542    -0.0807</code></pre>
<div class="sourceCode" id="cb69"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">FuelTempData</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Temp</span>, y <span class="op">=</span> <span class="va">Fuel</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, se <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/plotCIs-1.png" width="384" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb70"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mylm</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">broom</span><span class="fu">::</span><span class="fu"><a href="https://generics.r-lib.org/reference/augment.html">augment</a></span><span class="op">(</span><span class="va">FuelTempData</span>, interval <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"prediction"</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Temp</span>, y <span class="op">=</span> <span class="va">Fuel</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">.fitted</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_ribbon.html">geom_ribbon</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>ymin <span class="op">=</span> <span class="va">.lower</span>, ymax <span class="op">=</span> <span class="va">.upper</span><span class="op">)</span>, fill <span class="op">=</span> <span class="st">"pink"</span>, alpha <span class="op">=</span> <span class="fl">0.4</span><span class="op">)</span> </span></code></pre></div>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/plotCIs-2.png" width="384" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb71"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">newx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">30</span>, <span class="fl">60</span><span class="op">)</span></span>
<span></span>
<span><span class="va">CI</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">mylm</span>,</span>
<span>  newdata <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>Temp  <span class="op">=</span> <span class="va">newx</span><span class="op">)</span>, </span>
<span>  interval <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"confidence"</span><span class="op">)</span>,</span>
<span>  level <span class="op">=</span> <span class="fl">0.95</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">CI</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="fl">4</span><span class="op">)</span></span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 12.00021 11.17760 12.82281
## 2 11.87228 11.08013 12.66444
## 3 11.74436 10.98149 12.50724
## 4 11.61644 10.88152 12.35136</code></pre>
<div class="sourceCode" id="cb73"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">PI</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">mylm</span>,</span>
<span>  newdata <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>Temp <span class="op">=</span> <span class="va">newx</span><span class="op">)</span>, </span>
<span>  interval <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"prediction"</span><span class="op">)</span>,</span>
<span>  level <span class="op">=</span> <span class="fl">0.95</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">PI</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="fl">4</span><span class="op">)</span></span></code></pre></div>
<pre><code>##        fit       lwr      upr
## 1 12.00021 10.200424 13.79999
## 2 11.87228 10.086217 13.65835
## 3 11.74436  9.971085 13.51764
## 4 11.61644  9.855011 13.37787</code></pre>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-40-1.png" width="384" style="display: block; margin: auto;"></div>
</div>
</div>
<div id="analysis-of-variance-for-s.l.r." class="section level2" number="3.5">
<h2>
<span class="header-section-number">3.5</span> Analysis of variance (for s.l.r.)<a class="anchor" aria-label="anchor" href="#analysis-of-variance-for-s.l.r."><i class="fas fa-link"></i></a>
</h2>
<p>The analysis of variance is a method for comparing the fit of two or more models to the same dataset.</p>
<p>It is particularly useful in <strong>multiple regression</strong>.</p>
<div id="vote-data-does-regression-on-growth-explain-vote" class="section level3" number="3.5.1">
<h3>
<span class="header-section-number">3.5.1</span> Vote Data: does regression on growth explain vote?<a class="anchor" aria-label="anchor" href="#vote-data-does-regression-on-growth-explain-vote"><i class="fas fa-link"></i></a>
</h3>
<p>In simple linear regression, this is equivalent to testing hypotheses about the slope parameter.</p>
<p>In other words: comparing the fit of the simple linear regression model to the fit of the null (intercept only) model.</p>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-41-1.png" width="672"></div>
<div class="sourceCode" id="cb75"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">lin_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">vote</span> <span class="op">~</span> <span class="va">growth</span>, data <span class="op">=</span> <span class="va">hibbs</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/anova.html">anova</a></span><span class="op">(</span><span class="va">lin_mod</span><span class="op">)</span></span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: vote
##           Df Sum Sq Mean Sq F value  Pr(&gt;F)    
## growth     1 273.63 273.632  19.321 0.00061 ***
## Residuals 14 198.27  14.162                    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
<div id="anova-decomposition" class="section level3" number="3.5.2">
<h3>
<span class="header-section-number">3.5.2</span> ANOVA decomposition<a class="anchor" aria-label="anchor" href="#anova-decomposition"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math display">\[\begin{align*}
\mbox{data} &amp; = \mbox{fit} + \mbox{residual} \\
y_i &amp; = \hat{y}_i +e_i\\
y_i - \bar{y} &amp; = \hat{y}_i - \bar{y} +e_i \\
\sum_{i = 1}^n ( y_i - \bar{y})^2 &amp; =\sum_{i = 1}^n(\hat{y}_i - \bar{y} +e_i)^2\\
&amp; =\sum_{i = 1}^n(\hat{y}_i - \bar{y})^2 +\sum_{i = 1}^ne_i^2 + 2\sum_{i = 1}^n (\hat{y}_i - \bar{y})e_i
\end{align*}\]</span></p>
<p>The last term is zero because (from normal equations):</p>
<p><span class="math inline">\(\sum_{i = 1}^n\hat{y}_ie_i = 0\)</span> and <span class="math inline">\(\sum_{i = 1}^ne_i = 0\)</span>.</p>
<p>The decomposition:</p>
<p><span class="math display">\[\sum_{i = 1}^n ( y_i - \bar{y})^2 =\sum_{i = 1}^n(\hat{y}_i - \bar{y})^2 +\sum_{i = 1}^ne_i^2\]</span></p>
<p>is called the <strong>ANOVA decomposition</strong>.</p>
<p>These calculations are summarised in the <strong>ANOVA table</strong>.</p>
</div>
<div id="for-the-voting-data" class="section level3" number="3.5.3">
<h3>
<span class="header-section-number">3.5.3</span> For the voting data<a class="anchor" aria-label="anchor" href="#for-the-voting-data"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb77"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">SST</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">hibbs</span><span class="op">$</span><span class="va">vote</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">hibbs</span><span class="op">$</span><span class="va">vote</span><span class="op">)</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">SSR</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">lin_mod</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">hibbs</span><span class="op">$</span><span class="va">vote</span><span class="op">)</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">SSE</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">lin_mod</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">SST</span></span></code></pre></div>
<pre><code>## [1] 471.905</code></pre>
<div class="sourceCode" id="cb79"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">SSR</span></span></code></pre></div>
<pre><code>## [1] 273.6323</code></pre>
<div class="sourceCode" id="cb81"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">SSE</span></span></code></pre></div>
<pre><code>## [1] 198.2727</code></pre>
<div class="sourceCode" id="cb83"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">SSR</span><span class="op">/</span><span class="va">SST</span></span></code></pre></div>
<pre><code>## [1] 0.5798462</code></pre>
</div>
<div id="anova-table" class="section level3" number="3.5.4">
<h3>
<span class="header-section-number">3.5.4</span> ANOVA table<a class="anchor" aria-label="anchor" href="#anova-table"><i class="fas fa-link"></i></a>
</h3>
<p>In general, an ANOVA table is a method for partitioning variability in a response variable into what is explained by the model fitted and what is left over.</p>
<p>The exact form of the ANOVA table will depend on the model that has been fitted.</p>
<p>The hypothesis being tested by the model will also depend on the model that has been fitted.</p>
<p>An ANOVA table for the simple linear regression model:</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="left">SOURCE</th>
<th align="left">df</th>
<th align="left">SS</th>
<th align="left">MS</th>
<th align="left">F</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Regression</td>
<td align="left">1</td>
<td align="left">SSR</td>
<td align="left">MSR = SSR/1</td>
<td align="left">MSR/MSE</td>
</tr>
<tr class="even">
<td align="left">Error</td>
<td align="left">n-2</td>
<td align="left">SSE</td>
<td align="left">MSE = SSE/(n-2)</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Total</td>
<td align="left">n-1</td>
<td align="left">SST</td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table></div>
<!-- \begin{tabular}{l | r l l l } --><!-- Source & df & SS & MS & F\\ \hline --><!-- &&&& \\ --><!-- Regression & $1$ & $\mbox{SSR} = \sum_{i=1}^n (\hat{y}_i-\bar{y})^2$ & $\mbox{MSR} = \mbox{SSR}/1$& $\mbox{MSR}/\mbox{MSE}$\\ --><!-- &&&& \\ --><!-- Error & $n-2$ & \mbox{SSE} = $\sum_{i=1}^n (y_i-\hat{y}_i)^2$ &$\mbox{MSE} = \mbox{SSE}/(n-2)$ &\\ \hline --><!-- &&&& \\ --><!-- Total & $n-1$ & $\mbox{SST} = \sum_{i=1}^n (y_i-\bar{y})^2$ && --><!-- \end{tabular} --><p>NOTE:</p>
<ul>
<li><p>The <strong>total sum of squares</strong>, <span class="math inline">\(\mbox{SST} = \sum_{i=1}^{n}(y_{i} - \bar{y})^{2}\)</span> is the sum of squares of <span class="math inline">\(y\)</span> about the mean. The total sum of squares does not depend on <span class="math inline">\(x\)</span>. (NB: this is <span class="math inline">\(S_{yy}\)</span>)</p></li>
<li><p>The <strong>regression sum of squares</strong>, <span class="math inline">\(\mbox{SSR} = \sum_{i=1}^{n}(\hat{y}_{i} - \bar{y})^{2}\)</span>. Note that <span class="math inline">\(\hat{y}_{i}\)</span> depends on <span class="math inline">\(x\)</span>.</p></li>
<li><p>The <strong>residual/error sum of squares</strong>, <span class="math inline">\(\mbox{SSE} = \sum_{i=1}^{n}e_{i}^{2} = \sum_{i=1}^{n}(y_{i} - \hat{y}_{i})^{2}\)</span>.</p></li>
<li><p><span class="math inline">\(\mbox{SST} = \mbox{SSR} + \mbox{SSE}\)</span>.</p></li>
<li><p>The sums of squares have associated degrees of freedom (df).
<!-- %The number of independent pieces of information that go into the estimate of a parameter is called the degrees of freedom. In general, the degrees of freedom of an estimate of a parameter is equal to the number of independent scores that go into the estimate minus the number of parameters used as intermediate steps in the estimation of the parameter itself (i.e., the sample variance has N-1 degrees of freedom, since it is computed from N random scores minus the only 1 parameter estimated as intermediate step, which is the sample mean) --></p></li>
<li><p>MS = SS/df</p></li>
<li><p>The <strong>mean squared error</strong> <span class="math inline">\(\mbox{MSE}\)</span> estimates <span class="math inline">\(\sigma^{2}\)</span>.</p></li>
<li><p>The <strong>coefficient of determination</strong> is:
<span class="math display">\[R^{2} = \frac{\mbox{SSR}}{\mbox{SST}} = 1 - \frac{\mbox{SSE}}{\mbox{SST}}.\]</span></p></li>
<li><p><span class="math inline">\(R^{2}\)</span> is always between 0 and 1 and it measures the proportion of variation in <span class="math inline">\(y\)</span> that is explained by regression with <span class="math inline">\(x\)</span>.</p></li>
</ul>
<div class="sourceCode" id="cb85"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lin_mod</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = vote ~ growth, data = hibbs)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.9929 -0.6674  0.2556  2.3225  5.3094 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  46.2476     1.6219  28.514 8.41e-14 ***
## growth        3.0605     0.6963   4.396  0.00061 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.763 on 14 degrees of freedom
## Multiple R-squared:  0.5798, Adjusted R-squared:  0.5498 
## F-statistic: 19.32 on 1 and 14 DF,  p-value: 0.00061</code></pre>
</div>
<div id="special-cases-1" class="section level3" number="3.5.5">
<h3>
<span class="header-section-number">3.5.5</span> Special cases<a class="anchor" aria-label="anchor" href="#special-cases-1"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<span class="math inline">\(R^{2}=1\)</span>, <span class="math inline">\(\mbox{SSR} = \mbox{SST}\)</span>, <span class="math inline">\(\mbox{SSE}\)</span> = 0.</li>
</ul>
<p><span class="math inline">\(e_{i}=0\)</span>, <span class="math inline">\(i = 1, \dots,n\)</span>, data fall on a straight line.</p>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-46-1.png" width="288" style="display: block; margin: auto;"></div>
<ul>
<li>
<span class="math inline">\(R^{2}=0\)</span>, <span class="math inline">\(\mbox{SSR} = 0\)</span>, <span class="math inline">\(\mbox{SSE} = \mbox{SST}\)</span>.</li>
</ul>
<p><span class="math inline">\(\hat{y}_{i}=\bar{y}\)</span>, <span class="math inline">\(\hat{\beta}_1=0\)</span>.</p>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-47-1.png" width="288" style="display: block; margin: auto;"></div>
</div>
<div id="does-regression-on-x-explain-y" class="section level3" number="3.5.6">
<h3>
<span class="header-section-number">3.5.6</span> Does regression on x explain y?<a class="anchor" aria-label="anchor" href="#does-regression-on-x-explain-y"><i class="fas fa-link"></i></a>
</h3>
<p>In simple linear regression this amounts to testing:</p>
<p><span class="math display">\[H_0: \beta_1 = 0\]</span></p>
<p><span class="math display">\[H_A: \beta_1 \ne 0\]</span></p>
<p>We can use a t-test for this, but there is an equivalent test based on the F distribution. As we will see later, F-tests have a wide range of applications.</p>
<p>If <span class="math inline">\(H_0\)</span> holds, then <span class="math inline">\(\mbox{SSR}\)</span> is small and <span class="math inline">\(\mbox{SSE}\)</span> large. Therefore large values of <span class="math inline">\(\mbox{SSR}\)</span> relative to <span class="math inline">\(\mbox{SSE}\)</span> provide evidence against <span class="math inline">\(H_0\)</span>.</p>
<p>The F-statistic is:</p>
<p><span class="math display">\[F=\frac{\mbox{SSR}/df_R}{\mbox{SSE}/df_E}.\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(df_R=1\)</span> is the degrees of freedom of <span class="math inline">\(\mbox{SSR}\)</span> and</li>
<li>
<span class="math inline">\(df_E=n-2\)</span> is the degrees of freedom of <span class="math inline">\(\mbox{SSE}\)</span>.</li>
</ul>
<p>Under <span class="math inline">\(H_0\)</span>,<span class="math inline">\(F \sim F_{1,n-2}\)</span>.</p>
<p>By dividing each SS by the <span class="math inline">\(df\)</span> we put them on a common scale, so that if <span class="math inline">\(H_0\)</span> is true:</p>
<p><span class="math display">\[\mbox{SSR}/1 \approx \mbox{SSE}/(n-2)\]</span>
and
<span class="math display">\[F_{obs}\approx 1.\]</span></p>
<p>Large values of <span class="math inline">\(F_{obs}\)</span> provide evidence against <span class="math inline">\(H_0\)</span>.</p>
<p>P-value: <span class="math inline">\(P( F_{1,n-2} \geq F_{obs})\)</span>.</p>
</div>
<div id="notes-on-the-anova-table-not-examinable" class="section level3" number="3.5.7">
<h3>
<span class="header-section-number">3.5.7</span> Notes on the ANOVA table (not examinable)<a class="anchor" aria-label="anchor" href="#notes-on-the-anova-table-not-examinable"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<span class="math inline">\(\mathbb{E}[\mbox{MSR}] = \sigma^2 + \beta_1^2 S_{xx}\)</span>.</li>
</ul>
<p>Proof:</p>
<p><span class="math display">\[\begin{align*}
\mbox{MSR} &amp; = \sum_{i = 1}^n (\hat{y}_i - \bar{y})^2/1\\
&amp; = \sum_{i = 1}^n (\hat{\beta}_0 + \hat{\beta}_1x_i - \bar{y})^2\\
&amp; = \sum_{i = 1}^n (\bar{y} - \hat{\beta}_1 \bar{x} + \hat{\beta}_1x_i - \bar{y})^2\\
&amp; =\hat{\beta}_1^2\sum_{i = 1}^n (x_i - \bar{x})^2\\
&amp; =\hat{\beta}_1^2 S_{xx}
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[\mbox{MSR}] &amp; = \mathbb{E}[\hat{\beta}_1^2 S_{xx}]\\
&amp; =S_{xx} \mathbb{E}[\hat{\beta}_1^2]\\
&amp; = S_{xx} \left(\mbox{Var}(\hat{\beta}_1) + \mathbb{E}[\hat{\beta}_1]^2 \right)\\
&amp; = S_{xx} \left(\frac{\sigma^2}{S_{xx}} + \beta_1^2 \right)\\
&amp; = \sigma^2 + \beta_1^2 S_{xx}
\end{align*}\]</span></p>
<ul>
<li>
<span class="math inline">\(\mathbb{E}[\mbox{MSE}] = \sigma^2\)</span>.</li>
</ul>
<p>Proof:</p>
<p><span class="math display">\[\begin{align*}
\mbox{MSE} &amp; = \frac{\sum_{i = 1}^n (y_i - \hat{y}_i)^2}{n-2}\\
&amp; = \frac{\sum_{i = 1}^ne_i^2}{n-2}
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[\mbox{MSE}] &amp; = \frac{1}{n-2} \mathbb{E}\left[\sum_{i = 1}^ne_i^2 \right]\\
&amp; = \frac{1}{n-2} \sum_{i = 1}^n\mathbb{E}[e_i^2]\\
&amp; = \frac{1}{n-2} \sum_{i = 1}^n\left( \mbox{Var}(e_i) + \mathbb{E}[e_i]^2 \right)
\end{align*}\]</span></p>
<p>NOTE: <span class="math inline">\(\mathbb{E}[\epsilon_i] = 0\)</span>, <span class="math inline">\(\mbox{Var}(\epsilon_i) = \sigma^2\)</span>, but <span class="math inline">\(\mathbb{E}[e_i] = 0\)</span>, <span class="math inline">\(\mbox{Var}(e_i)= \sigma^2(1- h_{ii})\)</span>. We will revisit this later in the course.</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[\mbox{MSE}] &amp; = \frac{1}{n-2} \sum_{i = 1}^n \left( \sigma^2( 1 - h_{ii}) + 0 \right)\\
&amp; =\frac{1}{n-2} \sum_{i = 1}^n \sigma^2 \left(1-\left(\frac{1}{n} +\frac{(x_i - \bar{x})^2}{S_{xx}}\right)\right)\\
&amp; =\frac{1}{n-2} \sum_{i = 1}^n \left(\sigma^2 - \frac{\sigma^2}{n} -\frac{\sigma^2 (x_i - \bar{x})^2}{S_{xx}}\right)\\
&amp; =\frac{1}{n-2} \left(\sigma^2 n- \sigma^2 -\frac{\sigma^2 }{S_{xx}}\sum_{i = 1}^n(x_i - \bar{x})^2\right)\\
&amp; = \frac{1}{n-2} \left((n-2) \sigma^2\right)\\
&amp; =\sigma^2
\end{align*}\]</span></p>
<ul>
<li><p>Under the <span class="math inline">\(H_0\)</span>, <span class="math inline">\(\beta_1 = 0\)</span> and then <span class="math inline">\(\mathbb{E}[\mbox{MSE}] = \mathbb{E}[\mbox{MSR}]\)</span>.</p></li>
<li><p><span class="math inline">\(\mbox{MSE} = \hat{\sigma}^2\)</span> can be computed using the formula:</p></li>
</ul>
<p><span class="math display">\[\hat{\sigma}^2 = \frac{S_{yy} - \hat{\beta}_1^2S_{xx}}{n-2}.\]</span></p>
</div>
</div>
<div id="sample-correlation" class="section level2" number="3.6">
<h2>
<span class="header-section-number">3.6</span> Sample correlation<a class="anchor" aria-label="anchor" href="#sample-correlation"><i class="fas fa-link"></i></a>
</h2>
<p>The relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> can be examined using a scatterplot <span class="math inline">\((x_i, y_i)\)</span>.</p>
<p>Sample correlation measures the strength and direction of the linear association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. It is defined as:</p>
<p><span class="math display">\[r = \frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}\]</span></p>
<p><span class="math inline">\(r\)</span> is the estimate of the population correlation (<span class="math inline">\(\rho\)</span>) between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<div class="sourceCode" id="cb87"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">hibbs</span><span class="op">$</span><span class="va">vote</span>, <span class="va">hibbs</span><span class="op">$</span><span class="va">growth</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0.7614763</code></pre>
<div class="sourceCode" id="cb89"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">hibbs</span><span class="op">$</span><span class="va">vote</span>, <span class="va">hibbs</span><span class="op">$</span><span class="va">growth</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span></span></code></pre></div>
<pre><code>## [1] 0.5798462</code></pre>
<div id="connection-between-correlation-and-regression" class="section level4" number="3.6.0.1">
<h4>
<span class="header-section-number">3.6.0.1</span> Connection between correlation and regression:<a class="anchor" aria-label="anchor" href="#connection-between-correlation-and-regression"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li><p><span class="math inline">\(\hat{\beta}_1=\sqrt{\frac{\mbox{SST}}{S_{xx}}}r=\frac{s_y}{s_x}r\)</span> where <span class="math inline">\(s_y\)</span> and <span class="math inline">\(s_x\)</span> are the standard deviations of the <span class="math inline">\(y_i\)</span>’s and <span class="math inline">\(x_i\)</span>’s.
Note that if you change the role of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> the resulting regression line would have slope <span class="math inline">\(\frac{s_x}{s_y}r\)</span>.</p></li>
<li><p><span class="math inline">\(r^2 =R^2\)</span> the coefficient of determination.</p></li>
<li><p>In the SLR model variable <span class="math inline">\(x\)</span> is treated as <strong>fixed</strong> and <span class="math inline">\(y\)</span> and <span class="math inline">\(\epsilon\)</span> are <strong>random</strong>. It is convenient to think of the predictor variable as fixed even if it israndom as we are interested in the behaviour of <span class="math inline">\(y\)</span> at various fixed <span class="math inline">\(x\)</span> values.</p></li>
<li><p>One can calculate <span class="math inline">\(r\)</span> for any pair of variables (see next page), but correlation assumes that variables are bivariately normally distributed.</p></li>
<li><p>Whereas correlation treats both variables symmetrically, in regression, the exploratory variable is used to explain or predict the response variable.</p></li>
</ul>
<p>Father and son heights (Galton data)</p>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-49-1.png" width="432" style="display: block; margin: auto;"></div>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-50-1.png" width="480" style="display: block; margin: auto;"></div>
</div>
<div id="examples-of-correlation" class="section level3" number="3.6.1">
<h3>
<span class="header-section-number">3.6.1</span> Examples of correlation<a class="anchor" aria-label="anchor" href="#examples-of-correlation"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-51-1.png" width="480" style="display: block; margin: auto;"></div>
<p>Anscombe data. Can be found in <span class="citation">Fox, Weisberg, and Price (<a href="references.html#ref-carData">2022</a>)</span>.</p>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-52-1.png" width="480" style="display: block; margin: auto;"></div>
<p>In all graphs below, correlation is <span class="math inline">\(r = -0.06\)</span>.</p>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-53-1.png" width="768" style="display: block; margin: auto;"></div>
</div>
<div id="comparison-of-the-correlation-and-coefficient-of-determination-for-two-data-sets." class="section level3" number="3.6.2">
<h3>
<span class="header-section-number">3.6.2</span> Comparison of the correlation and coefficient of determination for two data sets.<a class="anchor" aria-label="anchor" href="#comparison-of-the-correlation-and-coefficient-of-determination-for-two-data-sets."><i class="fas fa-link"></i></a>
</h3>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-54-1.png" width="624" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb91"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">X1</span>, <span class="va">Y1</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span></span></code></pre></div>
<pre><code>## [1] 0.6699889</code></pre>
<div class="sourceCode" id="cb93"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y1</span> <span class="op">~</span> <span class="va">X1</span><span class="op">)</span><span class="op">)</span><span class="op">[</span><span class="fl">8</span><span class="op">]</span></span></code></pre></div>
<pre><code>## $r.squared
## [1] 0.6699889</code></pre>
<div class="sourceCode" id="cb95"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">X2</span>, <span class="va">Y2</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span></span></code></pre></div>
<pre><code>## [1] 0.6895371</code></pre>
<div class="sourceCode" id="cb97"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y2</span> <span class="op">~</span> <span class="va">X2</span><span class="op">)</span><span class="op">)</span><span class="op">[</span><span class="fl">8</span><span class="op">]</span></span></code></pre></div>
<pre><code>## $r.squared
## [1] 0.6895371</code></pre>
</div>
</div>
<div id="assessing-the-simple-linear-regression-model-assumptions" class="section level2" number="3.7">
<h2>
<span class="header-section-number">3.7</span> Assessing the simple linear regression model assumptions<a class="anchor" aria-label="anchor" href="#assessing-the-simple-linear-regression-model-assumptions"><i class="fas fa-link"></i></a>
</h2>
<div id="assumptions-1" class="section level3" number="3.7.1">
<h3>
<span class="header-section-number">3.7.1</span> Assumptions<a class="anchor" aria-label="anchor" href="#assumptions-1"><i class="fas fa-link"></i></a>
</h3>
<p>In the SLR model, we assume that <span class="math inline">\(y_{i} \sim\)</span> N(<span class="math inline">\(\beta_{0} + \beta_{1}x_{i}, \sigma^{2}\)</span>) and that the <span class="math inline">\(y_{i}\)</span>’s are independent.</p>
<p>Equivalently, since <span class="math inline">\(\epsilon_{i} = y_{i} - \beta_{0} - \beta_{1}x_{i}\)</span>, the SLR model assumes that <span class="math inline">\(\epsilon_{i} \sim\)</span> N(0, <span class="math inline">\(\sigma^{2}\)</span>) and the <span class="math inline">\(\epsilon_{i}\)</span>’s are independent and identically distributed.</p>
<p>We want to check the following:</p>
<ol style="list-style-type: decimal">
<li><p>There is a <strong>linear relationship</strong>, i.e. <span class="math inline">\(\mathbb{E}\)</span>[<span class="math inline">\(y_{i}\)</span>] = <span class="math inline">\(\beta_{0} + \beta_{1}x_{i}\)</span>. If the data do not follow a linear relationship then the simple linear regression model is not appropriate.</p></li>
<li><p>The <span class="math inline">\(\epsilon_{i}\)</span>’s have a <strong>constant variance</strong>, i.e. Var(<span class="math inline">\(\epsilon_{i}\)</span>) = <span class="math inline">\(\sigma^{2}\)</span> for all <span class="math inline">\(i\)</span>. If there is not constant variance, the line will summarise the data okay but the parameter estimate standard errors, estimates of <span class="math inline">\(\sigma\)</span> etc, are all based on incorrect assumptions.</p></li>
<li><p>The <span class="math inline">\(\epsilon_{i}\)</span>’s are <strong>independent</strong>.</p></li>
<li><p>The <span class="math inline">\(\epsilon_{i}\)</span>’s are <strong>normally distributed</strong> (with mean 0).</p></li>
</ol>
</div>
<div id="violations-and-consequences" class="section level3" number="3.7.2">
<h3>
<span class="header-section-number">3.7.2</span> Violations and consequences<a class="anchor" aria-label="anchor" href="#violations-and-consequences"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li>Linearity:</li>
</ol>
<ul>
<li>A straight linear relationship may be inadequate.</li>
<li>A straight linear relationship may only be appropriate for most of the data.</li>
<li>When linearity is violated least squares estimates can be biased and standard errors may be inaccurate.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Constant variance:</li>
</ol>
<ul>
<li>When the <strong>variance is not constant</strong> least squares estimate are unbiased but standard errors are inaccurate.</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Independence:</li>
</ol>
<ul>
<li>When there is a <strong>lack of independence</strong> least squares estimates are unbiased but standard errors are seriously affected.</li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li>Normality:</li>
</ol>
<ul>
<li>
<strong>Violations of normality</strong> do not have much impact on estimates and standard errors.</li>
<li>Tests and C.I.’s are not usually seriously affected because of the C.L.T.</li>
</ul>
<!-- ### Hypothetical graphs for s.l.r assumption violations --><!-- Display below is from @ramsey2002statistical, Section 8.3, page 213.  --><!-- ```{r echo = FALSE} --><!-- knitr::include_graphics("Graphic1-9.jpg") --><!-- ``` -->
</div>
<div id="graphical-tools-for-assessment" class="section level3" number="3.7.3">
<h3>
<span class="header-section-number">3.7.3</span> Graphical tools for assessment<a class="anchor" aria-label="anchor" href="#graphical-tools-for-assessment"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li>Plot of <span class="math inline">\(y_i\)</span> versus <span class="math inline">\(x_i\)</span>. If satisfactory use simple linear regression.</li>
</ol>
<p>Sometimes the patterns in the plot of <span class="math inline">\(y_i\)</span> versus <span class="math inline">\(x_i\)</span> are difficult to detect because of the total variability of the response variable is much larger than the variability around the regression line. Scatterplots of residuals vs fits are better at finding patterns because the linear component of the variation in the responses has been removed, leaving a clearer picture about curvature and spread. The plot alerts the user of nonlinearity, non-constant variance and the presence of outliers.</p>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-56-1.png" width="672"></div>
<ol start="2" style="list-style-type: decimal">
<li>Plot of <span class="math inline">\(e_i\)</span> versus <span class="math inline">\(\hat{y}_i\)</span> (or <span class="math inline">\(x_i\)</span>).</li>
</ol>
<ul>
<li>If satisfactory use simple linear regression.</li>
<li>If linearity is violated but <span class="math inline">\(\mathbb{E}[y]\)</span> is monotonic in <span class="math inline">\(x\)</span> and <span class="math inline">\(\mbox{Var}(y)\)</span> is constant, try transforming <span class="math inline">\(x\)</span> and then use simple linear regression.</li>
<li>If linearity is violated and <span class="math inline">\(\mathbb{E}[y]\)</span> is not monotonic, try quadratic regression <span class="math inline">\(\mathbb{E}[y] = \beta_0+\beta_1 x+\beta_2 x^2\)</span> (we will look at this later).</li>
<li>If linearity is violated and <span class="math inline">\(\mbox{Var}(y)\)</span> increases with <span class="math inline">\(\mathbb{E}[y]\)</span>, try transforming y and then use simple linear regression.</li>
<li>If the distribution of <span class="math inline">\(y\)</span> about <span class="math inline">\(\mathbb{E}[y]\)</span> is skewed, i.e. non-normal, then use simple linear regression but report skewness.</li>
<li>If linearity is not violated but <span class="math inline">\(\mbox{Var}(y)\)</span> increases with <span class="math inline">\(\mathbb{E}[y]\)</span>, use weighted regression (we will look at this later).</li>
</ul>
<div class="sourceCode" id="cb99"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lin_mod</span>,<span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-57-1.png" width="672"></div>
<ol start="3" style="list-style-type: decimal">
<li>Normal probability plot</li>
</ol>
<ul>
<li><p>The model assumes normality of <span class="math inline">\(y\)</span> about <span class="math inline">\(\mathbb{E}[y]\)</span>, or, equivalently, normality of <span class="math inline">\(\epsilon\)</span>.</p></li>
<li><p>The residuals <span class="math inline">\(e_i\)</span> approximate <span class="math inline">\(\epsilon\)</span> and should therefore have a normal distribution.</p></li>
<li><p>The normal probability (quantile) plot is a plot of <span class="math inline">\(z_i\)</span> versus <span class="math inline">\(e_i\)</span>, where <span class="math inline">\(z_i\)</span> are quantiles from the standard normal distribution.</p></li>
<li><p>This plot should roughly follow a straight line pattern.</p></li>
</ul>
<div class="sourceCode" id="cb100"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lin_mod</span>,<span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-58-1.png" width="672"></div>
<ol start="4" style="list-style-type: decimal">
<li>Residuals vs. time order</li>
</ol>
<ul>
<li><p>If the data are collected over time, serial correlation or a general time trend may occur.</p></li>
<li><p>A plot of <span class="math inline">\(e_i\)</span> vs. <span class="math inline">\(t_i\)</span> (time of the i<span class="math inline">\(^{th}\)</span> observation) may be examined for patterns.</p></li>
</ul>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-59-1.png" width="672"></div>
<p>Everytime you use SLR you should also draw graphs 1) to 3). Also plot 4) when appropriate.</p>
</div>
<div id="cigarette" class="section level3" number="3.7.4">
<h3>
<span class="header-section-number">3.7.4</span> Cigarette Data<a class="anchor" aria-label="anchor" href="#cigarette"><i class="fas fa-link"></i></a>
</h3>
<p>FDA data on cigarettes, response is carbon monoxide, predictor is nicotine. Data from <span class="citation">McIntyre (<a href="references.html#ref-mcintyre94">1994</a>)</span>.</p>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-60-1.png" width="384" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb101"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = carbon.monoxide ~ nicotine)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.3273 -1.2228  0.2304  1.2700  3.9357 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   1.6647     0.9936   1.675    0.107    
## nicotine     12.3954     1.0542  11.759 3.31e-11 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.828 on 23 degrees of freedom
## Multiple R-squared:  0.8574, Adjusted R-squared:  0.8512 
## F-statistic: 138.3 on 1 and 23 DF,  p-value: 3.312e-11</code></pre>
<div class="sourceCode" id="cb103"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># studres(fit)</span></span></code></pre></div>
<p>To assess the fit construct residual plots</p>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-62-1.png" width="624" style="display: block; margin: auto;"></div>
<p>Plot 1: residuals increasing with the fit, non constant variance.</p>
<p>Plot 2: no indication that the assumption of Normality is unreasonable.</p>
<p>There are 3 unusual observations: 3, 19, 25. Obs 3 has a large negative residual. It is the point on the upper right of the fitted line plot. It is a high influence point, meaning it has a big effect on the fitted line obtained. Obs 19 and 25 have large positive residuals.</p>
<p>We could attempt to improve the fit, refit the model without observation 3.</p>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-63-1.png" width="384" style="display: block; margin: auto;"></div>
<p>Diagnostic plots:</p>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-64-1.png" width="624" style="display: block; margin: auto;"></div>
<p>Plot 1: no linear pattern, but some hint of non-constant variance.</p>
<div id="transformations" class="section level4" number="3.7.4.1">
<h4>
<span class="header-section-number">3.7.4.1</span> Transformations:<a class="anchor" aria-label="anchor" href="#transformations"><i class="fas fa-link"></i></a>
</h4>
<p>How can we pick the best transformation?</p>
<ul>
<li><p>Examine the fitted line plot: linearity, constant variance.</p></li>
<li><p>Examine the residual vs fit plot: no relationship, constant variance, no outliers.</p></li>
<li><p>Check the normality of residuals.</p></li>
<li><p>Check for sensitivity: whether fit would change substantially if extreme points are removed.</p></li>
<li>
<p>One can also compare <span class="math inline">\(R^2\)</span> as long as the <span class="math inline">\(y\)</span> values are on the same scale. Furthermore, <span class="math inline">\(R^2\)</span> doesn’t follow a distribution, so we can’t compare <span class="math inline">\(R^2\)</span> in two models and know that one is meaningfully better.</p>
<p>Note: interpretation changes after transformations.</p>
</li>
</ul>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-65-1.png" width="624" style="display: block; margin: auto;"></div>
<ul>
<li>Row 1: delete outlier</li>
<li>Row 2: use a square root transformation for the predictor. This diminishes the influence of the outlier. The residual plot hints at a small amount of bias.</li>
<li>Row 3: take square root transformations of both the response and the predictor.</li>
<li>Row 4: take log transformations of both the response and the predictor.</li>
</ul>
</div>
</div>
<div id="another-example-diamonds" class="section level3" number="3.7.5">
<h3>
<span class="header-section-number">3.7.5</span> Another Example: Diamonds<a class="anchor" aria-label="anchor" href="#another-example-diamonds"><i class="fas fa-link"></i></a>
</h3>
<p>Dataset from <span class="citation">Wickham (<a href="references.html#ref-ggplot2">2016</a>)</span>.</p>
<div class="sourceCode" id="cb104"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">diamonds</span> <span class="op">|&gt;</span> <span class="fu">glimpse</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code>## Rows: 53,940
## Columns: 10
## $ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.…
## $ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver…
## $ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,…
## $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, …
## $ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64…
## $ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58…
## $ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34…
## $ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.…
## $ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.…
## $ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.…</code></pre>
<div class="sourceCode" id="cb106"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">diamonds</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">carat</span>, y <span class="op">=</span> <span class="va">price</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> </span></code></pre></div>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-66-1.png" width="672"></div>
<p>SLR model does not seem to be a good option!</p>
<p>Let’s fit it anyway and explore the model diagnostics:</p>
<div class="sourceCode" id="cb107"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">diamond_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">price</span><span class="op">~</span><span class="va">carat</span>, <span class="va">diamonds</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">diamond_mod</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = price ~ carat, data = diamonds)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -18585.3   -804.8    -18.9    537.4  12731.7 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -2256.36      13.06  -172.8   &lt;2e-16 ***
## carat        7756.43      14.07   551.4   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1549 on 53938 degrees of freedom
## Multiple R-squared:  0.8493, Adjusted R-squared:  0.8493 
## F-statistic: 3.041e+05 on 1 and 53938 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb109"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">diamond_mod</span>, <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-67-1.png" width="672"></div>
<div class="sourceCode" id="cb110"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">diamond_mod</span>, <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-67-2.png" width="672"></div>
<p>Try a transformation:</p>
<div class="sourceCode" id="cb111"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">diamond_mod2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">price</span><span class="op">)</span><span class="op">~</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">carat</span><span class="op">)</span>, <span class="va">diamonds</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">diamond_mod2</span>, <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-68-1.png" width="672"></div>
<div class="sourceCode" id="cb112"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">diamond_mod2</span>, <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-68-2.png" width="672"></div>
</div>
</div>
<div id="a-note-on-the-galton-paradox" class="section level2" number="3.8">
<h2>
<span class="header-section-number">3.8</span> A note on the Galton paradox<a class="anchor" aria-label="anchor" href="#a-note-on-the-galton-paradox"><i class="fas fa-link"></i></a>
</h2>
<!-- ###History of regression -->
<!-- * A. De Moivre (1733) -  the first statement of an approximation to the binomial distribution in terms of the normal (or Gaussian) function. -->
<!-- * A.M. Legendre. Nouvelles m\'ethodes pour la d\'etermination des orbites des com\`etes, (1805). Sur la M\'ethode des moindres carr\'es is an appendix. No link to the theory of probability  Firmin Didot, Paris, -->
<!-- * C.F. Gauss. Theoria Motus Corporum Coelestium in Sectionibus Conicis Solem Ambientum. (1809) - connected the method of least squares with the principles of probability and to the normal distribution (which he invented in the process). He specified a mathematical form of the probability density for the observations and defined a method of estimation that minimizes the error of estimation. Gauss showed that if the errors are normally distributed, the least squares estimates give the most probable values for the coefficients. -->
<!-- * S. Laplace (1810-1811)  - proved CLT (motivated by Gauss). Used CLT to give a large sample justification for the method of LS and the normal distribution. -->
<!-- * C.F. Gauss. Theoria combinationis observationum erroribus minimis obnoxiae. (1821/1823) - in a linear model where the errors have a mean of zero, are uncorrelated, and have equal variances, the best linear unbiased estimator of the coefficients is the least-squares estimator. -->
<!-- * R. Adrain (1808) Research concerning the probabilities of the errors which happen in making observations, etc. Independently formulated LS working in survey measurements. -->
<!-- * F. Galton (1886). Regression Towards Mediocrity in Hereditary Stature. - correlation. -->
<!-- ```{r echo = FALSE} -->
<!-- knitr::include_graphics("deMoivre.jpg") -->
<!-- knitr::include_graphics("Legendre.jpg") -->
<!-- knitr::include_graphics("Gauss.jpg") -->
<!-- knitr::include_graphics("Laplace.jpg") -->
<!-- knitr::include_graphics("Adrain.jpg") -->
<!-- knitr::include_graphics("Galton.jpg") -->
<!-- ``` -->
<div id="the-galton-paradox" class="section level3" number="3.8.1">
<h3>
<span class="header-section-number">3.8.1</span> The Galton paradox<a class="anchor" aria-label="anchor" href="#the-galton-paradox"><i class="fas fa-link"></i></a>
</h3>
<p>Sons are on average taller than their fathers (by 1 inch approx)</p>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-69-1.png" width="288" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb113"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">father.son</span>, <span class="fl">2</span>, <span class="va">mean</span><span class="op">)</span></span></code></pre></div>
<pre><code>##   father      son 
## 67.68683 68.68423</code></pre>
<p>Taller than average fathers have taller than average sons. Regression towards the mean: although the above is true, for these tall people, the son’s height was on average less than the father’s.</p>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-71-1.png" width="384" style="display: block; margin: auto;"></div>
<p>The suggestion is that each generation would have offspring more near average than the previous generation and that over many generations the offspring would be of uniform heigth. However, the observations showed the sons as variable as the fathers.</p>
<div class="sourceCode" id="cb115"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">father.son</span>, <span class="fl">2</span>, <span class="va">sd</span><span class="op">)</span></span></code></pre></div>
<pre><code>##   father      son 
## 2.745827 2.816194</code></pre>
<p>An apparent paradox?</p>
</div>
<div id="two-regressions" class="section level3" number="3.8.2">
<h3>
<span class="header-section-number">3.8.2</span> Two regressions<a class="anchor" aria-label="anchor" href="#two-regressions"><i class="fas fa-link"></i></a>
</h3>
<p>Regressing <span class="math inline">\(y\)</span> on <span class="math inline">\(x\)</span>, treats <span class="math inline">\(x\)</span> variable as fixed and only vertical distances are minimized.
Howevever, regressing <span class="math inline">\(x\)</span> on <span class="math inline">\(y\)</span>, i.e. trying to predict the fathers’ heights from their sons’ treats the sons’ heights <span class="math inline">\(y\)</span> as fixed and the least squares criterion minimizes the horizontal distances.</p>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-73-1.png" width="480" style="display: block; margin: auto;"></div>
</div>
<div id="regression-vs-orthogonal-regression" class="section level3" number="3.8.3">
<h3>
<span class="header-section-number">3.8.3</span> Regression vs orthogonal regression<a class="anchor" aria-label="anchor" href="#regression-vs-orthogonal-regression"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-74-1.png" width="624" style="display: block; margin: auto;"></div>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-75-1.png" width="624" style="display: block; margin: auto;"></div>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="intro.html"><span class="header-section-number">2</span> Introduction</a></div>
<div class="next"><a href="multiple-regression.html"><span class="header-section-number">4</span> Multiple regression</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#SLR"><span class="header-section-number">3</span> Simple Linear regression</a></li>
<li>
<a class="nav-link" href="#OLS"><span class="header-section-number">3.1</span> Ordinary least squares</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#residuals"><span class="header-section-number">3.1.1</span> Residuals</a></li>
<li><a class="nav-link" href="#some-algebraic-implications-of-the-ols-fit"><span class="header-section-number">3.1.2</span> Some algebraic implications of the OLS fit</a></li>
<li><a class="nav-link" href="#ols-estimates-for-the-fuel-consumption-example"><span class="header-section-number">3.1.3</span> OLS Estimates for the Fuel Consumption Example</a></li>
<li><a class="nav-link" href="#interpretation-of-the-fitted-simple-linear-regression-line-parameter-estimates"><span class="header-section-number">3.1.4</span> Interpretation of the fitted simple linear regression line: Parameter estimates</a></li>
<li><a class="nav-link" href="#predicting"><span class="header-section-number">3.1.5</span> Predicting</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#the-formal-simple-linear-regression-model"><span class="header-section-number">3.2</span> The formal simple linear regression model</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#model"><span class="header-section-number">3.2.1</span> Model</a></li>
<li><a class="nav-link" href="#assumptions"><span class="header-section-number">3.2.2</span> Assumptions</a></li>
<li><a class="nav-link" href="#estimation-of-sigma2"><span class="header-section-number">3.2.3</span> Estimation of \(\sigma^2\)</a></li>
<li><a class="nav-link" href="#review-of-some-probability-results"><span class="header-section-number">3.2.4</span> Review of some probability results</a></li>
<li><a class="nav-link" href="#prop"><span class="header-section-number">3.2.5</span> Properties of the estimates</a></li>
<li><a class="nav-link" href="#special-cases"><span class="header-section-number">3.2.6</span> Special cases</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#simple-linear-regression-models-in-r"><span class="header-section-number">3.3</span> Simple linear regression models in R</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#r"><span class="header-section-number">3.3.1</span> R</a></li></ul>
</li>
<li>
<a class="nav-link" href="#statistical-inference"><span class="header-section-number">3.4</span> Statistical inference</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#r-simulation"><span class="header-section-number">3.4.1</span> R simulation:</a></li>
<li><a class="nav-link" href="#inference-for-beta_1"><span class="header-section-number">3.4.2</span> Inference for \(\beta_1\)</a></li>
<li><a class="nav-link" href="#inference-for-beta_0"><span class="header-section-number">3.4.3</span> Inference for \(\beta_0\)</a></li>
<li><a class="nav-link" href="#inference-for-mean-response"><span class="header-section-number">3.4.4</span> Inference for mean response</a></li>
<li><a class="nav-link" href="#inference-for-prediction"><span class="header-section-number">3.4.5</span> Inference for prediction</a></li>
<li><a class="nav-link" href="#plotting-cis-and-pis-in-r"><span class="header-section-number">3.4.6</span> Plotting CIs and PIs in R</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#analysis-of-variance-for-s.l.r."><span class="header-section-number">3.5</span> Analysis of variance (for s.l.r.)</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#vote-data-does-regression-on-growth-explain-vote"><span class="header-section-number">3.5.1</span> Vote Data: does regression on growth explain vote?</a></li>
<li><a class="nav-link" href="#anova-decomposition"><span class="header-section-number">3.5.2</span> ANOVA decomposition</a></li>
<li><a class="nav-link" href="#for-the-voting-data"><span class="header-section-number">3.5.3</span> For the voting data</a></li>
<li><a class="nav-link" href="#anova-table"><span class="header-section-number">3.5.4</span> ANOVA table</a></li>
<li><a class="nav-link" href="#special-cases-1"><span class="header-section-number">3.5.5</span> Special cases</a></li>
<li><a class="nav-link" href="#does-regression-on-x-explain-y"><span class="header-section-number">3.5.6</span> Does regression on x explain y?</a></li>
<li><a class="nav-link" href="#notes-on-the-anova-table-not-examinable"><span class="header-section-number">3.5.7</span> Notes on the ANOVA table (not examinable)</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#sample-correlation"><span class="header-section-number">3.6</span> Sample correlation</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#examples-of-correlation"><span class="header-section-number">3.6.1</span> Examples of correlation</a></li>
<li><a class="nav-link" href="#comparison-of-the-correlation-and-coefficient-of-determination-for-two-data-sets."><span class="header-section-number">3.6.2</span> Comparison of the correlation and coefficient of determination for two data sets.</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#assessing-the-simple-linear-regression-model-assumptions"><span class="header-section-number">3.7</span> Assessing the simple linear regression model assumptions</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#assumptions-1"><span class="header-section-number">3.7.1</span> Assumptions</a></li>
<li><a class="nav-link" href="#violations-and-consequences"><span class="header-section-number">3.7.2</span> Violations and consequences</a></li>
<li><a class="nav-link" href="#graphical-tools-for-assessment"><span class="header-section-number">3.7.3</span> Graphical tools for assessment</a></li>
<li><a class="nav-link" href="#cigarette"><span class="header-section-number">3.7.4</span> Cigarette Data</a></li>
<li><a class="nav-link" href="#another-example-diamonds"><span class="header-section-number">3.7.5</span> Another Example: Diamonds</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#a-note-on-the-galton-paradox"><span class="header-section-number">3.8</span> A note on the Galton paradox</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#the-galton-paradox"><span class="header-section-number">3.8.1</span> The Galton paradox</a></li>
<li><a class="nav-link" href="#two-regressions"><span class="header-section-number">3.8.2</span> Two regressions</a></li>
<li><a class="nav-link" href="#regression-vs-orthogonal-regression"><span class="header-section-number">3.8.3</span> Regression vs orthogonal regression</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Linear Models Lecture Notes</strong>" was written by Katarina Domijan. It was last built on 2025-12-16.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
