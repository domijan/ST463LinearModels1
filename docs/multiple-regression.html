<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 4 Multiple regression | Linear Models Lecture Notes</title>
<meta name="author" content="Katarina Domijan">
<meta name="description" content="4.1 Introductory examples    Setup: response variable \(y\), predictors \(x_1\), \(x_2\), …, \(x_k\).  4.1.1 Example 1: Fuel Use Example from Section 2. Information was recorded on fuel usage and...">
<meta name="generator" content="bookdown 0.46 with bs4_book()">
<meta property="og:title" content="Chapter 4 Multiple regression | Linear Models Lecture Notes">
<meta property="og:type" content="book">
<meta property="og:description" content="4.1 Introductory examples    Setup: response variable \(y\), predictors \(x_1\), \(x_2\), …, \(x_k\).  4.1.1 Example 1: Fuel Use Example from Section 2. Information was recorded on fuel usage and...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 4 Multiple regression | Linear Models Lecture Notes">
<meta name="twitter:description" content="4.1 Introductory examples    Setup: response variable \(y\), predictors \(x_1\), \(x_2\), …, \(x_k\).  4.1.1 Example 1: Fuel Use Example from Section 2. Information was recorded on fuel usage and...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
          margin-bottom: 0em;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Linear Models Lecture Notes</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Module Preliminaries</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">2</span> Introduction</a></li>
<li><a class="" href="SLR.html"><span class="header-section-number">3</span> Simple Linear regression</a></li>
<li><a class="active" href="multiple-regression.html"><span class="header-section-number">4</span> Multiple regression</a></li>
<li><a class="" href="model-comparisons-and-testing-for-lack-of-fit.html"><span class="header-section-number">5</span> Model comparisons and testing for lack of fit</a></li>
<li><a class="" href="diagnostic-methods-in-more-details.html"><span class="header-section-number">6</span> Diagnostic methods (in more details)</a></li>
<li><a class="" href="special-cases-of-multiple-regression.html"><span class="header-section-number">7</span> Special cases of multiple regression</a></li>
<li><a class="" href="about.html"><span class="header-section-number">8</span> About</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="multiple-regression" class="section level1" number="4">
<h1>
<span class="header-section-number">4</span> Multiple regression<a class="anchor" aria-label="anchor" href="#multiple-regression"><i class="fas fa-link"></i></a>
</h1>
<div id="introductory-examples-1" class="section level2" number="4.1">
<h2>
<span class="header-section-number">4.1</span> Introductory examples<a class="anchor" aria-label="anchor" href="#introductory-examples-1"><i class="fas fa-link"></i></a>
</h2>
<!-- # ```{r, echo=FALSE} -->
<!-- # library(rgl) -->
<!-- # library(htmltools) -->
<!-- # knitr::knit_hooks$set(webgl = hook_webgl) -->
<!-- #  -->
<!-- # ``` -->
<p>Setup: response variable <span class="math inline">\(y\)</span>, predictors <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, …, <span class="math inline">\(x_k\)</span>.</p>
<div id="example-1-fuel-use" class="section level3" number="4.1.1">
<h3>
<span class="header-section-number">4.1.1</span> Example 1: Fuel Use<a class="anchor" aria-label="anchor" href="#example-1-fuel-use"><i class="fas fa-link"></i></a>
</h3>
<p>Example from Section <a href="intro.html#intro">2</a>. Information was recorded on fuel usage and average temperature (<span class="math inline">\(^oF\)</span>) over the course of one week for eight office complexes of similar size. Data are from <span class="citation">Bowerman and Schafer (<a href="references.html#ref-bowerman1990linear">1990</a>)</span>.</p>
<p><span class="math inline">\(y\)</span> = fuel use,</p>
<p><span class="math inline">\(x_1\)</span> = temperature,</p>
<p><span class="math inline">\(x_2\)</span> = chill index.</p>
<p>Data:</p>
<!-- \begin{tabular}{ c c c c} -->
<!-- Week & $y$ = fuel use & $x_1$ = temperature & $x_2 = $ chill index \\ \hline -->
<!-- 1 & $y_1$ & $x_{11} $ & $x_{12} $\\ -->
<!-- 2 & $y_2$ & $x_{21} $ & $x_{22} $\\ -->
<!-- \vdots & \vdots& \vdots& \vdots \\ -->
<!-- 8 & $y_8$ & $x_{81} $ & $x_{82} $\\ -->
<!-- \end{tabular} -->
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="right">Temp</th>
<th align="right">Fuel</th>
<th align="right">Chill</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right">28.0</td>
<td align="right">12.4</td>
<td align="right">18</td>
</tr>
<tr class="even">
<td align="right">28.0</td>
<td align="right">11.7</td>
<td align="right">14</td>
</tr>
<tr class="odd">
<td align="right">32.5</td>
<td align="right">12.4</td>
<td align="right">24</td>
</tr>
<tr class="even">
<td align="right">39.0</td>
<td align="right">10.8</td>
<td align="right">22</td>
</tr>
<tr class="odd">
<td align="right">45.9</td>
<td align="right">9.4</td>
<td align="right">8</td>
</tr>
<tr class="even">
<td align="right">57.8</td>
<td align="right">9.5</td>
<td align="right">16</td>
</tr>
<tr class="odd">
<td align="right">58.1</td>
<td align="right">8.0</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">62.5</td>
<td align="right">7.5</td>
<td align="right">0</td>
</tr>
</tbody>
</table></div>
<p>We wish to use <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> to predict <span class="math inline">\(y\)</span>. This should give more accurate predictions than either <span class="math inline">\(x_1\)</span> or <span class="math inline">\(x_2\)</span> alone.</p>
<p>A multiple linear regression model is: fuel use <span class="math inline">\(\approx\)</span> a linear function of temperature and chill index.</p>
<p>More precisely:</p>
<p><span class="math display">\[y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon.\]</span></p>
<p>As before, <span class="math inline">\(\epsilon\)</span> is the unobserved error, <span class="math inline">\(\beta_0, \beta_1, \beta_2\)</span> are the unknown parameters.</p>
<p>When <span class="math inline">\(\mathbb{E}[\epsilon ] = 0\)</span> we have</p>
<p><span class="math display">\[\mathbb{E}[y] = \beta_0 + \beta_1 x_1 + \beta_2 x_2.\]</span></p>
<p>In SLR we can check model appropriateness by plotting <span class="math inline">\(y\)</span> vs <span class="math inline">\(x\)</span> and observing whether the points fall close to a line. Here we could construct a 3-d plot of <span class="math inline">\(y\)</span>, <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span> and points should fall close to a plane.</p>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-77-1.png" width="384" style="display: block; margin: auto;"></div>
<p>For a given set of values of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, say <span class="math inline">\(x_1 = 45.9\)</span> and <span class="math inline">\(x_2 = 8\)</span>, the model says that the <strong>mean</strong> fuel use is:</p>
<p><span class="math display">\[\mathbb{E}[y] = \beta_0 + \beta_1 \times 45.9 + \beta_2 \times 8.\]</span></p>
<p>If <span class="math inline">\(x_1 = x_2 = 0\)</span> then <span class="math inline">\(\mathbb{E}[y] = \beta_0\)</span>, the model intercept.</p>
<p>To interpret <span class="math inline">\(\beta_1\)</span> suppose <span class="math inline">\(x_1 = t\)</span> and <span class="math inline">\(x_2 = c\)</span>. Then</p>
<p><span class="math display">\[\mathbb{E}[y]=\beta_0 + \beta_1 \times t + \beta_2 \times c.\]</span></p>
<p>Now suppose <span class="math inline">\(x_1\)</span> increases by <span class="math inline">\(1\)</span> and <span class="math inline">\(x_2\)</span> stays fixed:</p>
<p><span class="math display">\[\mathbb{E}[y]=\beta_0 + \beta_1 \times (t + 1) + \beta_2 \times c.\]</span></p>
<p>Substracting these we find that <span class="math inline">\(\beta_1\)</span> is the increase in <span class="math inline">\(\mathbb{E}[y]\)</span> associated with 1 unit increase in <span class="math inline">\(x_1\)</span> for a fixed <span class="math inline">\(x_2\)</span>.</p>
<p>I.e. two weeks having the same chill index but whose temperature differed by <span class="math inline">\(1^o\)</span> would have a mean fuel use difference of <span class="math inline">\(\beta_1\)</span>.</p>
</div>
<div id="example-2-categorical-predictors" class="section level3" number="4.1.2">
<h3>
<span class="header-section-number">4.1.2</span> Example 2: Categorical predictors<a class="anchor" aria-label="anchor" href="#example-2-categorical-predictors"><i class="fas fa-link"></i></a>
</h3>
<p>Suppose we wish to predict the fuel efficiency of different car types. Data are from <span class="citation">Cryer and Miller (<a href="references.html#ref-cryer1991statistics">1991</a>)</span>. We have data on:</p>
<p><span class="math inline">\(y\)</span> = gallons per mile (gpm),</p>
<p><span class="math inline">\(x_1\)</span> = car weight (w),</p>
<p><span class="math inline">\(x_2\)</span> = transmission type (ttype): 1 = automatic or 0 = manual.</p>
<p>We use the model</p>
<p><span class="math display">\[\mathbb{E}[y] = \beta_0 + \beta_1 x_1 + \beta_2 x_2.\]</span></p>
<p><span class="math inline">\(\beta_0\)</span> = the mean gpm for cars of weight <span class="math inline">\(w = 0\)</span> and ttype = manual.
<span class="math inline">\(\beta_1\)</span> = change in mean gpm when weight increases by 1 for the same ttype.
<span class="math inline">\(\beta_2\)</span> = change in mean gpm when the car of the same weight is changed from manual to automatic.</p>
<p>The model says that:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[y] &amp; = \beta_0 + \beta_1 x_1 \quad \mbox{ for manual}\\
&amp; = \beta_0 + \beta_2 + \beta_1 x_1 \quad \mbox{ for automatic.}
\end{align*}\]</span></p>
<p>Therefore we are fitting two lines with different intercepts but the same slope.</p>
<p>The data should look like:</p>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-78-1.png" width="384" style="display: block; margin: auto;"></div>
<p>Suppose the data look like this:</p>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-79-1.png" width="384" style="display: block; margin: auto;"></div>
<p>This suggests we should fit two lines with different intercepts and different slopes. We introduce a third predictor:</p>
<p><span class="math display">\[\mathbb{E}[y] = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1x_2,\]</span></p>
<p>giving:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[y] &amp; = \beta_0 + \beta_1 x_1 \quad \mbox{ for manual}\\
&amp; = (\beta_0 + \beta_2) + (\beta_1 + \beta_3) x_1 \quad \mbox{ for automatic.}
\end{align*}\]</span></p>
<p>The term <span class="math inline">\(x_1x_2\)</span> is called an interaction term.</p>
<p>Here:</p>
<p><span class="math inline">\(\beta_2\)</span> = difference in intercept</p>
<p><span class="math inline">\(\beta_3\)</span> = difference in slope.</p>
</div>
<div id="example-3-polynomials" class="section level3" number="4.1.3">
<h3>
<span class="header-section-number">4.1.3</span> Example 3: Polynomials<a class="anchor" aria-label="anchor" href="#example-3-polynomials"><i class="fas fa-link"></i></a>
</h3>
<p>We have one predictor <span class="math inline">\(x\)</span> but the plot of <span class="math inline">\(y\)</span> vs <span class="math inline">\(x\)</span> exhibits a quadratic pattern.</p>
<div class="inline-figure">
<img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-80-1.png" width="384" style="display: block; margin: auto;">
Then we can fit a multiple regression model:</div>
<p><span class="math display">\[\mathbb{E}[y] = \beta_0 + \beta_1 x + \beta_2 x^2.\]</span></p>
<p>This is also called a quadratic regression model or, more generally, a polynomial regression model.</p>
<p>Higher order polynomial regression models can also be used if needed.</p>
<!-- ```{r results='hide',echo=FALSE, webgl=TRUE} -->
<!-- # plot3d(x1, x1^2, y) -->
<!-- x <- seq(0,1, by = 0.1) -->
<!-- x2 <- x^2 -->
<!-- f <- function(x,x2) { r <-  0.2 + 2 * x + 5 * x2 } -->
<!-- z <- outer(x, x2, f) -->
<!-- persp3d(x, x2, z,  col = "light blue", xlab = "X1", ylab = "X1^2", zlab = "Y") -->
<!-- points3d(x1,x1^2,y,col="red") -->
<!-- ``` -->
<p>Link: <a href="http://www.rpubs.com/kdomijan/333155" class="uri">http://www.rpubs.com/kdomijan/333155</a></p>
</div>
<div id="example-4-nonlinear-relationships" class="section level3" number="4.1.4">
<h3>
<span class="header-section-number">4.1.4</span> Example 4: Nonlinear relationships<a class="anchor" aria-label="anchor" href="#example-4-nonlinear-relationships"><i class="fas fa-link"></i></a>
</h3>
<p>For example,</p>
<p><span class="math display">\[y = \alpha x_1 ^{\beta x_2} \epsilon.\]</span></p>
<p>Nonlinear models can sometimes be linearized, for example:</p>
<p><span class="math display">\[log(y) = log(\alpha) + \beta x_2 log(x_1) + log(\epsilon).\]</span></p>
<p>Here: <span class="math inline">\(x = x_2 log(x_1)\)</span>.</p>
<p>NOTE: the term linear refers to the linearity of regression parameters.</p>
<p>A general form for multiple linear regression model (with two explanatory variables):</p>
<p><span class="math display">\[y = \beta_0 f_0(x_1, x_2) + \beta_1 f_1(x_1, x_2) + \beta_2 f_2(x_1, x_2) + \dots\]</span></p>
<p>where <span class="math inline">\(f_j(x_1, x_2)\)</span> are known functions of explanatory variables.</p>
<p>The extension to more than two explanatory variables is straightforward.</p>
</div>
<div id="diamonds-data" class="section level3" number="4.1.5">
<h3>
<span class="header-section-number">4.1.5</span> Diamonds data<a class="anchor" aria-label="anchor" href="#diamonds-data"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb117"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">diamonds</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">carat</span>, y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">price</span><span class="op">)</span>, col <span class="op">=</span> <span class="va">color</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html">facet_wrap</a></span><span class="op">(</span><span class="op">~</span><span class="va">clarity</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-81-1.png" width="672"></div>
<div class="sourceCode" id="cb118"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># model.orig &lt;- lm(log(price) ~ poly(carat,2) + color + clarity +cut,  diamonds)</span></span>
<span><span class="co"># summary(model.orig)</span></span></code></pre></div>
</div>
<div id="cigarette-data-continued" class="section level3" number="4.1.6">
<h3>
<span class="header-section-number">4.1.6</span> Cigarette Data continued<a class="anchor" aria-label="anchor" href="#cigarette-data-continued"><i class="fas fa-link"></i></a>
</h3>
<p>Data from <a href="SLR.html#cigarette">3.7.4</a>.
Consider a second predictor (weight):</p>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-82-1.png" width="624" style="display: block; margin: auto;"></div>
<p>Regression (nicotine only)</p>
<div class="sourceCode" id="cb119"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = carbon.monoxide ~ nicotine)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.3273 -1.2228  0.2304  1.2700  3.9357 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   1.6647     0.9936   1.675    0.107    
## nicotine     12.3954     1.0542  11.759 3.31e-11 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.828 on 23 degrees of freedom
## Multiple R-squared:  0.8574, Adjusted R-squared:  0.8512 
## F-statistic: 138.3 on 1 and 23 DF,  p-value: 3.312e-11</code></pre>
<p>Regression (weight only)</p>
<div class="sourceCode" id="cb121"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fit2</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = carbon.monoxide ~ weight)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -6.524 -2.533  0.622  2.842  7.268 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)  -11.795      9.722  -1.213   0.2373  
## weight        25.068      9.980   2.512   0.0195 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 4.289 on 23 degrees of freedom
## Multiple R-squared:  0.2153, Adjusted R-squared:  0.1811 
## F-statistic: 6.309 on 1 and 23 DF,  p-value: 0.01948</code></pre>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-85-1.png" width="624" style="display: block; margin: auto;"></div>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-86-1.png" width="576" style="display: block; margin: auto;"></div>
<p>Regression (both predictors)</p>
<div class="sourceCode" id="cb123"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">carbon.monoxide</span> <span class="op">~</span> <span class="va">weight</span> <span class="op">+</span> <span class="va">nicotine</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fit3</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = carbon.monoxide ~ weight + nicotine)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.3304 -1.2249  0.2314  1.2677  3.9371 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.61398    4.44663   0.363    0.720    
## weight       0.05883    5.02395   0.012    0.991    
## nicotine    12.38812    1.24473   9.952 1.32e-09 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.87 on 22 degrees of freedom
## Multiple R-squared:  0.8574, Adjusted R-squared:  0.8444 
## F-statistic: 66.13 on 2 and 22 DF,  p-value: 4.966e-10</code></pre>
<p>Regression (quadratic)</p>
<div class="sourceCode" id="cb125"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">nicotine.sq</span> <span class="op">&lt;-</span> <span class="va">nicotine</span><span class="op">^</span><span class="fl">2</span></span>
<span><span class="va">fit4</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">carbon.monoxide</span> <span class="op">~</span> <span class="va">nicotine</span> <span class="op">+</span> <span class="va">nicotine.sq</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fit4</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = carbon.monoxide ~ nicotine + nicotine.sq)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.9857 -1.1052  0.1834  0.8654  3.4145 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   -1.784      1.453  -1.227  0.23264    
## nicotine      20.111      2.775   7.248 2.92e-07 ***
## nicotine.sq   -3.730      1.267  -2.945  0.00749 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.583 on 22 degrees of freedom
## Multiple R-squared:  0.8977, Adjusted R-squared:  0.8884 
## F-statistic: 96.53 on 2 and 22 DF,  p-value: 1.284e-11</code></pre>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-89-1.png" width="288" style="display: block; margin: auto;"></div>
</div>
</div>
<div id="least-squares-estimation-for-multiple-regression" class="section level2" number="4.2">
<h2>
<span class="header-section-number">4.2</span> Least squares estimation for multiple regression<a class="anchor" aria-label="anchor" href="#least-squares-estimation-for-multiple-regression"><i class="fas fa-link"></i></a>
</h2>
<p>Our model states that:</p>
<p><span class="math display">\[y = \beta_0 + \beta_1x_{1} + \beta_2x_{2} + ... + \beta_kx_k + \epsilon,\]</span></p>
<p>where <span class="math inline">\(k&lt;n\)</span>.</p>
<p>For each observation we have:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + ... + \beta_kx_{ik} + \epsilon_i.\]</span></p>
<p>We can write this more compactly using matrix notation.</p>
<p>Let <span class="math inline">\(\mathbf{Y}\)</span> be the <strong>response vector</strong>:</p>
<p><span class="math display">\[\mathbf{Y} =\begin{bmatrix}
y_{1} \\
y_{2} \\
\vdots\\
y_{n}
\end{bmatrix}\]</span></p>
<p>Let <span class="math inline">\(\mathbf{X}\)</span> be the <span class="math inline">\(n \times p\)</span> matrix, where <span class="math inline">\(p = k+1\)</span>:</p>
<p><span class="math display">\[\mathbf{X} = \begin{bmatrix}
1 &amp; x_{11} &amp; x_{12} &amp; x_{13} &amp; \dots  &amp; x_{1k} \\
1 &amp; x_{21} &amp; x_{22} &amp; x_{23} &amp; \dots  &amp; x_{2k} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp;  x_{n1} &amp; x_{n2} &amp; x_{n3} &amp; \dots  &amp; x_{nk}
\end{bmatrix}\]</span></p>
<p>Let <span class="math inline">\(\boldsymbol{\beta}\)</span> be the <span class="math inline">\(p\)</span>-dim <strong>parameter vector</strong>:</p>
<p><span class="math display">\[\boldsymbol{\beta} =\begin{bmatrix}
\beta_{0} \\
\beta_{1} \\
\vdots\\
\beta_{k}
\end{bmatrix}\]</span></p>
<p>Let <span class="math inline">\(\boldsymbol{\epsilon}\)</span> be the <span class="math inline">\(n\)</span>-dim <strong>error vector</strong>:</p>
<p><span class="math display">\[\boldsymbol{\epsilon} =\begin{bmatrix}
\epsilon_{1} \\
\epsilon_{2} \\
\vdots\\
\epsilon_{n}
\end{bmatrix}\]</span></p>
<p>The model states that:</p>
<p><span class="math display">\[\mathbf{Y}=\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}.\]</span></p>
<p>The vector of <strong>fitted values</strong> is:</p>
<p><span class="math display">\[\hat{\mathbf{Y}}=\mathbf{X}\hat{\boldsymbol{\beta}}.\]</span></p>
<p>The corresponding <strong>residual values</strong> are:</p>
<p><span class="math display">\[\mathbf{e}=\mathbf{Y}-\hat{\mathbf{Y}}.\]</span></p>
<p>The <strong>OLS estimates</strong> minimise:</p>
<p><span class="math display">\[S(\boldsymbol{\beta}) = \sum_{i=1}^{n}(y_i-\beta_0-\beta_1x_{i1}- ... - \beta_kx_{ik})^2\]</span></p>
<p>over <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
<p>Therefore the OLS estimates satisfy:</p>
<p><span class="math display">\[\frac{\delta S(\boldsymbol{\beta})}{\delta \beta_j} = 0, \quad \forall j\]</span></p>
<p>and as before we evaluate at <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span></p>
<p><span class="math display">\[\frac{\delta S(\boldsymbol{\beta})}{\delta \beta_0} = -2 \sum_{i=1}^{n}(y_i-\beta_0-\beta_1x_{i1}- ... - \beta_kx_{ik})\]</span></p>
<p><span class="math display">\[\frac{\delta S(\boldsymbol{\beta})}{\delta \beta_j} = -2 \sum_{i=1}^{n} x_{ij}(y_i-\beta_0-\beta_1x_{i1}- ... - \beta_kx_{ik}), \quad \forall j = 1,...,k.\]</span></p>
<p>The OLS estimates of <span class="math inline">\(\boldsymbol{\beta}\)</span> satisfy:</p>
<p><span class="math display">\[\sum_{i=1}^{n}(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}- ... - \hat{\beta}_kx_{ik}) = 0\]</span></p>
<p>and</p>
<p><span class="math display">\[\sum_{i=1}^{n}(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}- ... - \hat{\beta}_kx_{ik})x_{ij} = 0, \quad \forall j = 1,...,k.\]</span></p>
<p>These <strong>normal equations</strong> (see <a href="SLR.html#eq:normal1">(3.1)</a> and <a href="SLR.html#eq:normal2">(3.2)</a>) can be written as:</p>
<p><span class="math display">\[\sum_{i=1}^{n}e_i = 0\]</span></p>
<p>and</p>
<p><span class="math display">\[\sum_{i=1}^{n}x_{ij}e_i = 0, \quad \forall j = 1,...,k.\]</span></p>
<p>We can combine this into one matrix equation:</p>
<p><span class="math display">\[\mathbf{X}^T\mathbf{e}= \mathbf{0}\]</span></p>
<p>or equivalently:</p>
<p><span class="math display">\[\mathbf{X}^T(\mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}})= \mathbf{0}\]</span></p>
<p>Therefore the OLS estimator <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> satisfies:</p>
<p><span class="math display" id="eq:betah">\[\begin{align}
\mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}} &amp;= \mathbf{X}^T\mathbf{Y}\\
\hat{\boldsymbol{\beta}} &amp;= (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T\mathbf{Y}.\tag{4.1}
\end{align}\]</span></p>
<p>Matrix <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> is non-singular (i.e has an inverse) iff <span class="math inline">\(rank(\mathbf{X}) =p\)</span>, i.e. <span class="math inline">\(\mathbf{X}\)</span> has full rank and the columns of <span class="math inline">\(\mathbf{X}\)</span> are linearly independent.</p>
<!-- The maximum number of linearly independent vectors in a matrix is equal to the number of non-zero rows in its row echelon matrix. Therefore, to find the rank of a matrix, we simply transform the matrix to its row echelon form and count the number of non-zero rows. -->
<!-- **NOTE**: collinearity (in general) is a relationship between the predictors $\mathbf{x}_k$ in which one or more are almost a linear combination of the others. This causes inflation in the variance of $\hat{\beta}_j$, or of linear combinations of $\hat{\beta}_j$.  -->
<div id="estimation-of-sigma2-varepsilon" class="section level3" number="4.2.1">
<h3>
<span class="header-section-number">4.2.1</span> Estimation of <span class="math inline">\(\sigma^2\)</span> = Var<span class="math inline">\((\epsilon)\)</span><a class="anchor" aria-label="anchor" href="#estimation-of-sigma2-varepsilon"><i class="fas fa-link"></i></a>
</h3>
<p>A point estimate of <span class="math inline">\(\sigma^2\)</span> is the mean squared error:</p>
<p><span class="math display">\[\hat{\sigma}^2 = \mbox{MSE} = \frac{\mbox{SSE}}{n-p} = \frac{\sum_{i=1}^n e_i^2}{n-p}.\]</span></p>
</div>
<div id="estimation-of-varhatbeta" class="section level3" number="4.2.2">
<h3>
<span class="header-section-number">4.2.2</span> Estimation of Var<span class="math inline">\((\hat{\beta})\)</span><a class="anchor" aria-label="anchor" href="#estimation-of-varhatbeta"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math display">\[\mbox{Var}(\hat{\boldsymbol{\beta}}) = (\mathbf{X}^T\mathbf{X})^{-1} \sigma^2.\]</span></p>
<p><span class="math display">\[\widehat{\mbox{Var}(\hat{\boldsymbol{\beta}})} = (\mathbf{X}^T\mathbf{X})^{-1} \hat{\sigma}^2.\]</span></p>
</div>
</div>
<div id="prediction-from-multiple-linear-regression-model" class="section level2" number="4.3">
<h2>
<span class="header-section-number">4.3</span> Prediction from multiple linear regression model<a class="anchor" aria-label="anchor" href="#prediction-from-multiple-linear-regression-model"><i class="fas fa-link"></i></a>
</h2>
<p>As we have seen already, to predict from a multiple regression model we use:</p>
<p><span class="math display">\[\hat{y}_i = \hat{\beta}_0+ \hat{\beta}_1x_{i1}+ \cdots+\hat{\beta}_kx_{ik}\]</span></p>
<p>or <span class="math display">\[\hat{\mathbf{Y}} = \mathbf{X}\hat{\boldsymbol{\beta}}\]</span></p>
<p>At a particular set of <span class="math inline">\(x_0\)</span> values we predict the response <span class="math inline">\(y_0\)</span> by:</p>
<p><span class="math display">\[\hat{y}_0 = \mathbf{x}_0^T\hat{\boldsymbol{\beta}}\]</span></p>
<p>where <span class="math inline">\(\mathbf{x}_0^T = ( 1, x_{01},x_{02},..., x_{0k})\)</span>.</p>
<p>We also use <span class="math inline">\(\hat{y}_0\)</span> to estimate <span class="math inline">\(\mathbb{E}(y_0)\)</span>, the mean of <span class="math inline">\(y_0\)</span> at a given set of <span class="math inline">\(x_0\)</span> values.</p>
<p>The <span class="math inline">\(\mbox{S.E.}\)</span> for the estimate of the mean <span class="math inline">\(\mathbb{E}(y_0)\)</span> is:</p>
<p><span class="math display">\[\mbox{S.E.}_{\mbox{fit}} (\hat{y}_0)= \hat{\sigma}\sqrt{\mathbf{x}_0^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_0}.\]</span></p>
<p>A <span class="math inline">\(1-\alpha\)</span> <strong>confidence interval</strong> for the expected response at <span class="math inline">\(\mathbf{x}_0\)</span> is given by:</p>
<p><span class="math display">\[\hat{y}_0 \pm t_{n-p}(\alpha/2) \times \mbox{S.E.}_{\mbox{fit}} (\hat{y}_0).\]</span></p>
<p>The <span class="math inline">\(\mbox{S.E.}\)</span> for the predicted <span class="math inline">\(y_0\)</span>:</p>
<p><span class="math display">\[\mbox{S.E.}_{\mbox{pred}}(\hat{y}_0) = \hat{\sigma}\sqrt{1+ \mathbf{x}_0^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_0}.\]</span></p>
<p>Note: <span class="math display">\[\mbox{S.E.}_{\mbox{pred}}(\hat{y}_0)= \sqrt{\hat{\sigma}^2+\mbox{S.E.}_{\mbox{fit}}(\hat{y}_0)^2}\]</span></p>
</div>
<div id="regression-models-in-matrix-notation-examples" class="section level2" number="4.4">
<h2>
<span class="header-section-number">4.4</span> Regression models in matrix notation: examples<a class="anchor" aria-label="anchor" href="#regression-models-in-matrix-notation-examples"><i class="fas fa-link"></i></a>
</h2>
<div id="example-1-slr" class="section level3" number="4.4.1">
<h3>
<span class="header-section-number">4.4.1</span> Example 1: SLR<a class="anchor" aria-label="anchor" href="#example-1-slr"><i class="fas fa-link"></i></a>
</h3>
<p>The <span class="math inline">\(\mathbf{X}\)</span> matrix is:</p>
<p><span class="math display">\[\mathbf{X} = \begin{bmatrix}
1 &amp; x_{1}\\
\vdots &amp; \vdots\\
1 &amp;x_{n}
\end{bmatrix}\]</span></p>
<p>To estimate the coefficients <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{X}^T\mathbf{X} &amp;= \begin{bmatrix}
n &amp; \sum x_{i}\\
\sum x_{i}&amp; \sum x_{i}^2
\end{bmatrix}\\
(\mathbf{X}^T\mathbf{X})^{-1} &amp; = \frac{1}{n \sum x_{i}^2 - (\sum x_{i})^2}\begin{bmatrix}
\sum x_{i}^2&amp; -\sum x_{i}\\
-\sum x_{i}  &amp; n
\end{bmatrix} \\
&amp; = \frac{1}{n (\sum x_{i}^2 - n\bar{x}^2)}\begin{bmatrix}
\sum x_{i}^2  &amp; -n\bar{x}  \\
-n\bar{x}  &amp; n
\end{bmatrix} \\
&amp; = \frac{1}{S_{xx}}\begin{bmatrix}
\sum x_{i}^2/n  &amp; -\bar{x}  \\
-\bar{x}  &amp; 1
\end{bmatrix} \\
\mathbf{X}^T\mathbf{Y} &amp;= \begin{bmatrix}
\sum y_{i}  \\
\sum x_{i}y_{i}
\end{bmatrix} = \begin{bmatrix}
n\bar{y}  \\
\sum x_{i}y_{i}
\end{bmatrix}\\
\hat{\boldsymbol{\beta}} &amp; = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} \\
&amp; = \frac{1}{S_{xx}}\begin{bmatrix}
\bar{y}\sum x_{i}^2 -\bar{x} \sum x_{i}y_i \\
-n \bar{x} \bar{y} + \sum x_{i}y_i
\end{bmatrix}
\end{align*}\]</span></p>
<p>With some algebra, this gives:</p>
<p><span class="math display">\[\hat{\beta}_1 = \frac{S_{xy}}{S_{xx}}\]</span>
and</p>
<p><span class="math display">\[\hat{\beta}_0= \bar{y} - \hat{\beta}_1\bar{x}\]</span></p>
<p>as before, and</p>
<p><span class="math display">\[\begin{align*}
\mbox{Var}(\hat{\boldsymbol{\beta}})&amp; = (\mathbf{X}^T\mathbf{X})^{-1}\sigma^2\\
&amp; = \frac{\sigma^2}{S_{xx}}
\begin{bmatrix}
\sum x_{i}^2/n&amp; -\bar{x} \\
-\bar{x}&amp; 1
\end{bmatrix}
\end{align*}\]</span></p>
<p>which gives</p>
<p><span class="math display">\[\mbox{Var}(\hat{\beta}_0) = \sigma^2\left(\frac{1}{n}+ \frac{\bar{x}^2}{S_{xx}}\right),\]</span></p>
<p><span class="math display">\[\mbox{Var}(\hat{\beta}_1) = \frac{\sigma^2}{S_{xx}},\]</span></p>
<p><span class="math display">\[\mbox{Cov}(\hat{\beta}_0, \hat{\beta}_1) = -\bar{x}\frac{\sigma^2}{S_{xx}}.\]</span></p>
</div>
<div id="example-2" class="section level3" number="4.4.2">
<h3>
<span class="header-section-number">4.4.2</span> Example 2<a class="anchor" aria-label="anchor" href="#example-2"><i class="fas fa-link"></i></a>
</h3>
<p>Example from <span class="citation">Stapleton (<a href="references.html#ref-stapleton09linear">2009</a>)</span>, Problem 3.1.1, pg 81.</p>
<p>A scale has 2 pans. The measurement given by the scale is the difference between the weight in pan 1 and pan 2, plus a random error <span class="math inline">\(\epsilon\)</span>.</p>
<p>Suppose that <span class="math inline">\(\mathbb{E}[\epsilon] = 0\)</span> and <span class="math inline">\(\mbox{Var}(\epsilon) = \sigma^2\)</span> and the <span class="math inline">\(\epsilon_i\)</span> are independent.</p>
<p>Suppose also that two objects have weight <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> and that 4 measurements are taken:</p>
<ul>
<li>Pan 1: object 1, Pan 2: empty</li>
<li>Pan 1: empty, Pan 2: object 2</li>
<li>Pan 1: object 1, Pan 2: object 2</li>
<li>Pan 1: object 1 and 2, Pan 2: empty</li>
</ul>
<p>Let <span class="math inline">\(y_1\)</span>, <span class="math inline">\(y_2\)</span>, <span class="math inline">\(y_3\)</span> and <span class="math inline">\(y_4\)</span> be the four observations. Then:</p>
<p><span class="math display">\[\begin{align*}
y_1 &amp; = \beta_1 + \epsilon_1\\
y_2 &amp; =- \beta_2 + \epsilon_2\\
y_3 &amp; = \beta_1 - \beta_2 + \epsilon_3\\
y_4 &amp; = \beta_1 + \beta_2 + \epsilon_4\\
\end{align*}\]</span></p>
<p><span class="math inline">\(\mathbf{X} = \begin{bmatrix}
1 &amp;0  \\
0 &amp; -1  \\
1 &amp; -1  \\
1 &amp;  1
\end{bmatrix}\)</span></p>
<p><span class="math inline">\(\mathbf{Y} = \begin{bmatrix}
y_1 \\
y_2 \\
y_3  \\
y_4
\end{bmatrix}\)</span></p>
<p><span class="math inline">\(\boldsymbol{\beta} = \begin{bmatrix}
\beta_1 \\
\beta_2
\end{bmatrix}\)</span></p>
<p><span class="math inline">\(\boldsymbol{\epsilon} = \begin{bmatrix}
\epsilon_1 \\
\epsilon_2 \\
\epsilon_3  \\
\epsilon_4
\end{bmatrix}\)</span></p>
<p>The model is:</p>
<p><span class="math display">\[\mathbf{Y} = \begin{bmatrix}
1 &amp;0  \\
0 &amp; -1  \\
1 &amp; -1  \\
1 &amp;  1
\end{bmatrix} \times\begin{bmatrix}
\beta_1 \\
\beta_2
\end{bmatrix} +  \boldsymbol{\epsilon} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}\]</span></p>
<p>The OLS estimates of <span class="math inline">\(\boldsymbol{\beta}\)</span> are given by:</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T\mathbf{Y}.\]</span></p>
<p><span class="math display">\[\begin{align*}
\mathbf{X}^T\mathbf{X} &amp; = \begin{bmatrix}
1 &amp;  0  &amp;  1 &amp; 1\\
0 &amp; -1  &amp; -1 &amp; 1\\
\end{bmatrix}
\begin{bmatrix}
1 &amp;  0  \\
0 &amp; -1  \\
1 &amp; -1  \\
1 &amp;  1\\
\end{bmatrix}\\
&amp; = \begin{bmatrix}
3 &amp;  0 \\
0 &amp;  3 \\
\end{bmatrix}\\
&amp; = 3\begin{bmatrix}
1 &amp;  0 \\
0 &amp;  1 \\
\end{bmatrix}\\
\mathbf{X}^T\mathbf{Y} &amp;= \begin{bmatrix}
y_1 + y_3 + y_4\\
-y_2 - y_3 + y_4\\
\end{bmatrix}\\
\hat{\boldsymbol{\beta}} &amp; = (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T\mathbf{Y}\\
&amp; = \frac{1}{3}\begin{bmatrix}
1 &amp;  0 \\
0 &amp;  1 \\
\end{bmatrix} \begin{bmatrix}
y_1 + y_3 + y_4\\
-y_2 - y_3 + y_4\\
\end{bmatrix}\\
&amp; = \frac{1}{3}\begin{bmatrix}
y_1 + y_3 + y_4\\
-y_2 - y_3 + y_4\\
\end{bmatrix}\\
\mbox{Var}(\hat{\boldsymbol{\beta}}) &amp;= (\mathbf{X}^T\mathbf{X})^{-1}  \sigma^2  = \frac{1}{3}\begin{bmatrix}
1 &amp;  0 \\
0 &amp;  1 \\
\end{bmatrix}  \sigma^2.\\
\end{align*}\]</span></p>
<p>Can we improve the experiment so that the 4 measurements yield estimates of <span class="math inline">\(\boldsymbol{\beta}\)</span> with smaller variance?</p>
<ul>
<li><p>present design: <span class="math inline">\(\mbox{Var}(\hat{\beta}_i) = \frac{\sigma^2}{3}\)</span></p></li>
<li><p>Let <span class="math inline">\(\mathbf{X} = \begin{bmatrix}
1 &amp; -1  \\
1 &amp; -1  \\
1 &amp;  1  \\
1 &amp;  1
\end{bmatrix}\)</span>,</p></li>
</ul>
<p><span class="math inline">\(\mbox{Var}(\hat{\beta}_i) = \frac{\sigma^2}{4}\)</span></p>
<ul>
<li>Let <span class="math inline">\(\mathbf{X} = \begin{bmatrix}
1 &amp;  0  \\
1 &amp;  0  \\
0 &amp;  1  \\
0 &amp;  1
\end{bmatrix}\)</span>,</li>
</ul>
<p><span class="math inline">\(\mbox{Var}(\hat{\beta}_i) = \frac{\sigma^2}{2}\)</span>.</p>
</div>
</div>
<div id="the-formal-multiple-regression-model-and-properties" class="section level2" number="4.5">
<h2>
<span class="header-section-number">4.5</span> The formal multiple regression model and properties<a class="anchor" aria-label="anchor" href="#the-formal-multiple-regression-model-and-properties"><i class="fas fa-link"></i></a>
</h2>
<div id="concepts-random-vectors-covariance-matrix-multivariate-normal-distribution-mvn." class="section level3" number="4.5.1">
<h3>
<span class="header-section-number">4.5.1</span> Concepts: random vectors, covariance matrix, multivariate normal distribution (MVN).<a class="anchor" aria-label="anchor" href="#concepts-random-vectors-covariance-matrix-multivariate-normal-distribution-mvn."><i class="fas fa-link"></i></a>
</h3>
<p>Let <span class="math inline">\(Y_1,...,Y_n\)</span> be r.v.s defined on a common probability space.</p>
<p>Then <span class="math inline">\(\mathbf{Y}\)</span> is a <strong>random vector</strong>.</p>
<p>Let <span class="math inline">\(\mu_i = \mathbb{E}[y_i]\)</span> and <span class="math inline">\(\boldsymbol{\mu} = \begin{bmatrix}
\mu_1 \\
\vdots \\
\mu_n
\end{bmatrix}\)</span>.</p>
<p>Then <span class="math inline">\(\boldsymbol{\mu}\)</span> is a <strong>mean vector</strong> and we write:</p>
<p><span class="math display">\[\mathbb{E}[\mathbf{Y}] = \boldsymbol{\mu}.\]</span></p>
<p>Let <span class="math inline">\(\sigma_{ij} = Cov(y_i, y_j)\)</span>. Then <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is the <strong>covariance matrix</strong> of <span class="math inline">\(\mathbf{Y}\)</span>, where <span class="math inline">\(\boldsymbol{\Sigma}_{ij} = [\sigma_{ij}].\)</span></p>
<p>We write:</p>
<p><span class="math display">\[\mbox{Var}(\mathbf{Y}) = \boldsymbol{\Sigma}.\]</span></p>
<p>Aside:</p>
<p><span class="math inline">\(\mbox{Cov}(Y_i,Y_j) = \mathbb{E}[(Y_i-\mathbb{E}[Y_i])(Y_j-\mathbb{E}[Y_j])]\)</span></p>
<p><span class="math inline">\(\mbox{Cov}(Y_i,Y_i) = \mbox{Var}(Y_i)\)</span></p>
<p>If <span class="math inline">\(Y_i,Y_j\)</span> are independent then <span class="math inline">\(\mbox{Cov}(Y_i,Y_j) = 0\)</span>.</p>
<p>When <span class="math inline">\(Y_i,Y_j\)</span> have bivariate normal distribution, if <span class="math inline">\(\mbox{Cov}(Y_i,Y_j) = 0\)</span>, then <span class="math inline">\(Y_i,Y_j\)</span> are independent.</p>
<p><strong>Fact:</strong> Suppose <span class="math inline">\(\mathbf{Y}\)</span> has mean <span class="math inline">\(\boldsymbol{\mu}\)</span> and variance <span class="math inline">\(\boldsymbol{\Sigma}\)</span>. Then for a vector of constants <span class="math inline">\(\mathbf{b}\)</span> and matrix of constants <span class="math inline">\(\mathbf{C}\)</span>:</p>
<p><span class="math display">\[\mathbb{E}[\mathbf{C}\mathbf{Y} + \mathbf{b}] = \mathbf{C}\boldsymbol{\mu} + \mathbf{b}\]</span></p>
<p>and</p>
<p><span class="math display">\[\mbox{Var}( \mathbf{C}\mathbf{Y} + \mathbf{b} ) = \mathbf{C}\boldsymbol{\Sigma} \mathbf{C}^T.\]</span></p>
<p><strong>Defn:</strong> A random <span class="math inline">\(n\)</span> - dim vector <span class="math inline">\(\mathbf{Y}\)</span> is said to have a MVN distribution if <span class="math inline">\(\mathbf{Y}\)</span> can be written as
<span class="math display">\[\mathbf{Y} = \mathbf{A}\mathbf{Z} + \boldsymbol{\mu}\]</span>
where:</p>
<ul>
<li><p><span class="math inline">\(Z_1, Z_2, ..., Z_p\)</span> are iid N(0,1),</p></li>
<li><p><span class="math inline">\(\mathbf{A}\)</span> is an <span class="math inline">\(n \times p\)</span> matrix of constants and</p></li>
<li><p><span class="math inline">\(\boldsymbol{\mu}\)</span> is an <span class="math inline">\(n\)</span> vector of constants.</p></li>
</ul>
<p><strong>Notes</strong>:</p>
<ul>
<li><p>Random vector <span class="math inline">\(\mathbf{Z}\)</span> is multivariate normal with mean <span class="math inline">\(\mathbf{0}\)</span> and covariance <span class="math inline">\(\mathbf{I}_p\)</span> since <span class="math inline">\(Z_i\)</span>s are independent so their covariances are 0. We write: <span class="math display">\[\mathbf{Z} \sim N_p(\mathbf{0}, \mathbf{I}_p).\]</span></p></li>
<li><p><span class="math inline">\(\mathbb{E}[\mathbf{Y}] =  \mathbb{E}[\mathbf{A}\mathbf{Z} + \boldsymbol{\mu}] = \boldsymbol{\mu}\)</span>,</p></li>
<li><p><span class="math inline">\(\mbox{Var}(\mathbf{Y}) = \mathbf{A}\mathbf{A}^T\)</span>,</p></li>
<li><p><span class="math inline">\(\mathbf{Y} \sim N_n (\boldsymbol{\mu}, \mathbf{A}\mathbf{A}^T)\)</span>.</p></li>
</ul>
<!-- %item[] We can write $\mathbf{Y} \sim N_n (\boldsymbol{\mu}, \mathbf{A}\mathbf{A}^T)$. -->
</div>
<div id="multiple-regression-model" class="section level3" number="4.5.2">
<h3>
<span class="header-section-number">4.5.2</span> Multiple regression model<a class="anchor" aria-label="anchor" href="#multiple-regression-model"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math display">\[\mathbf{Y}=\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}.\]</span></p>
<p><span class="math inline">\(\mathbf{Y}=\)</span> <span class="math inline">\(n\)</span> - dimensional response random vector.</p>
<p><span class="math inline">\(\boldsymbol{\beta}=\)</span> unknown <span class="math inline">\(p\)</span> - dimensional parameter vector.</p>
<p><span class="math inline">\(\mathbf{X}=\)</span> an <span class="math inline">\(n \times p\)</span> matrix of constants.</p>
<p><span class="math inline">\(\boldsymbol{\epsilon}=\)</span> <span class="math inline">\(n\)</span> - dimensional error vector.</p>
<p><strong>Assumptions:</strong></p>
<ul>
<li><p><strong>Linearity</strong>: <span class="math inline">\(\mathbb{E}[\boldsymbol{\epsilon} ] =\mathbf{0}\)</span>, hence <span class="math inline">\(\mathbb{E}[\mathbf{Y}] = \mathbf{X}\boldsymbol{\beta}\)</span>.</p></li>
<li><p><strong>Constant variance and 0 covariances</strong> <span class="math inline">\(\mbox{Var}(\boldsymbol{\epsilon}) = \sigma^2 I_n\)</span> and <span class="math inline">\(\mbox{Var}(\mathbf{Y}) = \sigma^2 I_n\)</span>.</p></li>
<li><p><strong>MVN distribution:</strong> <span class="math inline">\(\boldsymbol{\epsilon} \sim N_n(\mathbf{0},\sigma^2 I_n )\)</span> and <span class="math inline">\(\mathbf{Y} \sim N_n(\mathbf{X}\boldsymbol{\beta},\sigma^2 I_n )\)</span></p></li>
</ul>
<p>Notes: When the off diagonal entries of the covariance matrix of a MVN distribution are 0, the <span class="math inline">\(Y_1, ..., Y_n\)</span> are independent.</p>
<div class="theorem">
<p><span id="thm:unnamed-chunk-90" class="theorem"><strong>Theorem 4.1  </strong></span>Let <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> be the OLS estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span>. When the model assumptions hold:</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} \sim N_p(\boldsymbol{\beta}, (\mathbf{X}^T\mathbf{X})^{-1}\sigma^2)\]</span></p>
</div>
<p><strong>Corollary</strong>: <span class="math inline">\(\hat{\beta}_j \sim N(\beta_j, c_{jj}\sigma^2)\)</span>, where <span class="math inline">\(c_{jj}\)</span> is the <span class="math inline">\(jj\)</span> entry of <span class="math inline">\((\mathbf{X}^T\mathbf{X})^{-1}\)</span> for <span class="math inline">\(j = 0, ..., k\)</span>.</p>
<div class="theorem">
<p><span id="thm:unnamed-chunk-91" class="theorem"><strong>Theorem 4.2  </strong></span>Let <span class="math inline">\(\hat{\sigma}^2 = \frac{\mbox{SSE}}{n-p}\)</span>. When the model assumptions hold:</p>
<p><span class="math display">\[(n-p)\frac{\hat{\sigma}^2}{\sigma^2} \sim \chi ^2_{(n-p)}\]</span></p>
</div>
<p>and</p>
<p><span class="math display">\[\mathbb{E}[\hat{\sigma}^2] =\sigma^2\]</span></p>
<p>The distribution of <span class="math inline">\(\hat{\sigma}^2\)</span> is independent of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>.</p>
<p><strong>Corollary</strong>:</p>
<p><span class="math display">\[\frac{\hat{\beta}_j - \beta_j }{\hat{\sigma} \sqrt{c_{jj}}} \sim t_{n-p}\]</span>
So we can do tests and obtain CIs for <span class="math inline">\(\beta_j\)</span></p>
</div>
</div>
<div id="the-hat-matrix" class="section level2" number="4.6">
<h2>
<span class="header-section-number">4.6</span> The hat matrix<a class="anchor" aria-label="anchor" href="#the-hat-matrix"><i class="fas fa-link"></i></a>
</h2>
<p>The vector of fitted values:</p>
<p><span class="math display">\[\hat{\mathbf{Y}} = \mathbf{X}\hat{\boldsymbol{\beta}} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y} = \mathbf{H}\mathbf{Y}.\]</span></p>
<p>The hat matrix (also known as the projection matrix):</p>
<p><span class="math display">\[\mathbf{H} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T\]</span></p>
<p>has dimension <span class="math inline">\(n \times n\)</span>
is symmetric (<span class="math inline">\(\mathbf{H}^T = \mathbf{H}\)</span>) and
is idempotent (<span class="math inline">\(\mathbf{H}^2 = \mathbf{H}\mathbf{H} = \mathbf{H}\)</span>).</p>
<p>We have</p>
<p><span class="math display">\[\hat{\mathbf{Y}}= \mathbf{H}\mathbf{Y}\]</span></p>
<p><span class="math display">\[\mathbf{e}= \mathbf{Y} - \hat{\mathbf{Y}} =\mathbf{Y} - \mathbf{H}\mathbf{Y} = (\mathbf{I} - \mathbf{H})\mathbf{Y}\]</span></p>
<p><span class="math display">\[\mbox{SSE} = \mathbf{e}^T\mathbf{e} = \mathbf{Y}^T (\mathbf{I} - \mathbf{H})\mathbf{Y}\]</span></p>
<div id="the-qr-decomposition-of-a-matrix" class="section level3" number="4.6.1">
<h3>
<span class="header-section-number">4.6.1</span> The QR Decomposition of a matrix<a class="anchor" aria-label="anchor" href="#the-qr-decomposition-of-a-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>We have seen that OLS estimates for <span class="math inline">\(\boldsymbol{\beta}\)</span> can be found by using:</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T\mathbf{Y}.\]</span></p>
<p>Inverting the <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> matrix can sometimes introduce significant rounding errors into the calculations and most software packages use QR decomposition of the design matrix <span class="math inline">\(\mathbf{X}\)</span> to compute the parameter estimates. E.g. take a look at the documentation for the lm method in R.</p>
<p>How does this work?</p>
<p>We need to find an <span class="math inline">\(n \times p\)</span> matrix <span class="math inline">\(\mathbf{Q}\)</span> and a <span class="math inline">\(p \times p\)</span> matrix <span class="math inline">\(\mathbf{R}\)</span> such that:</p>
<p><span class="math display">\[\mathbf{X}=\mathbf{Q}\mathbf{R}\]</span></p>
<p>and</p>
<ul>
<li><p><span class="math inline">\(\mathbf{Q}\)</span> has orthonormal columns, i.e. <span class="math inline">\(\mathbf{Q}^T\mathbf{Q} = \mathbf{I}_p\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{R}\)</span> is an upper triangular matrix.</p></li>
</ul>
<p>There are several methods for computing the <span class="math inline">\(\mathbf{Q}\mathbf{R}\)</span> factorization (we won’t study them, but high quality code for the computation exists in publicly available Lapack package {<a href="http://www.netlib.org/lapack/lug/" class="uri">http://www.netlib.org/lapack/lug/</a>}).</p>
<p>We can show that:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{X} &amp;=\mathbf{Q}\mathbf{R} \\
\mathbf{X}^T\mathbf{X} &amp;=(\mathbf{Q}\mathbf{R})^T(\mathbf{Q}\mathbf{R}) = \mathbf{R}^T\mathbf{R}\\
\end{align*}\]</span></p>
<!-- (\mathbf{X}^T\mathbf{X})^{-1} &= (mathbf{R}^T\mathbf{R})^{-1} = mathbf{R}^{-1}(\mathbf{R}^T)^{-1}\\ -->
<!-- Note $\mathbf{R}$ is a square matrix.  -->
<p>Then:</p>
<p><span class="math display">\[\begin{align*}
(\mathbf{X}^T\mathbf{X})\hat{\boldsymbol{\beta}} &amp; =\mathbf{X}^T\mathbf{Y}\\
(\mathbf{R}^T\mathbf{R})\hat{\boldsymbol{\beta}} &amp; =\mathbf{R}^T\mathbf{Q}^T\mathbf{Y}\\
\mathbf{R}\hat{\boldsymbol{\beta}} &amp; = \mathbf{Q}^T\mathbf{Y}\\
\end{align*}\]</span></p>
<p>Since <span class="math inline">\(\mathbf{R}\)</span> is a triangular matrix we can use backsolving and this is an easy equation to solve.</p>
<p>We can also show that the hat matrix becomes:</p>
<p><span class="math display">\[\mathbf{H} = \mathbf{Q}\mathbf{Q}^T\]</span>
<!-- ## Confidence Interval for the Mean Response --></p>
<!-- Suppose we want to estimate $\mu=\mathbb{E}[y]$ at a particular value of $\mathbf{x}_0 = (1, x_{01}, x_{02}, ..., x_{0k})^T$. -->
<!-- The estimate is $$\mu_0 = \hat{y}_0 = \mathbf{x}_0^T \hat{\boldsymbol{\beta}}$$ -->
<!-- A $95\%$ C.I. for $\mu_0$ is given by -->
<!--  \begin{align*} -->
<!--  \mbox{Est} &\pm t_{n-(k+1)}(\alpha/2) \times \mbox{S.E.(Est)}\\ -->
<!--  \hat{y}_0 &\pm t_{n-(k+1)}(\alpha/2) \times \mbox{S.E.}_{\mbox{fit}}(\hat{y}_0) -->
<!--  \end{align*} -->
<!-- where -->
<!-- $$\mbox{S.E.}_{\mbox{fit}}(\hat{y}_0) = \sqrt{\sigma^2\mathbf{x}_0^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_0}$$ -->
<!-- ## Prediction Interval for a New Response -->
<!-- For a new observation: $\mathbf{x}_0 = (1, x_{01}, x_{02}, ..., x_{0k})^T$, the estimate is: -->
<!-- $$\hat{y}_0 = \mathbf{x}_0^T \hat{\boldsymbol{\beta}}$$ -->
<!-- Prediction interval is : -->
<!-- $$\hat{y}_0 &\pm t_{n-(k+1)}(\alpha/2) \times \mbox{S.E.}_{\mbox{pred}}(\hat{y}_0)$$ -->
<!-- where: -->
<!-- $$\mbox{S.E.}_{\mbox{pred}}(\hat{y}_0) = \sqrt{\sigma^2+\sigma^2\mathbf{x}_0^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_0}$$ -->
</div>
</div>
<div id="anova-for-multiple-regression" class="section level2" number="4.7">
<h2>
<span class="header-section-number">4.7</span> ANOVA for multiple regression<a class="anchor" aria-label="anchor" href="#anova-for-multiple-regression"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Recap</strong>: ANOVA decomposition</p>
<p><span class="math display">\[\begin{align*}
\mbox{SSR} &amp; = \sum_{i=1}^n(\hat{y}_i - \bar{y})^2 = \sum_{i=1}^n \hat{y}_i ^2- n\bar{y}^2 \\
\mbox{SSE} &amp; = \sum_{i=1}^n(y_i - \hat{y}_i)^2 = \sum_{i=1}^n e_i^2\\
\mbox{SST} &amp; = \sum_{i=1}^n(y_i - \bar{y})^2 = \sum_{i=1}^n y_i ^2- n\bar{y}^2 \\
\end{align*}\]</span></p>
<div class="theorem">
<p><span id="thm:unnamed-chunk-92" class="theorem"><strong>Theorem 4.3  </strong></span><span class="math inline">\(\mbox{SST} = \mbox{SSR} + \mbox{SSE}\)</span></p>
</div>
<p><strong>Proof</strong>: this follows from the decomposition of response = fit + residual.</p>
<p><span class="math display">\[\begin{align*}
\mathbf{Y} &amp; = \hat{\mathbf{Y}}  + \mathbf{e}\\
\mathbf{Y}^T\mathbf{Y} &amp; = (\hat{\mathbf{Y}} + \mathbf{e})^T (\hat{\mathbf{Y}} + \mathbf{e})\\
&amp; = \hat{\mathbf{Y}}^T\hat{\mathbf{Y}} + \mathbf{e}^T\mathbf{e}+ 2\hat{\mathbf{Y}}^T\mathbf{e} \\
\end{align*}\]</span></p>
<p>But <span class="math inline">\(\hat{\mathbf{Y}}^T = \hat{\boldsymbol{\beta}}^T\mathbf{X}^T\)</span> and <span class="math inline">\(\mathbf{X}^T\mathbf{e} = 0\)</span>, from normal equations, so
<span class="math inline">\(\hat{\mathbf{Y}}^T\mathbf{e} = 0\)</span>.</p>
<p>Alternatively: <span class="math inline">\(\hat{\mathbf{Y}} = \mathbf{H}\mathbf{Y}\)</span>, <span class="math inline">\(\mathbf{e} = (\mathbf{I} - \mathbf{H})\mathbf{Y}\)</span>, so
<span class="math inline">\(\hat{\mathbf{Y}}^T\mathbf{e} = \mathbf{Y}^T\mathbf{H}(\mathbf{I} - \mathbf{H})\mathbf{Y} = 0\)</span>, since <span class="math inline">\(\mathbf{H}^2=\mathbf{H}\)</span>.</p>
<p>Therefore,</p>
<p><span class="math display">\[\mathbf{Y}^T\mathbf{Y}  = \hat{\mathbf{Y}}^T\hat{\mathbf{Y}} + \mathbf{e}^T\mathbf{e}\]</span></p>
<p><span class="math display">\[\sum_{i=1}^n y_i^2 =\sum_{i=1}^n \hat{y}_i^2 + \sum_{i=1}^n e_i^2\]</span></p>
<p>and substracting <span class="math inline">\(n\bar{y}^2\)</span> from both sides completes the proof.</p>
<p><strong>ANOVA table</strong>:</p>
<p><span class="math display">\[\begin{align*}
\mbox{SSR} &amp; = \sum_{i=1}^n(\hat{y}_i - \bar{y})^2, \;\;\;\; df =p-1 \\
\mbox{SSE} &amp; = \sum_{i=1}^n(y_i - \hat{y}_i)^2, \;\;\;\; df = n-p \\
\mbox{SST} &amp; = \sum_{i=1}^n(y_i - \bar{y})^2, \;\;\;\; df =n-1.
\end{align*}\]</span></p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="left">SOURCE</th>
<th align="left">df</th>
<th align="left">SS</th>
<th align="left">MS</th>
<th align="left">F</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Regression</td>
<td align="left">p-1</td>
<td align="left">SSR</td>
<td align="left">MSR = SSR/(p-1)</td>
<td align="left">MSR/MSE</td>
</tr>
<tr class="even">
<td align="left">Error</td>
<td align="left">n-p</td>
<td align="left">SSE</td>
<td align="left">MSE = SSE/(n-p)</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Total</td>
<td align="left">n-1</td>
<td align="left">SST</td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table></div>
<p>If <span class="math inline">\(\beta_1 = \beta_2 = ... = \beta_k = 0\)</span> then <span class="math inline">\(\hat{\beta}_j \approx 0\)</span> for <span class="math inline">\(j = 1,...,k\)</span> and <span class="math inline">\(\hat{y}_i \approx \bar{y}\)</span>.</p>
<p>Then, <span class="math inline">\(\mbox{SSE} \approx \mbox{SST}\)</span> and <span class="math inline">\(\mbox{SSR} \approx 0\)</span>. Small values of <span class="math inline">\(\mbox{SSR}\)</span> relative to <span class="math inline">\(\mbox{SSE}\)</span> provide indication that <span class="math inline">\(\beta_1 = \beta_2 = ... = \beta_k = 0\)</span>.</p>
<p><span class="math display">\[\begin{align*}
&amp;H_0: \beta_1 = \beta_2 = ... = \beta_k = 0\\
&amp;H_A: \mbox{ not all }\beta_j = 0 \mbox{ for }j = 1,...,k
\end{align*}\]</span></p>
<p>Under <span class="math inline">\(H_0\)</span>:</p>
<p><span class="math display">\[F= \frac{\mbox{SSR}/(p-1)}{\mbox{SSE}/(n-p)} \sim F_{(p-1, n-p)}\]</span></p>
<p>P-value is <span class="math inline">\(P( F_{(p-1, n-p)} \geq F_{obs})\)</span>, where <span class="math inline">\(F_{obs}\)</span> is the observed <span class="math inline">\(F\)</span>-value.</p>
<p>Coefficient of determination <span class="math inline">\(R^2 = \frac{\mbox{SSR}}{\mbox{SST}}\)</span>, <span class="math inline">\(0 \leq R^2 \leq 1\)</span>.</p>
<p><span class="math inline">\(R^2\)</span> is the proportion of variability in <span class="math inline">\(Y\)</span> explained by regression on <span class="math inline">\(X_1,...,X_k\)</span>.</p>
<p>Adjusted <span class="math inline">\(R^2\)</span> is the modified version of <span class="math inline">\(R^2\)</span> adjusted for the number of predictors in the model. R uses:</p>
<p><span class="math display">\[R^2_{Adj} = 1-(1- R^2)\frac{n-1}{n-p-1}.\]</span></p>
<!-- %  -->
<p><!-- % % NOT what you'd expect -->
<!-- %```{r echo = TRUE, fig.align='center'} -->
<!-- % fit3 <- lm(carbon.monoxide ~ weight + nicotine) -->
<!-- % anova(fit3) # not one F test, but for each coefficient. Plus they are sequential SS(type 1) -->
<!-- % #library(car) -->
<!-- % Anova(fit3, type= 2) #Type-II tests are calculated according to the principle of marginality, testing each term after all others, except ignoring the term's higher-order relatives - same as MTB, but no one overall test.  -->
<!-- %``` -->
<!-- % } --></p>
</div>
<div id="way-anova-model" class="section level2" number="4.8">
<h2>
<span class="header-section-number">4.8</span> 1-way ANOVA model<a class="anchor" aria-label="anchor" href="#way-anova-model"><i class="fas fa-link"></i></a>
</h2>
<div id="example" class="section level3" number="4.8.1">
<h3>
<span class="header-section-number">4.8.1</span> Example:<a class="anchor" aria-label="anchor" href="#example"><i class="fas fa-link"></i></a>
</h3>
<p>A study was carried out to examine the effects of caffeine. Thirty students were randomly assigned to one of:</p>
<ul>
<li>control, no caffeine</li>
<li>low dose caffeine</li>
<li>low dose caffeine plus sugar</li>
</ul>
<p>The response <span class="math inline">\(y\)</span> is an index measuring unrest 2 hrs later.</p>
<p>(Example from <span class="citation">Draper and Smith (<a href="references.html#ref-draper66applied">1966</a>)</span>.)</p>
<p>Let <span class="math inline">\(y_{ij}\)</span> be the response for the <span class="math inline">\(j^{th}\)</span> person in the <span class="math inline">\(i^{th}\)</span> group, <span class="math inline">\(j=1,...,10\)</span>, <span class="math inline">\(i=1,2,3\)</span>.</p>
<p>Let <span class="math inline">\(n_i\)</span> be the number assigned to group <span class="math inline">\(i\)</span>.</p>
<p>Model:</p>
<ul>
<li>
<span class="math inline">\(y_{ij} = \mu_{i} + \epsilon_{ij}\)</span>, <span class="math inline">\(\quad\)</span> <span class="math inline">\(\epsilon_{ij}\)</span> iid <span class="math inline">\(N(0, \sigma^2)\)</span>,
where <span class="math inline">\(\mu_i\)</span> is the population mean for those at dose <span class="math inline">\(i\)</span>.</li>
</ul>
<p>Or equivalently:</p>
<ul>
<li>
<span class="math inline">\(y_{ij} = \mu + \alpha_{i} + \epsilon_{ij}\)</span>, <span class="math inline">\(\quad\)</span> <span class="math inline">\(\epsilon_{ij}\)</span> iid <span class="math inline">\(N(0, \sigma^2)\)</span>, where <span class="math inline">\(\mu\)</span> is the overall population mean and <span class="math inline">\(\alpha_i\)</span> is the effect of receiving treatment <span class="math inline">\(i\)</span>.</li>
</ul>
<p>O.L.S estimates for Model 1 are:</p>
<p><span class="math display">\[\begin{align*}
S(\mu_1, ..., \mu_g)&amp; =\sum_{i=1}^g\sum_{j=1}^{n_i}\epsilon_{ij}^2 = \sum_{i=1}^g\sum_{j=1}^{n_i}(y_{ij}-\mu_i)^2,\\
\frac{\delta S(\mu_1, ..., \mu_g)}{\delta \mu_i}&amp; = -2\sum_{j=1}^{n_i}(y_{ij}-\mu_i), \quad  \forall i = 1,...,g \\
\end{align*}\]</span></p>
<p>Setting these equal to 0 and evaluating at <span class="math inline">\(\hat{\mu}_i\)</span> gives:</p>
<p><span class="math display">\[\begin{align*}
\sum_{j=1}^{n_i}(y_{ij}-\hat{\mu}_i) &amp; =0.\\
\sum_{j=1}^{n_i}y_{ij}-n_i\hat{\mu}_i &amp; =0.\\
\hat{\mu}_i =\sum_{j=1}^{n_i}y_{ij}/n_i &amp; =\bar{y}_{i.}\\
\end{align*}\]</span></p>
<p>NOTE: <span class="math inline">\(\bar{y}_{i.}\)</span> is the average of responses at level <span class="math inline">\(i\)</span> of <span class="math inline">\(X\)</span>.</p>
<p>Model 1 has <span class="math inline">\(g=3\)</span> parameters but model 2 has 4 parameters and is over-parameterised (<span class="math inline">\(\mu_i = \mu + \alpha_{i}\)</span>).</p>
<p>Usually the constraint <span class="math inline">\(\sum \alpha_i = 0\)</span> or <span class="math inline">\(\alpha_3 = 0\)</span> is imposed.</p>
<p>The hypothesis of interest in this model is:</p>
<p><span class="math display">\[\begin{align*}
&amp; H_0: \mu_1=\mu_2 = ...= \mu_g\\
&amp; H_A: \mbox{not all } \mu_i \mbox{ are the same.}\\
\end{align*}\]</span></p>
<p>Equivalently:</p>
<p><span class="math display">\[\begin{align*}
&amp; H_0: \alpha_i=0, \hspace{1cm}\forall i = 1,...,g\\
&amp; H_A: \mbox{not all } \alpha_i = 0.\\
\end{align*}\]</span></p>
<p>Calculations can be summarised in the ANOVA table:</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="left">SOURCE</th>
<th align="left">df</th>
<th align="left">SS</th>
<th align="left">MS</th>
<th align="left">F</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Group</td>
<td align="left">g-1</td>
<td align="left">SSG</td>
<td align="left">MSG = SSG/(g-1)</td>
<td align="left">MSG/MSE</td>
</tr>
<tr class="even">
<td align="left">Error</td>
<td align="left">n-g</td>
<td align="left">SSE</td>
<td align="left">MSE = SSE/(n-g)</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Total</td>
<td align="left">n-1</td>
<td align="left">SST</td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table></div>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\mbox{SSG} = \sum_{i = 1}^gn_{i}(\bar{y}_{i.} - \bar{y}_{..})^{2}\)</span></p></li>
<li><p><span class="math inline">\(\mbox{SSE} = \sum_{i=1}^g\sum_{j = 1}^{n_i}(y_{ij} - \bar{y}_{i.})^{2}\)</span></p></li>
<li><p><span class="math inline">\(\mbox{SST} = \sum_{i=1}^g\sum_{j = 1}^{n_i}(y_{ij} - \bar{y}_{..})^{2}\)</span></p></li>
</ul>
<!-- \begin{tabular}{l | c c c c} --><!-- Source & df & SS & MS & F \\ \hline --><!-- &&&& \\ --><!-- Group & $g-1$ & $\mbox{SSG} = \sum_{i = 1}^gn_{i}(\bar{y}_{i.} - \bar{y}_{..})^{2}$ & $\mbox{MSG} = \mbox{SSG}/(g-1)$& $\mbox{MSG}/\mbox{MSE}$ \\ --><!-- &&&& \\ --><!-- Error & $n-g$ & \mbox{SSE} = $\sum_{i=1}^g\sum_{j = 1}^{n_i}(y_{ij} - \bar{y}_{i.})^{2}$ & $\mbox{MSE} = \mbox{SSE}/(n-g)$&\\ \hline --><!-- &&&& \\ --><!-- Total & $n-1$ & $\mbox{SST} = \sum_{i=1}^g\sum_{j = 1}^{n_i}(y_{ij} - \bar{y}_{..})^{2}$ && --><!-- \end{tabular} --><p>Under <span class="math inline">\(H_{0}\)</span>: <span class="math display">\[F_{obs} = \frac{\mbox{MSG}}{\mbox{MSE}} \sim F_{g-1, n-g}.\]</span></p>
<p>We reject <span class="math inline">\(H_{0}\)</span> for large values of <span class="math inline">\(F_{obs}\)</span>.</p>
</div>
</div>
<div id="one-way-anova-in-regression-notation" class="section level2" number="4.9">
<h2>
<span class="header-section-number">4.9</span> One way ANOVA in regression notation<a class="anchor" aria-label="anchor" href="#one-way-anova-in-regression-notation"><i class="fas fa-link"></i></a>
</h2>
<p>First we have to set up <strong>dummy variables</strong>:</p>
<p><span class="math display">\[X_i=  \{
\begin{array}{ll}
1 &amp; \quad\mbox{if observation in gp }i\\
0 &amp; \quad\mbox{ow}\end{array}\]</span></p>
<p>Model: (effects model <span class="math inline">\(y_{ij} = \mu + \alpha_{i} + \epsilon_{ij}\)</span>)</p>
<p><span class="math display">\[Y = \mu + \alpha_1X_1 + \alpha_2X_2 + \alpha_3X_3 + \epsilon\]</span></p>
<p><span class="math display">\[\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}\]</span></p>
<p>where <span class="math inline">\(\mathbf{Y}\)</span> is a <span class="math inline">\(30 \times 1\)</span> vector of responses and</p>
<p><span class="math display">\[\boldsymbol{\beta} =
\begin{bmatrix}
\mu \\
\alpha_{1}   \\
\alpha_{2}  \\
\alpha_{3}
\end{bmatrix} \quad \quad \quad \mathbf{X} =
\begin{bmatrix}
1 &amp; 1 &amp; 0 &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
1 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 1 &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
1 &amp; 0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 0 &amp; 1 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
1 &amp; 0 &amp; 0 &amp; 1
\end{bmatrix}\]</span></p>
<p>Note that in the <span class="math inline">\(\mathbf{X}\)</span> matrix the first column equals the sum of the second, third and fourth columns, therefore it is not of full rank so <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> does not have an inverse and there is not unique <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>.</p>
<p>We could require <span class="math inline">\(\sum\alpha_i = 0\)</span> and then the solution would be unique. Or, we could require that <span class="math inline">\(\alpha_3 = 0\)</span> and drop the last column of <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>We could also derive a solution where the first column of <span class="math inline">\(\mathbf{X}\)</span> is omitted. Then the model becomes:</p>
<p><span class="math display">\[Y = \mu_1X_1 + \mu_2X_2 + \mu_3X_3 + \epsilon\]</span></p>
<p><span class="math display">\[\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}\]</span></p>
<p>where</p>
<p><span class="math display">\[\boldsymbol{\beta} =\begin{bmatrix}
\mu_{1} \\
\mu_{2}\\
\mu_{3}
\end{bmatrix}\quad \quad \quad
\mathbf{X} = \begin{bmatrix}
1 &amp; 0 &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots \\
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
\vdots &amp; \vdots &amp; \vdots \\
0 &amp; 0 &amp; 1
\end{bmatrix}\]</span></p>
<p>This is the means model <span class="math inline">\(y_{ij} = \mu_i + \epsilon_{ij}\)</span>.</p>
<p><span class="math display">\[\begin{align*}
\hat{\boldsymbol{\beta}} &amp; = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}\\
&amp; = \begin{bmatrix}
10 &amp; 0 &amp; 0 \\
0 &amp; 10 &amp; 0 \\
0 &amp; 0 &amp; 10
\end{bmatrix}^{-1}\begin{bmatrix}
\sum_{j=1}^{10} y_{1j}   \\
\sum_{j=1}^{10} y_{2j}\\
\sum_{j=1}^{10} y_{3j}
\end{bmatrix}\\
&amp; = \frac{1}{10}\mathbf{I}_3\begin{bmatrix}
y_{1.}   \\
y_{2.}\\
y_{3.}
\end{bmatrix}\\
&amp; = \begin{bmatrix}
\bar{y}_{1.}   \\
\bar{y}_{2.}\\
\bar{y}_{3.}
\end{bmatrix}\\
&amp; = \begin{bmatrix}
\hat{\mu}_{1}   \\
\hat{\mu}_{2}\\
\hat{\mu}_{3}
\end{bmatrix}
\end{align*}\]</span></p>
<p>The fitted values are then:</p>
<p><span class="math display">\[\hat{Y} = \hat{\mu}_1X_1 + \hat{\mu}_2X_2 + \hat{\mu}_3X_3\]</span></p>
<p>or</p>
<p><span class="math display">\[\hat{Y} = \hat{\mu}_i = \bar{y}_{i.}\]</span></p>
<p>if <span class="math inline">\(Y\)</span> comes from group <span class="math inline">\(i\)</span>.</p>
<div id="fitting-the-model-in-r" class="section level3" number="4.9.1">
<h3>
<span class="header-section-number">4.9.1</span> Fitting the model in R<a class="anchor" aria-label="anchor" href="#fitting-the-model-in-r"><i class="fas fa-link"></i></a>
</h3>
<div id="example-caffeine-in-r" class="section level4" number="4.9.1.1">
<h4>
<span class="header-section-number">4.9.1.1</span> Example: Caffeine in R<a class="anchor" aria-label="anchor" href="#example-caffeine-in-r"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb127"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">coffee.data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.csv</a></span><span class="op">(</span><span class="st">"data/coffee.csv"</span><span class="op">)</span></span>
<span><span class="va">coffee.data</span> <span class="op">|&gt;</span> <span class="fu">glimpse</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#Tell R this is a categorical variable</span></span>
<span><span class="va">coffee.data</span><span class="op">$</span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="va">coffee.data</span><span class="op">$</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="va">coffee.data</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_boxplot.html">geom_boxplot</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-95-1.png" width="288" style="display: block; margin: auto;"></div>
<ol style="list-style-type: decimal">
<li>Using <code>aov</code>, <span class="math inline">\(Y\)</span> is response, <span class="math inline">\(X\)</span> is group.</li>
</ol>
<div class="sourceCode" id="cb128"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit.oneway.anova</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/aov.html">aov</a></span><span class="op">(</span><span class="va">y</span><span class="op">~</span><span class="va">x</span>, data <span class="op">=</span> <span class="va">coffee.data</span><span class="op">)</span> </span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fit.oneway.anova</span><span class="op">)</span></span></code></pre></div>
<pre><code>##             Df Sum Sq Mean Sq F value  Pr(&gt;F)   
## x            2   61.4  30.700   6.181 0.00616 **
## Residuals   27  134.1   4.967                   
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
<div class="sourceCode" id="cb130"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/anova.html">anova</a></span><span class="op">(</span><span class="va">fit.oneway.anova</span><span class="op">)</span></span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: y
##           Df Sum Sq Mean Sq F value   Pr(&gt;F)   
## x          2   61.4 30.7000  6.1812 0.006163 **
## Residuals 27  134.1  4.9667                    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
<div class="sourceCode" id="cb132"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/model.tables.html">model.tables</a></span><span class="op">(</span><span class="va">fit.oneway.anova</span>, <span class="st">"means"</span><span class="op">)</span></span></code></pre></div>
<pre><code>## Tables of means
## Grand mean
##       
## 246.5 
## 
##  x 
## x
##     1     2     3 
## 244.8 246.4 248.3</code></pre>
<div class="sourceCode" id="cb134"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># plot(fit.oneway.anova) #diagnostic plots</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coefficients</a></span><span class="op">(</span><span class="va">fit.oneway.anova</span><span class="op">)</span></span></code></pre></div>
<pre><code>## (Intercept)          x2          x3 
##       244.8         1.6         3.5</code></pre>
<div class="sourceCode" id="cb136"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/TukeyHSD.html">TukeyHSD</a></span><span class="op">(</span><span class="va">fit.oneway.anova</span>, <span class="st">"x"</span>, ordered <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
##     factor levels have been ordered
## 
## Fit: aov(formula = y ~ x, data = coffee.data)
## 
## $x
##     diff        lwr      upr     p adj
## 2-1  1.6 -0.8711391 4.071139 0.2606999
## 3-1  3.5  1.0288609 5.971139 0.0043753
## 3-2  1.9 -0.5711391 4.371139 0.1562593</code></pre>
<div class="sourceCode" id="cb138"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/TukeyHSD.html">TukeyHSD</a></span><span class="op">(</span><span class="va">fit.oneway.anova</span>, <span class="st">"x"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="ST303-Lecture-Notes_files/figure-html/unnamed-chunk-96-1.png" width="672"></div>
<ol start="2" style="list-style-type: decimal">
<li>Using <code>lm</code>
</li>
</ol>
<div class="sourceCode" id="cb139"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit.coffee</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span><span class="op">~</span><span class="va">x</span>, data <span class="op">=</span> <span class="va">coffee.data</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fit.coffee</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x, data = coffee.data)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.400 -2.075 -0.300  1.675  3.700 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 244.8000     0.7047 347.359  &lt; 2e-16 ***
## x2            1.6000     0.9967   1.605  0.12005    
## x3            3.5000     0.9967   3.512  0.00158 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.229 on 27 degrees of freedom
## Multiple R-squared:  0.3141, Adjusted R-squared:  0.2633 
## F-statistic: 6.181 on 2 and 27 DF,  p-value: 0.006163</code></pre>
<div class="sourceCode" id="cb141"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/anova.html">anova</a></span><span class="op">(</span><span class="va">fit.coffee</span><span class="op">)</span></span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: y
##           Df Sum Sq Mean Sq F value   Pr(&gt;F)   
## x          2   61.4 30.7000  6.1812 0.006163 **
## Residuals 27  134.1  4.9667                    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
<div class="sourceCode" id="cb143"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">fit.coffee</span><span class="op">)</span></span></code></pre></div>
<pre><code>##    (Intercept) x2 x3
## 1            1  0  0
## 2            1  0  0
## 3            1  0  0
## 4            1  0  0
## 5            1  0  0
## 6            1  0  0
## 7            1  0  0
## 8            1  0  0
## 9            1  0  0
## 10           1  0  0
## 11           1  1  0
## 12           1  1  0
## 13           1  1  0
## 14           1  1  0
## 15           1  1  0
## 16           1  1  0
## 17           1  1  0
## 18           1  1  0
## 19           1  1  0
## 20           1  1  0
## 21           1  0  1
## 22           1  0  1
## 23           1  0  1
## 24           1  0  1
## 25           1  0  1
## 26           1  0  1
## 27           1  0  1
## 28           1  0  1
## 29           1  0  1
## 30           1  0  1
## attr(,"assign")
## [1] 0 1 1
## attr(,"contrasts")
## attr(,"contrasts")$x
## [1] "contr.treatment"</code></pre>
<p><span class="math inline">\(X_1\)</span> is dropped. Compare ANOVA table with the 1-way ANOVA results.</p>
<ol start="3" style="list-style-type: decimal">
<li>Using <code>lm</code> with the constrain on the sum of effects equals 0.</li>
</ol>
<div class="sourceCode" id="cb145"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/options.html">options</a></span><span class="op">(</span>contrasts<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">'contr.sum'</span>, <span class="st">'contr.sum'</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">fit.coffee</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span><span class="op">~</span><span class="va">x</span>, data <span class="op">=</span> <span class="va">coffee.data</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fit.coffee</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x, data = coffee.data)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.400 -2.075 -0.300  1.675  3.700 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 246.5000     0.4069 605.822  &lt; 2e-16 ***
## x1           -1.7000     0.5754  -2.954  0.00642 ** 
## x2           -0.1000     0.5754  -0.174  0.86333    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.229 on 27 degrees of freedom
## Multiple R-squared:  0.3141, Adjusted R-squared:  0.2633 
## F-statistic: 6.181 on 2 and 27 DF,  p-value: 0.006163</code></pre>
<div class="sourceCode" id="cb147"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/anova.html">anova</a></span><span class="op">(</span><span class="va">fit.coffee</span><span class="op">)</span></span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: y
##           Df Sum Sq Mean Sq F value   Pr(&gt;F)   
## x          2   61.4 30.7000  6.1812 0.006163 **
## Residuals 27  134.1  4.9667                    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
<div class="sourceCode" id="cb149"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">fit.coffee</span><span class="op">)</span></span></code></pre></div>
<pre><code>##    (Intercept) x1 x2
## 1            1  1  0
## 2            1  1  0
## 3            1  1  0
## 4            1  1  0
## 5            1  1  0
## 6            1  1  0
## 7            1  1  0
## 8            1  1  0
## 9            1  1  0
## 10           1  1  0
## 11           1  0  1
## 12           1  0  1
## 13           1  0  1
## 14           1  0  1
## 15           1  0  1
## 16           1  0  1
## 17           1  0  1
## 18           1  0  1
## 19           1  0  1
## 20           1  0  1
## 21           1 -1 -1
## 22           1 -1 -1
## 23           1 -1 -1
## 24           1 -1 -1
## 25           1 -1 -1
## 26           1 -1 -1
## 27           1 -1 -1
## 28           1 -1 -1
## 29           1 -1 -1
## 30           1 -1 -1
## attr(,"assign")
## [1] 0 1 1
## attr(,"contrasts")
## attr(,"contrasts")$x
## [1] "contr.sum"</code></pre>
<div class="sourceCode" id="cb151"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/options.html">options</a></span><span class="op">(</span>contrasts<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">'contr.treatment'</span>, <span class="st">'contr.treatment'</span><span class="op">)</span><span class="op">)</span> <span class="co"># to reset</span></span></code></pre></div>
<ol start="4" style="list-style-type: decimal">
<li>Using <code>lm</code>, without intercept (this is the <strong>means</strong> model):</li>
</ol>
<div class="sourceCode" id="cb152"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit.coffee</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span> <span class="op">-</span> <span class="fl">1</span>, data <span class="op">=</span> <span class="va">coffee.data</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fit.coffee</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x - 1, data = coffee.data)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.400 -2.075 -0.300  1.675  3.700 
## 
## Coefficients:
##    Estimate Std. Error t value Pr(&gt;|t|)    
## x1 244.8000     0.7047   347.4   &lt;2e-16 ***
## x2 246.4000     0.7047   349.6   &lt;2e-16 ***
## x3 248.3000     0.7047   352.3   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.229 on 27 degrees of freedom
## Multiple R-squared:  0.9999, Adjusted R-squared:  0.9999 
## F-statistic: 1.223e+05 on 3 and 27 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb154"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/anova.html">anova</a></span><span class="op">(</span><span class="va">fit.coffee</span><span class="op">)</span></span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: y
##           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## x          3 1822929  607643  122344 &lt; 2.2e-16 ***
## Residuals 27     134       5                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
<div class="sourceCode" id="cb156"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">fit.coffee</span><span class="op">)</span></span></code></pre></div>
<pre><code>##    x1 x2 x3
## 1   1  0  0
## 2   1  0  0
## 3   1  0  0
## 4   1  0  0
## 5   1  0  0
## 6   1  0  0
## 7   1  0  0
## 8   1  0  0
## 9   1  0  0
## 10  1  0  0
## 11  0  1  0
## 12  0  1  0
## 13  0  1  0
## 14  0  1  0
## 15  0  1  0
## 16  0  1  0
## 17  0  1  0
## 18  0  1  0
## 19  0  1  0
## 20  0  1  0
## 21  0  0  1
## 22  0  0  1
## 23  0  0  1
## 24  0  0  1
## 25  0  0  1
## 26  0  0  1
## 27  0  0  1
## 28  0  0  1
## 29  0  0  1
## 30  0  0  1
## attr(,"assign")
## [1] 1 1 1
## attr(,"contrasts")
## attr(,"contrasts")$x
## [1] "contr.treatment"</code></pre>
<p>1), 2) and 3) give the same ANOVA table. In 4), when the model does not include an intercept, the ANOVA table shows the uncorrected SS, i.e. <span class="math inline">\(\bar{y}\)</span> not substracted:</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="left">SOURCE</th>
<th align="left">df</th>
<th align="left">SS</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Regression</td>
<td align="left">p</td>
<td align="left">SSR</td>
</tr>
<tr class="even">
<td align="left">Error</td>
<td align="left">n-p</td>
<td align="left">SSE</td>
</tr>
<tr class="odd">
<td align="left">Total</td>
<td align="left">n</td>
<td align="left">SST</td>
</tr>
</tbody>
</table></div>
<p>Where <span class="math inline">\(SSR = \sum \hat{y}^2\)</span>, <span class="math inline">\(SSE = \sum(y - \hat{y})^{2}\)</span> and <span class="math inline">\(SST = \sum y^{2}\)</span>.</p>
<!-- \begin{tabular}{l c c} -->
<!-- Source & df & SS \\ \hline -->
<!-- && \\ -->
<!-- Regression & $p$ & $\sum \hat{y}^2$ \\ -->
<!-- && \\ -->
<!-- Error & $n-p$ &  $\sum(y - \hat{y})^{2}$  \\ -->
<!-- && \\ -->
<!-- Total & $n$ & $\sum y^{2}$ \\ -->
<!-- \end{tabular} -->
<!-- In this example x is a factor, not a continuous variable, x1, x2 and x3 are the dummy variables that identify the groups in x. -->
<!-- make dummy variables $X_1$, $X_2$, $X_3$ and: -->
<!-- * use predictors $X_1$, $X_2$, $X_3$ with no intercept, or -->
<!-- * use predictors $X_1$, $X_2$ with intercept. -->
<p>It is also possible to fit the model by explicitly specifying the dummy variables.</p>
<div class="sourceCode" id="cb158"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># or using dummy variables</span></span>
<span></span>
<span><span class="va">d1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="va">coffee.data</span><span class="op">$</span><span class="va">x</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">d2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="va">coffee.data</span><span class="op">$</span><span class="va">x</span> <span class="op">==</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">d3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="va">coffee.data</span><span class="op">$</span><span class="va">x</span> <span class="op">==</span> <span class="fl">3</span><span class="op">)</span></span>
<span></span>
<span><span class="va">fit.coffee.dummy</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">coffee.data</span><span class="op">$</span><span class="va">y</span> <span class="op">~</span> <span class="va">d1</span> <span class="op">+</span><span class="va">d2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fit.coffee.dummy</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = coffee.data$y ~ d1 + d2)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.400 -2.075 -0.300  1.675  3.700 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 248.3000     0.7047 352.326  &lt; 2e-16 ***
## d1           -3.5000     0.9967  -3.512  0.00158 ** 
## d2           -1.9000     0.9967  -1.906  0.06730 .  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.229 on 27 degrees of freedom
## Multiple R-squared:  0.3141, Adjusted R-squared:  0.2633 
## F-statistic: 6.181 on 2 and 27 DF,  p-value: 0.006163</code></pre>
<div class="sourceCode" id="cb160"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#no intercept</span></span>
<span><span class="va">fit.coffee.dummy2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">coffee.data</span><span class="op">$</span><span class="va">y</span> <span class="op">~</span> <span class="va">d1</span> <span class="op">+</span><span class="va">d2</span> <span class="op">+</span><span class="va">d3</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fit.coffee.dummy2</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = coffee.data$y ~ d1 + d2 + d3 - 1)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.400 -2.075 -0.300  1.675  3.700 
## 
## Coefficients:
##    Estimate Std. Error t value Pr(&gt;|t|)    
## d1 244.8000     0.7047   347.4   &lt;2e-16 ***
## d2 246.4000     0.7047   349.6   &lt;2e-16 ***
## d3 248.3000     0.7047   352.3   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.229 on 27 degrees of freedom
## Multiple R-squared:  0.9999, Adjusted R-squared:  0.9999 
## F-statistic: 1.223e+05 on 3 and 27 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</div>
<div id="confidence-intervals-and-hypothesis-tests-for-linear-combinations-of-boldsymbolbeta" class="section level2" number="4.10">
<h2>
<span class="header-section-number">4.10</span> Confidence intervals and hypothesis tests for linear combinations of <span class="math inline">\(\boldsymbol{\beta}\)</span><a class="anchor" aria-label="anchor" href="#confidence-intervals-and-hypothesis-tests-for-linear-combinations-of-boldsymbolbeta"><i class="fas fa-link"></i></a>
</h2>
<p>From the theory of OLS:</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} \sim N(\boldsymbol{\beta},(\mathbf{X}^T\mathbf{X})^{-1}\sigma^2)\]</span></p>
<p>and</p>
<p><span class="math display">\[\mathbf{c}^T\hat{\boldsymbol{\beta}} \sim N(\mathbf{c}^T\boldsymbol{\beta},\mathbf{c}^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{c}\sigma^2)\]</span></p>
<p>For the caffeine example (1 way ANOVA model): suppose we want to compare the treatment means with the control mean, that is, we want a CI for:</p>
<p><span class="math display">\[\frac{\mu_2+\mu_3}{2}-\mu_1\]</span></p>
<p>Let <span class="math inline">\(\mathbf{c}^T= (-1, 1/2, 1/2)\)</span>, <span class="math inline">\(\boldsymbol{\beta}^T = (\mu_1, \mu_2, \mu_3)\)</span>.</p>
<p><span class="math display">\[\mathbf{c}^T\boldsymbol{\beta} = -\mu_1+\mu_2/2+ \mu_3/2\]</span></p>
<p>is estimated by:</p>
<p><span class="math display">\[\mathbf{c}^T\hat{\boldsymbol{\beta}} = -\bar{y}_{1.}+\bar{y}_{2.}/2+ \bar{y}_{3.}/2\]</span></p>
<p>and the variance is:</p>
<p><span class="math display">\[\begin{bmatrix}
-1 &amp; 1/2 &amp; 1/2
\end{bmatrix} \frac{1}{10}\mathbf{I}_3 \begin{bmatrix}
-1 \\
1/2 \\
1/2\end{bmatrix}\sigma^2 = \frac{3}{20}\sigma^2\]</span></p>
<p>So, the <span class="math inline">\(100 \times (1- \alpha) \%\)</span>CI is:</p>
<p><span class="math display">\[\mathbf{c}^T\hat{\boldsymbol{\beta}} \pm t_{27}(\alpha/2) \sqrt{\frac{3}{20}\hat{\sigma}^2}\]</span></p>
<p>The df is: <span class="math inline">\(n-g = 30-3 = 27\)</span>.</p>
<p>We could also test hypotheses e.g. <span class="math inline">\(H_o: \mathbf{c}^T\boldsymbol{\beta} = 0\)</span>. The test statistic:</p>
<p><span class="math display">\[\frac{\mathbf{c}^T\hat{\boldsymbol{\beta}}}{\sqrt{\frac{3}{20}\hat{\sigma}^2}} \sim t_{27}.\]</span></p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="SLR.html"><span class="header-section-number">3</span> Simple Linear regression</a></div>
<div class="next"><a href="model-comparisons-and-testing-for-lack-of-fit.html"><span class="header-section-number">5</span> Model comparisons and testing for lack of fit</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#multiple-regression"><span class="header-section-number">4</span> Multiple regression</a></li>
<li>
<a class="nav-link" href="#introductory-examples-1"><span class="header-section-number">4.1</span> Introductory examples</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#example-1-fuel-use"><span class="header-section-number">4.1.1</span> Example 1: Fuel Use</a></li>
<li><a class="nav-link" href="#example-2-categorical-predictors"><span class="header-section-number">4.1.2</span> Example 2: Categorical predictors</a></li>
<li><a class="nav-link" href="#example-3-polynomials"><span class="header-section-number">4.1.3</span> Example 3: Polynomials</a></li>
<li><a class="nav-link" href="#example-4-nonlinear-relationships"><span class="header-section-number">4.1.4</span> Example 4: Nonlinear relationships</a></li>
<li><a class="nav-link" href="#diamonds-data"><span class="header-section-number">4.1.5</span> Diamonds data</a></li>
<li><a class="nav-link" href="#cigarette-data-continued"><span class="header-section-number">4.1.6</span> Cigarette Data continued</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#least-squares-estimation-for-multiple-regression"><span class="header-section-number">4.2</span> Least squares estimation for multiple regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#estimation-of-sigma2-varepsilon"><span class="header-section-number">4.2.1</span> Estimation of \(\sigma^2\) = Var\((\epsilon)\)</a></li>
<li><a class="nav-link" href="#estimation-of-varhatbeta"><span class="header-section-number">4.2.2</span> Estimation of Var\((\hat{\beta})\)</a></li>
</ul>
</li>
<li><a class="nav-link" href="#prediction-from-multiple-linear-regression-model"><span class="header-section-number">4.3</span> Prediction from multiple linear regression model</a></li>
<li>
<a class="nav-link" href="#regression-models-in-matrix-notation-examples"><span class="header-section-number">4.4</span> Regression models in matrix notation: examples</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#example-1-slr"><span class="header-section-number">4.4.1</span> Example 1: SLR</a></li>
<li><a class="nav-link" href="#example-2"><span class="header-section-number">4.4.2</span> Example 2</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#the-formal-multiple-regression-model-and-properties"><span class="header-section-number">4.5</span> The formal multiple regression model and properties</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#concepts-random-vectors-covariance-matrix-multivariate-normal-distribution-mvn."><span class="header-section-number">4.5.1</span> Concepts: random vectors, covariance matrix, multivariate normal distribution (MVN).</a></li>
<li><a class="nav-link" href="#multiple-regression-model"><span class="header-section-number">4.5.2</span> Multiple regression model</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#the-hat-matrix"><span class="header-section-number">4.6</span> The hat matrix</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#the-qr-decomposition-of-a-matrix"><span class="header-section-number">4.6.1</span> The QR Decomposition of a matrix</a></li></ul>
</li>
<li><a class="nav-link" href="#anova-for-multiple-regression"><span class="header-section-number">4.7</span> ANOVA for multiple regression</a></li>
<li>
<a class="nav-link" href="#way-anova-model"><span class="header-section-number">4.8</span> 1-way ANOVA model</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#example"><span class="header-section-number">4.8.1</span> Example:</a></li></ul>
</li>
<li>
<a class="nav-link" href="#one-way-anova-in-regression-notation"><span class="header-section-number">4.9</span> One way ANOVA in regression notation</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#fitting-the-model-in-r"><span class="header-section-number">4.9.1</span> Fitting the model in R</a></li></ul>
</li>
<li><a class="nav-link" href="#confidence-intervals-and-hypothesis-tests-for-linear-combinations-of-boldsymbolbeta"><span class="header-section-number">4.10</span> Confidence intervals and hypothesis tests for linear combinations of \(\boldsymbol{\beta}\)</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Linear Models Lecture Notes</strong>" was written by Katarina Domijan. It was last built on 2025-12-16.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
